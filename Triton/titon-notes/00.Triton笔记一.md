# Triton笔记一

- [Triton笔记一](#triton笔记一)
  - [Overview](#overview)
    - [GPU 编程的挑战](#gpu-编程的挑战)
    - [Programming Model](#programming-model)
    - [Vector ADD 例子](#vector-add-例子)
  - [References](#references)

## Overview

OpenAI提供了一个blog:[Introducing Triton: Open-source GPU programming for neural networks](https://openai.com/index/triton/)来介绍Triton的设计想法和目标。

Triton是一个开源的Python-like语言（实际上我更想称它为Python-based的DSL语言），它用于帮助我们在没有CUDA经验的情况下就能写出高效的GPU代码，而达成这个目标的方法就是编译。所以Triton不只是一个Python-based的DSL语言，同时又是一个编译器。由于GPU编程本身的复杂性和难度，使得用户写出一个高效的算法变得比较困难，虽然现在有一些系统来简化这个过程，但是依然缺乏灵活性，因此Triton应运而生。

更具体来说，Triton想要用户用较少的努力就可以写出一个达到硬件性能巅峰的kernel。比如要写一个FP16的矩阵乘法的kernel，让它的性能达到cublas的水平，只需要不到25行代码。甚至能写出比Torch的性能高两倍的kernel。

### GPU 编程的挑战

现代的GPU架构由三个主要部分构成：

1. DRAM：Dynamic Random Access Memory，GPU中的主要内存，通常用作现存；
2. SRAM：Static Random Access Memory，价格更加昂贵，但是访问速度更快，所以主要用于存储临时数据和需要频繁访问的数据；
3. ALUs：Arithmetic Logic Units，算术逻辑单元，负责执行算术运算（如加法、减法、乘法等）和逻辑运算（如与、或、非等）；

![Basic architecture of a GPU.png](../.images/Basic%20architecture%20of%20a%20GPU.png)

在优化CUDA代码的时候，需要综合考虑到这三个组件：

1. DRAM的内存传输必须合并成大型事务，以利用现代内存接口的大总线宽度（内存合并访问）；
2. 数据必须在重复使用前手动存储到SRAM，并且进行管理来最小化检索时的共享内存冲突（minimize shared memory bank conflicts）；
3. 计算必须在SM之间和它们内部被小心地划分和调度，来提升指令或者线程级别的并行度，并且利用专用的ALU（如Tensor Core）；

即使对一个非常有经验的CUDA开发人员来说这些问题依然是一个挑战。而Triton的目标就是能完全自动化这些优化，使得用户能够关注他们代码的高层逻辑。Triton 的目标是广泛适用，因此不会自动安排跨 SM 的工作 - 将一些重要的算法考虑因素（如tiling, inter-SM synchronization）留给开发人员自行决定。

文章中给出的图很明确地写明了CUDA和Triton的区别：

![Triton vs CUDA.png](../.images/Triton%20vs%20CUDA.png)

### Programming Model

在类似的DSL+Compiler的中，Triton和Numba最为接近（Numba我之前也没有详细看过）：

1. Kernel通过被装饰的Python函数来定义；
2. 以不同的program_id并行启动在grid实例上；

Triton编程模型中最大的特点就是Triton不是SIMT的模式，而是只通过对block的操作来实现并行，从而规避了很多和CUDA Thread相关的问题（如memory coalescing, shared memory synchronization/conflicts, tensor core scheduling）

文中给出了一个Vector Add的例子，对比了Triton和Numba的使用

```python
BLOCK = 512

# This is a GPU kernel in Numba.
# Different instances of this
# function may run in parallel.
@jit
def add(X, Y, Z, N):
   # In Numba/CUDA, each kernel 
   # instance itself uses an SIMT execution
   # model, where instructions are executed in
   # parallel for different values of threadIdx
   tid = threadIdx.x
   bid = blockIdx.x
   # scalar index
   idx = bid * BLOCK + tid
   if id < N:
     # There is no pointer in Numba.
     # Z,X,Y are dense tensors
     Z[idx] = X[idx] + Y[idx]


...
grid = (ceil_div(N, BLOCK),)
block = (BLOCK,)
add[grid, block](x, y, z, x.shape[0])
```

```python
BLOCK = 512

# This is a GPU kernel in Triton.
# Different instances of this
# function may run in parallel.
@jit
def add(X, Y, Z, N):
   # In Triton, each kernel instance
   # executes block operations on a
   # single thread: there is no construct
   # analogous to threadIdx
   pid = program_id(0)
   # block of indices
   idx = pid * BLOCK + arange(BLOCK)
   mask = idx < N
   # Triton uses pointer arithmetics  
   # rather than indexing operators
   x = load(X + idx, mask=mask)
   y = load(Y + idx, mask=mask)
   store(Z + idx, x + y, mask=mask)
...
grid = (ceil_div(N, BLOCK),)
# no thread-block
add[grid](x, y, z, x.shape[0])
```

虽然这可能对并行计算没有很大的帮助（如elementwise）但是确实可以一定程度上简化复杂的计算。

### Vector ADD 例子

为了能够给后面的学习打一个好的基础，首先来具体分析一下Vector Add，以它为例子，入门Triton的基本语法。

这也是Triton官方文档的tutorial中的第一个例子



## References

- [Introducing Triton: Open-source GPU programming for neural networks](https://openai.com/index/triton/)
- [Triton’s documentation!](https://triton-lang.org/main/index.html)
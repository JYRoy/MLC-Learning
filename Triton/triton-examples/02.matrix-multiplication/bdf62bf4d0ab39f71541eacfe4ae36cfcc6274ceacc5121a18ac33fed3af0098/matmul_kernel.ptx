//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_80
.address_size 64

	// .globl	matmul_kernel
.extern .shared .align 16 .b8 global_smem[];

.visible .entry matmul_kernel(
	.param .u64 matmul_kernel_param_0,
	.param .u64 matmul_kernel_param_1,
	.param .u64 matmul_kernel_param_2,
	.param .u32 matmul_kernel_param_3,
	.param .u32 matmul_kernel_param_4,
	.param .u32 matmul_kernel_param_5,
	.param .u32 matmul_kernel_param_6,
	.param .u32 matmul_kernel_param_7,
	.param .u32 matmul_kernel_param_8
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<118>;
	.reg .b16 	%rs<129>;
	.reg .b32 	%r<1312>;
	.reg .f32 	%f<898>;
	.reg .b64 	%rd<141>;
	.loc	1 260 0
$L__func_begin0:
	.loc	1 260 0

	ld.param.u32 	%r275, [matmul_kernel_param_8];
	ld.param.u32 	%r274, [matmul_kernel_param_5];
	ld.param.u32 	%r273, [matmul_kernel_param_4];
	ld.param.u32 	%r272, [matmul_kernel_param_3];
	ld.param.u64 	%rd32, [matmul_kernel_param_2];
	ld.param.u64 	%rd31, [matmul_kernel_param_1];
	ld.param.u64 	%rd30, [matmul_kernel_param_0];
$L__tmp0:
	.loc	1 292 24
	// begin inline asm
	mov.u32 %r276, %ctaid.x;
	// end inline asm
$L__tmp1:
	.loc	2 44 22
	add.s32 	%r441, %r272, 63;
	.loc	2 44 28
	shr.s32 	%r442, %r441, 31;
	shr.u32 	%r443, %r442, 26;
	add.s32 	%r444, %r441, %r443;
	shr.s32 	%r445, %r444, 6;
$L__tmp2:
	.loc	2 44 22
	add.s32 	%r446, %r273, 255;
	.loc	2 44 28
	shr.s32 	%r447, %r446, 31;
	shr.u32 	%r448, %r447, 24;
	add.s32 	%r449, %r446, %r448;
	shr.s32 	%r450, %r449, 8;
$L__tmp3:
	.loc	1 295 38
	shl.b32 	%r452, %r450, 3;
	ld.param.u32 	%r453, [matmul_kernel_param_6];
	ld.param.u32 	%r454, [matmul_kernel_param_7];
	.loc	1 296 22
	div.s32 	%r455, %r276, %r452;
	.loc	1 297 29
	shl.b32 	%r456, %r455, 3;
	.loc	1 299 20
	sub.s32 	%r457, %r445, %r456;
	.loc	1 299 33
	min.s32 	%r459, %r457, 8;
	mul.lo.s32 	%r460, %r455, %r452;
	sub.s32 	%r461, %r276, %r460;
	.loc	1 304 40
	div.s32 	%r462, %r461, %r459;
	mul.lo.s32 	%r463, %r462, %r459;
	sub.s32 	%r464, %r461, %r463;
	.loc	1 302 8
	add.s32 	%r465, %r464, %r456;
	.loc	1 313 23
	shl.b32 	%r1, %r465, 6;
	.loc	1 313 51
	mov.u32 	%r2, %tid.x;
	and.b32  	%r3, %r2, 31;
	bfe.u32 	%r466, %r2, 2, 5;
	or.b32  	%r467, %r466, 32;
	bfe.u32 	%r4, %r2, 5, 2;
	or.b32  	%r5, %r4, 4;
	or.b32  	%r6, %r4, 8;
	or.b32  	%r7, %r4, 12;
	or.b32  	%r8, %r4, 16;
	or.b32  	%r9, %r4, 20;
	or.b32  	%r10, %r4, 24;
	or.b32  	%r11, %r4, 28;
	.loc	1 313 38
	or.b32  	%r468, %r1, %r466;
	or.b32  	%r469, %r1, %r467;
	.loc	1 313 68
	rem.s32 	%r470, %r468, %r272;
	rem.s32 	%r471, %r469, %r272;
	.loc	1 314 23
	shl.b32 	%r472, %r462, 8;
	.loc	1 314 51
	shl.b32 	%r473, %r2, 3;
	and.b32  	%r12, %r473, 24;
	and.b32  	%r474, %r473, 248;
	.loc	1 314 38
	or.b32  	%r13, %r472, %r474;
	.loc	1 314 68
	rem.s32 	%r475, %r13, %r273;
	.loc	1 316 53
	mad.lo.s32 	%r476, %r470, %r453, %r12;
	mad.lo.s32 	%r477, %r471, %r453, %r12;
	.loc	1 316 22
	mul.wide.s32 	%rd63, %r476, 2;
	add.s64 	%rd33, %rd30, %rd63;
	mul.wide.s32 	%rd64, %r477, 2;
	add.s64 	%rd34, %rd30, %rd64;
	.loc	1 318 40
	shl.b32 	%r478, %r454, 2;
	.loc	1 318 52
	mad.lo.s32 	%r479, %r4, %r454, %r475;
	add.s32 	%r480, %r479, %r478;
	add.s32 	%r481, %r480, %r478;
	add.s32 	%r482, %r481, %r478;
	add.s32 	%r483, %r482, %r478;
	add.s32 	%r484, %r483, %r478;
	add.s32 	%r485, %r484, %r478;
	add.s32 	%r486, %r485, %r478;
	.loc	1 318 22
	mul.wide.s32 	%rd65, %r479, 2;
	add.s64 	%rd35, %rd31, %rd65;
	mul.wide.s32 	%rd66, %r480, 2;
	add.s64 	%rd36, %rd31, %rd66;
	mul.wide.s32 	%rd67, %r481, 2;
	add.s64 	%rd37, %rd31, %rd67;
	mul.wide.s32 	%rd68, %r482, 2;
	add.s64 	%rd38, %rd31, %rd68;
	mul.wide.s32 	%rd69, %r483, 2;
	add.s64 	%rd39, %rd31, %rd69;
	mul.wide.s32 	%rd70, %r484, 2;
	add.s64 	%rd40, %rd31, %rd70;
	mul.wide.s32 	%rd71, %r485, 2;
	add.s64 	%rd41, %rd31, %rd71;
	mul.wide.s32 	%rd72, %r486, 2;
	add.s64 	%rd42, %rd31, %rd72;
$L__tmp4:
	.loc	2 44 22
	add.s32 	%r487, %r274, 31;
$L__tmp5:
	.loc	1 336 33
	shl.b32 	%r491, %r454, 5;
	.loc	1 327 22
	setp.lt.s32 	%p31, %r487, 32;
	setp.gt.s32 	%p32, %r487, 31;
	.loc	1 330 51
	setp.lt.s32 	%p33, %r12, %r274;
	.loc	1 330 20
	shl.b32 	%r492, %r466, 5;
	xor.b32  	%r493, %r473, %r2;
	and.b32  	%r494, %r493, 24;
	or.b32  	%r15, %r492, %r494;
	shl.b32 	%r495, %r15, 1;
	mov.u32 	%r496, global_smem;
	add.s32 	%r277, %r496, %r495;
	shl.b32 	%r497, %r467, 5;
	or.b32  	%r16, %r497, %r494;
	shl.b32 	%r498, %r16, 1;
	add.s32 	%r279, %r496, %r498;
	selp.b32 	%r499, 16, 0, %p32;
	selp.b32 	%r280, %r499, 0, %p33;
	mov.pred 	%p1, -1;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r277 + 0 ], [ %rd33 + 0 ], 0x10, %r280;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r279 + 0 ], [ %rd34 + 0 ], 0x10, %r280;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 51
	setp.lt.s32 	%p34, %r4, %r274;
	setp.lt.s32 	%p35, %r5, %r274;
	setp.lt.s32 	%p36, %r6, %r274;
	setp.lt.s32 	%p37, %r7, %r274;
	setp.lt.s32 	%p38, %r8, %r274;
	setp.lt.s32 	%p39, %r9, %r274;
	setp.lt.s32 	%p40, %r10, %r274;
	setp.lt.s32 	%p41, %r11, %r274;
	.loc	1 331 20
	shl.b32 	%r500, %r4, 8;
	bfe.u32 	%r501, %r473, 3, 5;
	xor.b32  	%r502, %r501, %r4;
	shl.b32 	%r503, %r502, 3;
	or.b32  	%r17, %r503, %r500;
	shl.b32 	%r504, %r17, 1;
	add.s32 	%r505, %r496, 12288;
	add.s32 	%r281, %r505, %r504;
	shl.b32 	%r506, %r5, 8;
	xor.b32  	%r507, %r501, %r5;
	shl.b32 	%r508, %r507, 3;
	or.b32  	%r18, %r508, %r506;
	shl.b32 	%r509, %r18, 1;
	add.s32 	%r283, %r505, %r509;
	shl.b32 	%r510, %r6, 8;
	or.b32  	%r19, %r503, %r510;
	shl.b32 	%r511, %r19, 1;
	add.s32 	%r285, %r505, %r511;
	shl.b32 	%r512, %r7, 8;
	or.b32  	%r20, %r508, %r512;
	shl.b32 	%r513, %r20, 1;
	add.s32 	%r287, %r505, %r513;
	shl.b32 	%r514, %r8, 8;
	or.b32  	%r21, %r503, %r514;
	shl.b32 	%r515, %r21, 1;
	add.s32 	%r289, %r505, %r515;
	shl.b32 	%r516, %r9, 8;
	or.b32  	%r22, %r508, %r516;
	shl.b32 	%r517, %r22, 1;
	add.s32 	%r291, %r505, %r517;
	shl.b32 	%r518, %r10, 8;
	or.b32  	%r23, %r503, %r518;
	shl.b32 	%r519, %r23, 1;
	add.s32 	%r293, %r505, %r519;
	shl.b32 	%r520, %r11, 8;
	or.b32  	%r24, %r508, %r520;
	shl.b32 	%r521, %r24, 1;
	add.s32 	%r295, %r505, %r521;
	selp.b32 	%r282, %r499, 0, %p34;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r281 + 0 ], [ %rd35 + 0 ], 0x10, %r282;
	// end inline asm
	selp.b32 	%r284, %r499, 0, %p35;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r283 + 0 ], [ %rd36 + 0 ], 0x10, %r284;
	// end inline asm
	selp.b32 	%r286, %r499, 0, %p36;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r285 + 0 ], [ %rd37 + 0 ], 0x10, %r286;
	// end inline asm
	selp.b32 	%r288, %r499, 0, %p37;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r287 + 0 ], [ %rd38 + 0 ], 0x10, %r288;
	// end inline asm
	selp.b32 	%r290, %r499, 0, %p38;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r289 + 0 ], [ %rd39 + 0 ], 0x10, %r290;
	// end inline asm
	selp.b32 	%r292, %r499, 0, %p39;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r291 + 0 ], [ %rd40 + 0 ], 0x10, %r292;
	// end inline asm
	selp.b32 	%r294, %r499, 0, %p40;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r293 + 0 ], [ %rd41 + 0 ], 0x10, %r294;
	// end inline asm
	selp.b32 	%r296, %r499, 0, %p41;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r295 + 0 ], [ %rd42 + 0 ], 0x10, %r296;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 327 22
	setp.gt.s32 	%p42, %r487, 63;
	.loc	1 335 18
	add.s64 	%rd43, %rd33, 64;
	add.s64 	%rd44, %rd34, 64;
	.loc	1 336 18
	mul.wide.s32 	%rd73, %r491, 2;
	add.s64 	%rd45, %rd35, %rd73;
	add.s64 	%rd46, %rd36, %rd73;
	add.s64 	%rd47, %rd37, %rd73;
	add.s64 	%rd48, %rd38, %rd73;
	add.s64 	%rd49, %rd39, %rd73;
	add.s64 	%rd50, %rd40, %rd73;
	add.s64 	%rd51, %rd41, %rd73;
	add.s64 	%rd52, %rd42, %rd73;
	.loc	1 330 55
	add.s32 	%r522, %r274, -32;
	.loc	1 330 51
	setp.lt.s32 	%p43, %r12, %r522;
	.loc	1 330 20
	bar.sync 	0;
	add.s32 	%r523, %r496, 4096;
	add.s32 	%r297, %r523, %r495;
	add.s32 	%r299, %r523, %r498;
	selp.b32 	%r524, 16, 0, %p43;
	selp.b32 	%r300, %r524, 0, %p42;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r297 + 0 ], [ %rd43 + 0 ], 0x10, %r300;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r299 + 0 ], [ %rd44 + 0 ], 0x10, %r300;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 51
	setp.lt.s32 	%p44, %r4, %r522;
	setp.lt.s32 	%p45, %r5, %r522;
	setp.lt.s32 	%p46, %r6, %r522;
	setp.lt.s32 	%p47, %r7, %r522;
	setp.lt.s32 	%p48, %r8, %r522;
	setp.lt.s32 	%p49, %r9, %r522;
	setp.lt.s32 	%p50, %r10, %r522;
	setp.lt.s32 	%p51, %r11, %r522;
	.loc	1 331 20
	add.s32 	%r525, %r496, 28672;
	add.s32 	%r301, %r525, %r504;
	add.s32 	%r303, %r525, %r509;
	add.s32 	%r305, %r525, %r511;
	add.s32 	%r307, %r525, %r513;
	add.s32 	%r309, %r525, %r515;
	add.s32 	%r311, %r525, %r517;
	add.s32 	%r313, %r525, %r519;
	add.s32 	%r315, %r525, %r521;
	selp.b32 	%r526, 16, 0, %p44;
	selp.b32 	%r302, %r526, 0, %p42;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r301 + 0 ], [ %rd45 + 0 ], 0x10, %r302;
	// end inline asm
	selp.b32 	%r527, 16, 0, %p45;
	selp.b32 	%r304, %r527, 0, %p42;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r303 + 0 ], [ %rd46 + 0 ], 0x10, %r304;
	// end inline asm
	selp.b32 	%r528, 16, 0, %p46;
	selp.b32 	%r306, %r528, 0, %p42;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r305 + 0 ], [ %rd47 + 0 ], 0x10, %r306;
	// end inline asm
	selp.b32 	%r529, 16, 0, %p47;
	selp.b32 	%r308, %r529, 0, %p42;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r307 + 0 ], [ %rd48 + 0 ], 0x10, %r308;
	// end inline asm
	selp.b32 	%r530, 16, 0, %p48;
	selp.b32 	%r310, %r530, 0, %p42;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r309 + 0 ], [ %rd49 + 0 ], 0x10, %r310;
	// end inline asm
	selp.b32 	%r531, 16, 0, %p49;
	selp.b32 	%r312, %r531, 0, %p42;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r311 + 0 ], [ %rd50 + 0 ], 0x10, %r312;
	// end inline asm
	selp.b32 	%r532, 16, 0, %p50;
	selp.b32 	%r314, %r532, 0, %p42;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r313 + 0 ], [ %rd51 + 0 ], 0x10, %r314;
	// end inline asm
	selp.b32 	%r533, 16, 0, %p51;
	selp.b32 	%r316, %r533, 0, %p42;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r315 + 0 ], [ %rd52 + 0 ], 0x10, %r316;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 327 22
	setp.gt.s32 	%p52, %r487, 95;
	.loc	1 335 18
	add.s64 	%rd53, %rd33, 128;
	add.s64 	%rd54, %rd34, 128;
	.loc	1 336 18
	add.s64 	%rd55, %rd45, %rd73;
	add.s64 	%rd56, %rd46, %rd73;
	add.s64 	%rd57, %rd47, %rd73;
	add.s64 	%rd58, %rd48, %rd73;
	add.s64 	%rd59, %rd49, %rd73;
	add.s64 	%rd60, %rd50, %rd73;
	add.s64 	%rd61, %rd51, %rd73;
	add.s64 	%rd62, %rd52, %rd73;
	.loc	1 330 55
	add.s32 	%r534, %r274, -64;
	.loc	1 330 51
	setp.lt.s32 	%p53, %r12, %r534;
	.loc	1 330 20
	bar.sync 	0;
	add.s32 	%r535, %r496, 8192;
	add.s32 	%r317, %r535, %r495;
	add.s32 	%r319, %r535, %r498;
	selp.b32 	%r536, 16, 0, %p53;
	selp.b32 	%r320, %r536, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r317 + 0 ], [ %rd53 + 0 ], 0x10, %r320;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r319 + 0 ], [ %rd54 + 0 ], 0x10, %r320;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 51
	setp.lt.s32 	%p54, %r4, %r534;
	setp.lt.s32 	%p55, %r5, %r534;
	setp.lt.s32 	%p56, %r6, %r534;
	setp.lt.s32 	%p57, %r7, %r534;
	setp.lt.s32 	%p58, %r8, %r534;
	setp.lt.s32 	%p59, %r9, %r534;
	setp.lt.s32 	%p60, %r10, %r534;
	setp.lt.s32 	%p61, %r11, %r534;
	.loc	1 331 20
	add.s32 	%r537, %r496, 45056;
	add.s32 	%r321, %r537, %r504;
	add.s32 	%r323, %r537, %r509;
	add.s32 	%r325, %r537, %r511;
	add.s32 	%r327, %r537, %r513;
	add.s32 	%r329, %r537, %r515;
	add.s32 	%r331, %r537, %r517;
	add.s32 	%r333, %r537, %r519;
	add.s32 	%r335, %r537, %r521;
	selp.b32 	%r538, 16, 0, %p54;
	selp.b32 	%r322, %r538, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r321 + 0 ], [ %rd55 + 0 ], 0x10, %r322;
	// end inline asm
	selp.b32 	%r539, 16, 0, %p55;
	selp.b32 	%r324, %r539, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r323 + 0 ], [ %rd56 + 0 ], 0x10, %r324;
	// end inline asm
	selp.b32 	%r540, 16, 0, %p56;
	selp.b32 	%r326, %r540, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r325 + 0 ], [ %rd57 + 0 ], 0x10, %r326;
	// end inline asm
	selp.b32 	%r541, 16, 0, %p57;
	selp.b32 	%r328, %r541, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r327 + 0 ], [ %rd58 + 0 ], 0x10, %r328;
	// end inline asm
	selp.b32 	%r542, 16, 0, %p58;
	selp.b32 	%r330, %r542, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r329 + 0 ], [ %rd59 + 0 ], 0x10, %r330;
	// end inline asm
	selp.b32 	%r543, 16, 0, %p59;
	selp.b32 	%r332, %r543, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r331 + 0 ], [ %rd60 + 0 ], 0x10, %r332;
	// end inline asm
	selp.b32 	%r544, 16, 0, %p60;
	selp.b32 	%r334, %r544, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r333 + 0 ], [ %rd61 + 0 ], 0x10, %r334;
	// end inline asm
	selp.b32 	%r545, 16, 0, %p61;
	selp.b32 	%r336, %r545, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r335 + 0 ], [ %rd62 + 0 ], 0x10, %r336;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 330 20
	// begin inline asm
	cp.async.wait_group 0x4;
	// end inline asm
	bar.sync 	0;
	and.b32  	%r546, %r2, 7;
	bfe.u32 	%r25, %r2, 4, 1;
	bfe.u32 	%r26, %r2, 1, 2;
	and.b32  	%r547, %r2, 15;
	xor.b32  	%r548, %r25, %r26;
	shl.b32 	%r27, %r547, 5;
	shl.b32 	%r549, %r548, 3;
	or.b32  	%r28, %r549, %r27;
	shl.b32 	%r550, %r28, 1;
	add.s32 	%r341, %r496, %r550;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1239, %r1240, %r1241, %r1242 }, [ %r341 + 0 ];
	// end inline asm
	add.s32 	%r346, %r341, 1024;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1235, %r1236, %r1237, %r1238 }, [ %r346 + 0 ];
	// end inline asm
	add.s32 	%r351, %r341, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1231, %r1232, %r1233, %r1234 }, [ %r351 + 0 ];
	// end inline asm
	add.s32 	%r356, %r341, 3072;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1227, %r1228, %r1229, %r1230 }, [ %r356 + 0 ];
	// end inline asm
	.loc	1 331 20
	shl.b32 	%r551, %r25, 2;
	or.b32  	%r552, %r551, %r4;
	xor.b32  	%r553, %r552, %r546;
	shl.b32 	%r554, %r547, 8;
	shl.b32 	%r555, %r553, 3;
	or.b32  	%r45, %r555, %r554;
	shl.b32 	%r556, %r45, 1;
	add.s32 	%r361, %r505, %r556;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1223, %r1224, %r1225, %r1226 }, [ %r361 + 0 ];
	// end inline asm
	or.b32  	%r557, %r552, 8;
	xor.b32  	%r558, %r557, %r546;
	shl.b32 	%r559, %r558, 3;
	add.s32 	%r50, %r559, %r554;
	shl.b32 	%r560, %r50, 1;
	add.s32 	%r366, %r505, %r560;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1219, %r1220, %r1221, %r1222 }, [ %r366 + 0 ];
	// end inline asm
	or.b32  	%r561, %r552, 16;
	xor.b32  	%r562, %r561, %r546;
	shl.b32 	%r563, %r562, 3;
	add.s32 	%r55, %r563, %r554;
	shl.b32 	%r564, %r55, 1;
	add.s32 	%r371, %r505, %r564;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1215, %r1216, %r1217, %r1218 }, [ %r371 + 0 ];
	// end inline asm
	or.b32  	%r565, %r552, 24;
	xor.b32  	%r566, %r565, %r546;
	shl.b32 	%r567, %r566, 3;
	add.s32 	%r60, %r567, %r554;
	shl.b32 	%r568, %r60, 1;
	add.s32 	%r376, %r505, %r568;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1211, %r1212, %r1213, %r1214 }, [ %r376 + 0 ];
	// end inline asm
	mov.b32 	%r1248, 0;
	mov.u32 	%r1249, %r1248;
	mov.u32 	%r1250, %r1248;
	mov.u32 	%r1251, %r1248;
	mov.u32 	%r1252, %r1248;
	mov.u32 	%r1253, %r1248;
	mov.u32 	%r1254, %r1248;
	mov.u32 	%r1255, %r1248;
	mov.u32 	%r1256, %r1248;
	mov.u32 	%r1257, %r1248;
	mov.u32 	%r1258, %r1248;
	mov.u32 	%r1259, %r1248;
	mov.u32 	%r1260, %r1248;
	mov.u32 	%r1261, %r1248;
	mov.u32 	%r1262, %r1248;
	mov.u32 	%r1263, %r1248;
	mov.u32 	%r1264, %r1248;
	mov.u32 	%r1265, %r1248;
	mov.u32 	%r1266, %r1248;
	mov.u32 	%r1267, %r1248;
	mov.u32 	%r1268, %r1248;
	mov.u32 	%r1269, %r1248;
	mov.u32 	%r1270, %r1248;
	mov.u32 	%r1271, %r1248;
	mov.u32 	%r1272, %r1248;
	mov.u32 	%r1273, %r1248;
	mov.u32 	%r1274, %r1248;
	mov.u32 	%r1275, %r1248;
	mov.u32 	%r1276, %r1248;
	mov.u32 	%r1277, %r1248;
	mov.u32 	%r1278, %r1248;
	mov.u32 	%r1279, %r1248;
	mov.u32 	%r1280, %r1248;
	mov.u32 	%r1281, %r1248;
	mov.u32 	%r1282, %r1248;
	mov.u32 	%r1283, %r1248;
	mov.u32 	%r1284, %r1248;
	mov.u32 	%r1285, %r1248;
	mov.u32 	%r1286, %r1248;
	mov.u32 	%r1287, %r1248;
	mov.u32 	%r1288, %r1248;
	mov.u32 	%r1289, %r1248;
	mov.u32 	%r1290, %r1248;
	mov.u32 	%r1291, %r1248;
	mov.u32 	%r1292, %r1248;
	mov.u32 	%r1293, %r1248;
	mov.u32 	%r1294, %r1248;
	mov.u32 	%r1295, %r1248;
	mov.u32 	%r1296, %r1248;
	mov.u32 	%r1297, %r1248;
	mov.u32 	%r1298, %r1248;
	mov.u32 	%r1299, %r1248;
	mov.u32 	%r1300, %r1248;
	mov.u32 	%r1301, %r1248;
	mov.u32 	%r1302, %r1248;
	mov.u32 	%r1303, %r1248;
	mov.u32 	%r1304, %r1248;
	mov.u32 	%r1305, %r1248;
	mov.u32 	%r1306, %r1248;
	mov.u32 	%r1307, %r1248;
	mov.u32 	%r1308, %r1248;
	mov.u32 	%r1309, %r1248;
	mov.u32 	%r1310, %r1248;
	mov.u32 	%r1311, %r1248;
	.loc	1 327 22
	@%p31 bra 	$L__BB0_4;
	.loc	1 0 22
	cvt.s64.s32 	%rd1, %r476;
	cvt.s64.s32 	%rd2, %r477;
	cvt.s64.s32 	%rd3, %r479;
	cvt.s64.s32 	%rd4, %r480;
	cvt.s64.s32 	%rd5, %r481;
	cvt.s64.s32 	%rd6, %r482;
	cvt.s64.s32 	%rd7, %r483;
	cvt.s64.s32 	%rd8, %r484;
	cvt.s64.s32 	%rd9, %r485;
	cvt.s64.s32 	%rd10, %r486;
	shr.s32 	%r488, %r487, 31;
	shr.u32 	%r489, %r488, 27;
	add.s32 	%r490, %r487, %r489;
	shr.s32 	%r14, %r490, 5;
	cvt.s64.s32 	%rd11, %r491;
	add.s32 	%r65, %r14, -3;
	or.b32  	%r573, %r25, 2;
	xor.b32  	%r574, %r573, %r26;
	shl.b32 	%r575, %r574, 3;
	add.s32 	%r1210, %r274, -96;
	or.b32  	%r67, %r27, %r575;
	.loc	1 327 22
	shl.b64 	%rd12, %rd10, 1;
	mul.lo.s64 	%rd74, %rd11, 6;
	add.s64 	%rd140, %rd31, %rd74;
	shl.b64 	%rd14, %rd11, 1;
	shl.b64 	%rd15, %rd9, 1;
	shl.b64 	%rd16, %rd8, 1;
	shl.b64 	%rd17, %rd7, 1;
	shl.b64 	%rd18, %rd6, 1;
	shl.b64 	%rd19, %rd5, 1;
	shl.b64 	%rd20, %rd4, 1;
	shl.b64 	%rd21, %rd3, 1;
	shl.b64 	%rd75, %rd2, 1;
	add.s64 	%rd76, %rd75, %rd30;
	add.s64 	%rd139, %rd76, 192;
	shl.b64 	%rd77, %rd1, 1;
	add.s64 	%rd78, %rd77, %rd30;
	add.s64 	%rd138, %rd78, 192;
	mov.f32 	%f770, 0f00000000;
	mov.b32 	%r1246, 2;
	mov.b32 	%r1245, 0;
	shl.b32 	%r1060, %r67, 1;
	mov.u32 	%r1243, %r505;
	mov.u32 	%r1244, %r496;
	mov.f32 	%f771, %f770;
	mov.f32 	%f772, %f770;
	mov.f32 	%f773, %f770;
	mov.f32 	%f774, %f770;
	mov.f32 	%f775, %f770;
	mov.f32 	%f776, %f770;
	mov.f32 	%f777, %f770;
	mov.f32 	%f778, %f770;
	mov.f32 	%f779, %f770;
	mov.f32 	%f780, %f770;
	mov.f32 	%f781, %f770;
	mov.f32 	%f782, %f770;
	mov.f32 	%f783, %f770;
	mov.f32 	%f784, %f770;
	mov.f32 	%f785, %f770;
	mov.f32 	%f786, %f770;
	mov.f32 	%f787, %f770;
	mov.f32 	%f788, %f770;
	mov.f32 	%f789, %f770;
	mov.f32 	%f790, %f770;
	mov.f32 	%f791, %f770;
	mov.f32 	%f792, %f770;
	mov.f32 	%f793, %f770;
	mov.f32 	%f794, %f770;
	mov.f32 	%f795, %f770;
	mov.f32 	%f796, %f770;
	mov.f32 	%f797, %f770;
	mov.f32 	%f798, %f770;
	mov.f32 	%f799, %f770;
	mov.f32 	%f800, %f770;
	mov.f32 	%f801, %f770;
	mov.f32 	%f802, %f770;
	mov.f32 	%f803, %f770;
	mov.f32 	%f804, %f770;
	mov.f32 	%f805, %f770;
	mov.f32 	%f806, %f770;
	mov.f32 	%f807, %f770;
	mov.f32 	%f808, %f770;
	mov.f32 	%f809, %f770;
	mov.f32 	%f810, %f770;
	mov.f32 	%f811, %f770;
	mov.f32 	%f812, %f770;
	mov.f32 	%f813, %f770;
	mov.f32 	%f814, %f770;
	mov.f32 	%f815, %f770;
	mov.f32 	%f816, %f770;
	mov.f32 	%f817, %f770;
	mov.f32 	%f818, %f770;
	mov.f32 	%f819, %f770;
	mov.f32 	%f820, %f770;
	mov.f32 	%f821, %f770;
	mov.f32 	%f822, %f770;
	mov.f32 	%f823, %f770;
	mov.f32 	%f824, %f770;
	mov.f32 	%f825, %f770;
	mov.f32 	%f826, %f770;
	mov.f32 	%f827, %f770;
	mov.f32 	%f828, %f770;
	mov.f32 	%f829, %f770;
	mov.f32 	%f830, %f770;
	mov.f32 	%f831, %f770;
	mov.f32 	%f832, %f770;
	mov.f32 	%f833, %f770;
	mov.f32 	%f834, %f770;
	mov.f32 	%f835, %f770;
	mov.f32 	%f836, %f770;
	mov.f32 	%f837, %f770;
	mov.f32 	%f838, %f770;
	mov.f32 	%f839, %f770;
	mov.f32 	%f840, %f770;
	mov.f32 	%f841, %f770;
	mov.f32 	%f842, %f770;
	mov.f32 	%f843, %f770;
	mov.f32 	%f844, %f770;
	mov.f32 	%f845, %f770;
	mov.f32 	%f846, %f770;
	mov.f32 	%f847, %f770;
	mov.f32 	%f848, %f770;
	mov.f32 	%f849, %f770;
	mov.f32 	%f850, %f770;
	mov.f32 	%f851, %f770;
	mov.f32 	%f852, %f770;
	mov.f32 	%f853, %f770;
	mov.f32 	%f854, %f770;
	mov.f32 	%f855, %f770;
	mov.f32 	%f856, %f770;
	mov.f32 	%f857, %f770;
	mov.f32 	%f858, %f770;
	mov.f32 	%f859, %f770;
	mov.f32 	%f860, %f770;
	mov.f32 	%f861, %f770;
	mov.f32 	%f862, %f770;
	mov.f32 	%f863, %f770;
	mov.f32 	%f864, %f770;
	mov.f32 	%f865, %f770;
	mov.f32 	%f866, %f770;
	mov.f32 	%f867, %f770;
	mov.f32 	%f868, %f770;
	mov.f32 	%f869, %f770;
	mov.f32 	%f870, %f770;
	mov.f32 	%f871, %f770;
	mov.f32 	%f872, %f770;
	mov.f32 	%f873, %f770;
	mov.f32 	%f874, %f770;
	mov.f32 	%f875, %f770;
	mov.f32 	%f876, %f770;
	mov.f32 	%f877, %f770;
	mov.f32 	%f878, %f770;
	mov.f32 	%f879, %f770;
	mov.f32 	%f880, %f770;
	mov.f32 	%f881, %f770;
	mov.f32 	%f882, %f770;
	mov.f32 	%f883, %f770;
	mov.f32 	%f884, %f770;
	mov.f32 	%f885, %f770;
	mov.f32 	%f886, %f770;
	mov.f32 	%f887, %f770;
	mov.f32 	%f888, %f770;
	mov.f32 	%f889, %f770;
	mov.f32 	%f890, %f770;
	mov.f32 	%f891, %f770;
	mov.f32 	%f892, %f770;
	mov.f32 	%f893, %f770;
	mov.f32 	%f894, %f770;
	mov.f32 	%f895, %f770;
	mov.f32 	%f896, %f770;
	mov.f32 	%f897, %f770;
	mov.u32 	%r1247, %r1245;
$L__BB0_2:
	setp.lt.s32 	%p72, %r1247, %r65;
	.loc	1 330 20
	add.s32 	%r580, %r1244, %r1060;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r808, %r809, %r810, %r811 }, [ %r580 + 0 ];
	// end inline asm
	add.s32 	%r585, %r580, 1024;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r856, %r857, %r858, %r859 }, [ %r585 + 0 ];
	// end inline asm
	add.s32 	%r590, %r580, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r904, %r905, %r906, %r907 }, [ %r590 + 0 ];
	// end inline asm
	add.s32 	%r595, %r580, 3072;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r952, %r953, %r954, %r955 }, [ %r595 + 0 ];
	// end inline asm
	.loc	1 331 20
	add.s32 	%r1062, %r1243, %r556;
	add.s32 	%r600, %r1062, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r812, %r813, %r818, %r819 }, [ %r600 + 0 ];
	// end inline asm
	add.s32 	%r1064, %r1243, %r560;
	add.s32 	%r605, %r1064, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r824, %r825, %r830, %r831 }, [ %r605 + 0 ];
	// end inline asm
	add.s32 	%r1066, %r1243, %r564;
	add.s32 	%r610, %r1066, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r836, %r837, %r842, %r843 }, [ %r610 + 0 ];
	// end inline asm
	add.s32 	%r1068, %r1243, %r568;
	add.s32 	%r615, %r1068, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r848, %r849, %r854, %r855 }, [ %r615 + 0 ];
	// end inline asm
	.loc	1 333 35
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f770, %f771, %f772, %f773 }, { %r1239, %r1240, %r1241, %r1242 }, { %r1223, %r1224 }, { %f770, %f771, %f772, %f773 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f774, %f775, %f776, %f777 }, { %r1239, %r1240, %r1241, %r1242 }, { %r1225, %r1226 }, { %f774, %f775, %f776, %f777 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f778, %f779, %f780, %f781 }, { %r1239, %r1240, %r1241, %r1242 }, { %r1219, %r1220 }, { %f778, %f779, %f780, %f781 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f782, %f783, %f784, %f785 }, { %r1239, %r1240, %r1241, %r1242 }, { %r1221, %r1222 }, { %f782, %f783, %f784, %f785 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f786, %f787, %f788, %f789 }, { %r1239, %r1240, %r1241, %r1242 }, { %r1215, %r1216 }, { %f786, %f787, %f788, %f789 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f790, %f791, %f792, %f793 }, { %r1239, %r1240, %r1241, %r1242 }, { %r1217, %r1218 }, { %f790, %f791, %f792, %f793 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f794, %f795, %f796, %f797 }, { %r1239, %r1240, %r1241, %r1242 }, { %r1211, %r1212 }, { %f794, %f795, %f796, %f797 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f798, %f799, %f800, %f801 }, { %r1239, %r1240, %r1241, %r1242 }, { %r1213, %r1214 }, { %f798, %f799, %f800, %f801 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f802, %f803, %f804, %f805 }, { %r1235, %r1236, %r1237, %r1238 }, { %r1223, %r1224 }, { %f802, %f803, %f804, %f805 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f806, %f807, %f808, %f809 }, { %r1235, %r1236, %r1237, %r1238 }, { %r1225, %r1226 }, { %f806, %f807, %f808, %f809 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f810, %f811, %f812, %f813 }, { %r1235, %r1236, %r1237, %r1238 }, { %r1219, %r1220 }, { %f810, %f811, %f812, %f813 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f814, %f815, %f816, %f817 }, { %r1235, %r1236, %r1237, %r1238 }, { %r1221, %r1222 }, { %f814, %f815, %f816, %f817 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f818, %f819, %f820, %f821 }, { %r1235, %r1236, %r1237, %r1238 }, { %r1215, %r1216 }, { %f818, %f819, %f820, %f821 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f822, %f823, %f824, %f825 }, { %r1235, %r1236, %r1237, %r1238 }, { %r1217, %r1218 }, { %f822, %f823, %f824, %f825 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f826, %f827, %f828, %f829 }, { %r1235, %r1236, %r1237, %r1238 }, { %r1211, %r1212 }, { %f826, %f827, %f828, %f829 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f830, %f831, %f832, %f833 }, { %r1235, %r1236, %r1237, %r1238 }, { %r1213, %r1214 }, { %f830, %f831, %f832, %f833 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f834, %f835, %f836, %f837 }, { %r1231, %r1232, %r1233, %r1234 }, { %r1223, %r1224 }, { %f834, %f835, %f836, %f837 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f838, %f839, %f840, %f841 }, { %r1231, %r1232, %r1233, %r1234 }, { %r1225, %r1226 }, { %f838, %f839, %f840, %f841 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f842, %f843, %f844, %f845 }, { %r1231, %r1232, %r1233, %r1234 }, { %r1219, %r1220 }, { %f842, %f843, %f844, %f845 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f846, %f847, %f848, %f849 }, { %r1231, %r1232, %r1233, %r1234 }, { %r1221, %r1222 }, { %f846, %f847, %f848, %f849 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f850, %f851, %f852, %f853 }, { %r1231, %r1232, %r1233, %r1234 }, { %r1215, %r1216 }, { %f850, %f851, %f852, %f853 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f854, %f855, %f856, %f857 }, { %r1231, %r1232, %r1233, %r1234 }, { %r1217, %r1218 }, { %f854, %f855, %f856, %f857 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f858, %f859, %f860, %f861 }, { %r1231, %r1232, %r1233, %r1234 }, { %r1211, %r1212 }, { %f858, %f859, %f860, %f861 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f862, %f863, %f864, %f865 }, { %r1231, %r1232, %r1233, %r1234 }, { %r1213, %r1214 }, { %f862, %f863, %f864, %f865 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f866, %f867, %f868, %f869 }, { %r1227, %r1228, %r1229, %r1230 }, { %r1223, %r1224 }, { %f866, %f867, %f868, %f869 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f870, %f871, %f872, %f873 }, { %r1227, %r1228, %r1229, %r1230 }, { %r1225, %r1226 }, { %f870, %f871, %f872, %f873 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f874, %f875, %f876, %f877 }, { %r1227, %r1228, %r1229, %r1230 }, { %r1219, %r1220 }, { %f874, %f875, %f876, %f877 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f878, %f879, %f880, %f881 }, { %r1227, %r1228, %r1229, %r1230 }, { %r1221, %r1222 }, { %f878, %f879, %f880, %f881 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f882, %f883, %f884, %f885 }, { %r1227, %r1228, %r1229, %r1230 }, { %r1215, %r1216 }, { %f882, %f883, %f884, %f885 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f886, %f887, %f888, %f889 }, { %r1227, %r1228, %r1229, %r1230 }, { %r1217, %r1218 }, { %f886, %f887, %f888, %f889 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f890, %f891, %f892, %f893 }, { %r1227, %r1228, %r1229, %r1230 }, { %r1211, %r1212 }, { %f890, %f891, %f892, %f893 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f894, %f895, %f896, %f897 }, { %r1227, %r1228, %r1229, %r1230 }, { %r1213, %r1214 }, { %f894, %f895, %f896, %f897 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f770, %f771, %f772, %f773 }, { %r808, %r809, %r810, %r811 }, { %r812, %r813 }, { %f770, %f771, %f772, %f773 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f774, %f775, %f776, %f777 }, { %r808, %r809, %r810, %r811 }, { %r818, %r819 }, { %f774, %f775, %f776, %f777 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f778, %f779, %f780, %f781 }, { %r808, %r809, %r810, %r811 }, { %r824, %r825 }, { %f778, %f779, %f780, %f781 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f782, %f783, %f784, %f785 }, { %r808, %r809, %r810, %r811 }, { %r830, %r831 }, { %f782, %f783, %f784, %f785 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f786, %f787, %f788, %f789 }, { %r808, %r809, %r810, %r811 }, { %r836, %r837 }, { %f786, %f787, %f788, %f789 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f790, %f791, %f792, %f793 }, { %r808, %r809, %r810, %r811 }, { %r842, %r843 }, { %f790, %f791, %f792, %f793 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f794, %f795, %f796, %f797 }, { %r808, %r809, %r810, %r811 }, { %r848, %r849 }, { %f794, %f795, %f796, %f797 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f798, %f799, %f800, %f801 }, { %r808, %r809, %r810, %r811 }, { %r854, %r855 }, { %f798, %f799, %f800, %f801 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f802, %f803, %f804, %f805 }, { %r856, %r857, %r858, %r859 }, { %r812, %r813 }, { %f802, %f803, %f804, %f805 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f806, %f807, %f808, %f809 }, { %r856, %r857, %r858, %r859 }, { %r818, %r819 }, { %f806, %f807, %f808, %f809 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f810, %f811, %f812, %f813 }, { %r856, %r857, %r858, %r859 }, { %r824, %r825 }, { %f810, %f811, %f812, %f813 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f814, %f815, %f816, %f817 }, { %r856, %r857, %r858, %r859 }, { %r830, %r831 }, { %f814, %f815, %f816, %f817 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f818, %f819, %f820, %f821 }, { %r856, %r857, %r858, %r859 }, { %r836, %r837 }, { %f818, %f819, %f820, %f821 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f822, %f823, %f824, %f825 }, { %r856, %r857, %r858, %r859 }, { %r842, %r843 }, { %f822, %f823, %f824, %f825 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f826, %f827, %f828, %f829 }, { %r856, %r857, %r858, %r859 }, { %r848, %r849 }, { %f826, %f827, %f828, %f829 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f830, %f831, %f832, %f833 }, { %r856, %r857, %r858, %r859 }, { %r854, %r855 }, { %f830, %f831, %f832, %f833 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f834, %f835, %f836, %f837 }, { %r904, %r905, %r906, %r907 }, { %r812, %r813 }, { %f834, %f835, %f836, %f837 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f838, %f839, %f840, %f841 }, { %r904, %r905, %r906, %r907 }, { %r818, %r819 }, { %f838, %f839, %f840, %f841 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f842, %f843, %f844, %f845 }, { %r904, %r905, %r906, %r907 }, { %r824, %r825 }, { %f842, %f843, %f844, %f845 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f846, %f847, %f848, %f849 }, { %r904, %r905, %r906, %r907 }, { %r830, %r831 }, { %f846, %f847, %f848, %f849 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f850, %f851, %f852, %f853 }, { %r904, %r905, %r906, %r907 }, { %r836, %r837 }, { %f850, %f851, %f852, %f853 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f854, %f855, %f856, %f857 }, { %r904, %r905, %r906, %r907 }, { %r842, %r843 }, { %f854, %f855, %f856, %f857 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f858, %f859, %f860, %f861 }, { %r904, %r905, %r906, %r907 }, { %r848, %r849 }, { %f858, %f859, %f860, %f861 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f862, %f863, %f864, %f865 }, { %r904, %r905, %r906, %r907 }, { %r854, %r855 }, { %f862, %f863, %f864, %f865 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f866, %f867, %f868, %f869 }, { %r952, %r953, %r954, %r955 }, { %r812, %r813 }, { %f866, %f867, %f868, %f869 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f870, %f871, %f872, %f873 }, { %r952, %r953, %r954, %r955 }, { %r818, %r819 }, { %f870, %f871, %f872, %f873 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f874, %f875, %f876, %f877 }, { %r952, %r953, %r954, %r955 }, { %r824, %r825 }, { %f874, %f875, %f876, %f877 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f878, %f879, %f880, %f881 }, { %r952, %r953, %r954, %r955 }, { %r830, %r831 }, { %f878, %f879, %f880, %f881 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f882, %f883, %f884, %f885 }, { %r952, %r953, %r954, %r955 }, { %r836, %r837 }, { %f882, %f883, %f884, %f885 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f886, %f887, %f888, %f889 }, { %r952, %r953, %r954, %r955 }, { %r842, %r843 }, { %f886, %f887, %f888, %f889 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f890, %f891, %f892, %f893 }, { %r952, %r953, %r954, %r955 }, { %r848, %r849 }, { %f890, %f891, %f892, %f893 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f894, %f895, %f896, %f897 }, { %r952, %r953, %r954, %r955 }, { %r854, %r855 }, { %f894, %f895, %f896, %f897 };
	// end inline asm
	.loc	1 336 18
	add.s64 	%rd81, %rd140, %rd21;
	add.s64 	%rd82, %rd140, %rd20;
	add.s64 	%rd83, %rd140, %rd19;
	add.s64 	%rd84, %rd140, %rd18;
	add.s64 	%rd85, %rd140, %rd17;
	add.s64 	%rd86, %rd140, %rd16;
	add.s64 	%rd87, %rd140, %rd15;
	.loc	1 327 22
	add.s64 	%rd88, %rd140, %rd12;
	add.s32 	%r1069, %r1246, 1;
	setp.lt.s32 	%p73, %r1069, 3;
	selp.b32 	%r1246, %r1069, 0, %p73;
	.loc	1 330 51
	setp.lt.s32 	%p74, %r12, %r1210;
	.loc	1 330 20
	shl.b32 	%r1070, %r1246, 12;
	add.s32 	%r1072, %r496, %r1070;
	bar.sync 	0;
	add.s32 	%r1000, %r1072, %r495;
	add.s32 	%r1002, %r1072, %r498;
	selp.b32 	%r1075, 16, 0, %p74;
	selp.b32 	%r1003, %r1075, 0, %p72;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1000 + 0 ], [ %rd138 + 0 ], 0x10, %r1003;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1002 + 0 ], [ %rd139 + 0 ], 0x10, %r1003;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 51
	setp.lt.s32 	%p75, %r4, %r1210;
	setp.lt.s32 	%p76, %r5, %r1210;
	setp.lt.s32 	%p77, %r6, %r1210;
	setp.lt.s32 	%p78, %r7, %r1210;
	setp.lt.s32 	%p79, %r8, %r1210;
	setp.lt.s32 	%p80, %r9, %r1210;
	setp.lt.s32 	%p81, %r10, %r1210;
	setp.lt.s32 	%p82, %r11, %r1210;
	.loc	1 331 20
	shl.b32 	%r1076, %r1246, 14;
	add.s32 	%r1078, %r505, %r1076;
	add.s32 	%r1004, %r1078, %r504;
	add.s32 	%r1006, %r1078, %r509;
	add.s32 	%r1008, %r1078, %r511;
	add.s32 	%r1010, %r1078, %r513;
	add.s32 	%r1012, %r1078, %r515;
	add.s32 	%r1014, %r1078, %r517;
	add.s32 	%r1016, %r1078, %r519;
	add.s32 	%r1018, %r1078, %r521;
	selp.b32 	%r1087, 16, 0, %p75;
	selp.b32 	%r1005, %r1087, 0, %p72;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1004 + 0 ], [ %rd81 + 0 ], 0x10, %r1005;
	// end inline asm
	selp.b32 	%r1088, 16, 0, %p76;
	selp.b32 	%r1007, %r1088, 0, %p72;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1006 + 0 ], [ %rd82 + 0 ], 0x10, %r1007;
	// end inline asm
	selp.b32 	%r1089, 16, 0, %p77;
	selp.b32 	%r1009, %r1089, 0, %p72;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1008 + 0 ], [ %rd83 + 0 ], 0x10, %r1009;
	// end inline asm
	selp.b32 	%r1090, 16, 0, %p78;
	selp.b32 	%r1011, %r1090, 0, %p72;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1010 + 0 ], [ %rd84 + 0 ], 0x10, %r1011;
	// end inline asm
	selp.b32 	%r1091, 16, 0, %p79;
	selp.b32 	%r1013, %r1091, 0, %p72;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1012 + 0 ], [ %rd85 + 0 ], 0x10, %r1013;
	// end inline asm
	selp.b32 	%r1092, 16, 0, %p80;
	selp.b32 	%r1015, %r1092, 0, %p72;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1014 + 0 ], [ %rd86 + 0 ], 0x10, %r1015;
	// end inline asm
	selp.b32 	%r1093, 16, 0, %p81;
	selp.b32 	%r1017, %r1093, 0, %p72;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1016 + 0 ], [ %rd87 + 0 ], 0x10, %r1017;
	// end inline asm
	selp.b32 	%r1094, 16, 0, %p82;
	selp.b32 	%r1019, %r1094, 0, %p72;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1018 + 0 ], [ %rd88 + 0 ], 0x10, %r1019;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 327 22
	add.s32 	%r1095, %r1245, 1;
	setp.lt.s32 	%p83, %r1095, 3;
	selp.b32 	%r1245, %r1095, 0, %p83;
	.loc	1 330 20
	shl.b32 	%r1096, %r1245, 12;
	add.s32 	%r1244, %r496, %r1096;
	// begin inline asm
	cp.async.wait_group 0x4;
	// end inline asm
	bar.sync 	0;
	.loc	1 331 20
	shl.b32 	%r1097, %r1245, 14;
	add.s32 	%r1243, %r505, %r1097;
	.loc	1 330 20
	add.s32 	%r1024, %r1244, %r550;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1239, %r1240, %r1241, %r1242 }, [ %r1024 + 0 ];
	// end inline asm
	add.s32 	%r1029, %r1024, 1024;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1235, %r1236, %r1237, %r1238 }, [ %r1029 + 0 ];
	// end inline asm
	add.s32 	%r1034, %r1024, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1231, %r1232, %r1233, %r1234 }, [ %r1034 + 0 ];
	// end inline asm
	add.s32 	%r1039, %r1024, 3072;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1227, %r1228, %r1229, %r1230 }, [ %r1039 + 0 ];
	// end inline asm
	.loc	1 331 20
	add.s32 	%r1044, %r1243, %r556;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1223, %r1224, %r1225, %r1226 }, [ %r1044 + 0 ];
	// end inline asm
	add.s32 	%r1049, %r1243, %r560;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1219, %r1220, %r1221, %r1222 }, [ %r1049 + 0 ];
	// end inline asm
	add.s32 	%r1054, %r1243, %r564;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1215, %r1216, %r1217, %r1218 }, [ %r1054 + 0 ];
	// end inline asm
	add.s32 	%r1059, %r1243, %r568;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1211, %r1212, %r1213, %r1214 }, [ %r1059 + 0 ];
	// end inline asm
	.loc	1 327 22
	add.s32 	%r1247, %r1247, 1;
	add.s64 	%rd140, %rd140, %rd14;
	add.s64 	%rd139, %rd139, 64;
	add.s64 	%rd138, %rd138, 64;
	add.s32 	%r1210, %r1210, -32;
	setp.lt.s32 	%p84, %r1247, %r14;
	@%p84 bra 	$L__BB0_2;
	.loc	1 341 23
	cvt.rn.f16.f32 	%rs1, %f897;
	cvt.rn.f16.f32 	%rs2, %f896;
	mov.b32 	%r1311, {%rs2, %rs1};
	cvt.rn.f16.f32 	%rs3, %f895;
	cvt.rn.f16.f32 	%rs4, %f894;
	mov.b32 	%r1310, {%rs4, %rs3};
	cvt.rn.f16.f32 	%rs5, %f893;
	cvt.rn.f16.f32 	%rs6, %f892;
	mov.b32 	%r1309, {%rs6, %rs5};
	cvt.rn.f16.f32 	%rs7, %f891;
	cvt.rn.f16.f32 	%rs8, %f890;
	mov.b32 	%r1308, {%rs8, %rs7};
	cvt.rn.f16.f32 	%rs9, %f889;
	cvt.rn.f16.f32 	%rs10, %f888;
	mov.b32 	%r1307, {%rs10, %rs9};
	cvt.rn.f16.f32 	%rs11, %f887;
	cvt.rn.f16.f32 	%rs12, %f886;
	mov.b32 	%r1306, {%rs12, %rs11};
	cvt.rn.f16.f32 	%rs13, %f885;
	cvt.rn.f16.f32 	%rs14, %f884;
	mov.b32 	%r1305, {%rs14, %rs13};
	cvt.rn.f16.f32 	%rs15, %f883;
	cvt.rn.f16.f32 	%rs16, %f882;
	mov.b32 	%r1304, {%rs16, %rs15};
	cvt.rn.f16.f32 	%rs17, %f881;
	cvt.rn.f16.f32 	%rs18, %f880;
	mov.b32 	%r1303, {%rs18, %rs17};
	cvt.rn.f16.f32 	%rs19, %f879;
	cvt.rn.f16.f32 	%rs20, %f878;
	mov.b32 	%r1302, {%rs20, %rs19};
	cvt.rn.f16.f32 	%rs21, %f877;
	cvt.rn.f16.f32 	%rs22, %f876;
	mov.b32 	%r1301, {%rs22, %rs21};
	cvt.rn.f16.f32 	%rs23, %f875;
	cvt.rn.f16.f32 	%rs24, %f874;
	mov.b32 	%r1300, {%rs24, %rs23};
	cvt.rn.f16.f32 	%rs25, %f873;
	cvt.rn.f16.f32 	%rs26, %f872;
	mov.b32 	%r1299, {%rs26, %rs25};
	cvt.rn.f16.f32 	%rs27, %f871;
	cvt.rn.f16.f32 	%rs28, %f870;
	mov.b32 	%r1298, {%rs28, %rs27};
	cvt.rn.f16.f32 	%rs29, %f869;
	cvt.rn.f16.f32 	%rs30, %f868;
	mov.b32 	%r1297, {%rs30, %rs29};
	cvt.rn.f16.f32 	%rs31, %f867;
	cvt.rn.f16.f32 	%rs32, %f866;
	mov.b32 	%r1296, {%rs32, %rs31};
	cvt.rn.f16.f32 	%rs33, %f865;
	cvt.rn.f16.f32 	%rs34, %f864;
	mov.b32 	%r1295, {%rs34, %rs33};
	cvt.rn.f16.f32 	%rs35, %f863;
	cvt.rn.f16.f32 	%rs36, %f862;
	mov.b32 	%r1294, {%rs36, %rs35};
	cvt.rn.f16.f32 	%rs37, %f861;
	cvt.rn.f16.f32 	%rs38, %f860;
	mov.b32 	%r1293, {%rs38, %rs37};
	cvt.rn.f16.f32 	%rs39, %f859;
	cvt.rn.f16.f32 	%rs40, %f858;
	mov.b32 	%r1292, {%rs40, %rs39};
	cvt.rn.f16.f32 	%rs41, %f857;
	cvt.rn.f16.f32 	%rs42, %f856;
	mov.b32 	%r1291, {%rs42, %rs41};
	cvt.rn.f16.f32 	%rs43, %f855;
	cvt.rn.f16.f32 	%rs44, %f854;
	mov.b32 	%r1290, {%rs44, %rs43};
	cvt.rn.f16.f32 	%rs45, %f853;
	cvt.rn.f16.f32 	%rs46, %f852;
	mov.b32 	%r1289, {%rs46, %rs45};
	cvt.rn.f16.f32 	%rs47, %f851;
	cvt.rn.f16.f32 	%rs48, %f850;
	mov.b32 	%r1288, {%rs48, %rs47};
	cvt.rn.f16.f32 	%rs49, %f849;
	cvt.rn.f16.f32 	%rs50, %f848;
	mov.b32 	%r1287, {%rs50, %rs49};
	cvt.rn.f16.f32 	%rs51, %f847;
	cvt.rn.f16.f32 	%rs52, %f846;
	mov.b32 	%r1286, {%rs52, %rs51};
	cvt.rn.f16.f32 	%rs53, %f845;
	cvt.rn.f16.f32 	%rs54, %f844;
	mov.b32 	%r1285, {%rs54, %rs53};
	cvt.rn.f16.f32 	%rs55, %f843;
	cvt.rn.f16.f32 	%rs56, %f842;
	mov.b32 	%r1284, {%rs56, %rs55};
	cvt.rn.f16.f32 	%rs57, %f841;
	cvt.rn.f16.f32 	%rs58, %f840;
	mov.b32 	%r1283, {%rs58, %rs57};
	cvt.rn.f16.f32 	%rs59, %f839;
	cvt.rn.f16.f32 	%rs60, %f838;
	mov.b32 	%r1282, {%rs60, %rs59};
	cvt.rn.f16.f32 	%rs61, %f837;
	cvt.rn.f16.f32 	%rs62, %f836;
	mov.b32 	%r1281, {%rs62, %rs61};
	cvt.rn.f16.f32 	%rs63, %f835;
	cvt.rn.f16.f32 	%rs64, %f834;
	mov.b32 	%r1280, {%rs64, %rs63};
	cvt.rn.f16.f32 	%rs65, %f833;
	cvt.rn.f16.f32 	%rs66, %f832;
	mov.b32 	%r1279, {%rs66, %rs65};
	cvt.rn.f16.f32 	%rs67, %f831;
	cvt.rn.f16.f32 	%rs68, %f830;
	mov.b32 	%r1278, {%rs68, %rs67};
	cvt.rn.f16.f32 	%rs69, %f829;
	cvt.rn.f16.f32 	%rs70, %f828;
	mov.b32 	%r1277, {%rs70, %rs69};
	cvt.rn.f16.f32 	%rs71, %f827;
	cvt.rn.f16.f32 	%rs72, %f826;
	mov.b32 	%r1276, {%rs72, %rs71};
	cvt.rn.f16.f32 	%rs73, %f825;
	cvt.rn.f16.f32 	%rs74, %f824;
	mov.b32 	%r1275, {%rs74, %rs73};
	cvt.rn.f16.f32 	%rs75, %f823;
	cvt.rn.f16.f32 	%rs76, %f822;
	mov.b32 	%r1274, {%rs76, %rs75};
	cvt.rn.f16.f32 	%rs77, %f821;
	cvt.rn.f16.f32 	%rs78, %f820;
	mov.b32 	%r1273, {%rs78, %rs77};
	cvt.rn.f16.f32 	%rs79, %f819;
	cvt.rn.f16.f32 	%rs80, %f818;
	mov.b32 	%r1272, {%rs80, %rs79};
	cvt.rn.f16.f32 	%rs81, %f817;
	cvt.rn.f16.f32 	%rs82, %f816;
	mov.b32 	%r1271, {%rs82, %rs81};
	cvt.rn.f16.f32 	%rs83, %f815;
	cvt.rn.f16.f32 	%rs84, %f814;
	mov.b32 	%r1270, {%rs84, %rs83};
	cvt.rn.f16.f32 	%rs85, %f813;
	cvt.rn.f16.f32 	%rs86, %f812;
	mov.b32 	%r1269, {%rs86, %rs85};
	cvt.rn.f16.f32 	%rs87, %f811;
	cvt.rn.f16.f32 	%rs88, %f810;
	mov.b32 	%r1268, {%rs88, %rs87};
	cvt.rn.f16.f32 	%rs89, %f809;
	cvt.rn.f16.f32 	%rs90, %f808;
	mov.b32 	%r1267, {%rs90, %rs89};
	cvt.rn.f16.f32 	%rs91, %f807;
	cvt.rn.f16.f32 	%rs92, %f806;
	mov.b32 	%r1266, {%rs92, %rs91};
	cvt.rn.f16.f32 	%rs93, %f805;
	cvt.rn.f16.f32 	%rs94, %f804;
	mov.b32 	%r1265, {%rs94, %rs93};
	cvt.rn.f16.f32 	%rs95, %f803;
	cvt.rn.f16.f32 	%rs96, %f802;
	mov.b32 	%r1264, {%rs96, %rs95};
	cvt.rn.f16.f32 	%rs97, %f801;
	cvt.rn.f16.f32 	%rs98, %f800;
	mov.b32 	%r1263, {%rs98, %rs97};
	cvt.rn.f16.f32 	%rs99, %f799;
	cvt.rn.f16.f32 	%rs100, %f798;
	mov.b32 	%r1262, {%rs100, %rs99};
	cvt.rn.f16.f32 	%rs101, %f797;
	cvt.rn.f16.f32 	%rs102, %f796;
	mov.b32 	%r1261, {%rs102, %rs101};
	cvt.rn.f16.f32 	%rs103, %f795;
	cvt.rn.f16.f32 	%rs104, %f794;
	mov.b32 	%r1260, {%rs104, %rs103};
	cvt.rn.f16.f32 	%rs105, %f793;
	cvt.rn.f16.f32 	%rs106, %f792;
	mov.b32 	%r1259, {%rs106, %rs105};
	cvt.rn.f16.f32 	%rs107, %f791;
	cvt.rn.f16.f32 	%rs108, %f790;
	mov.b32 	%r1258, {%rs108, %rs107};
	cvt.rn.f16.f32 	%rs109, %f789;
	cvt.rn.f16.f32 	%rs110, %f788;
	mov.b32 	%r1257, {%rs110, %rs109};
	cvt.rn.f16.f32 	%rs111, %f787;
	cvt.rn.f16.f32 	%rs112, %f786;
	mov.b32 	%r1256, {%rs112, %rs111};
	cvt.rn.f16.f32 	%rs113, %f785;
	cvt.rn.f16.f32 	%rs114, %f784;
	mov.b32 	%r1255, {%rs114, %rs113};
	cvt.rn.f16.f32 	%rs115, %f783;
	cvt.rn.f16.f32 	%rs116, %f782;
	mov.b32 	%r1254, {%rs116, %rs115};
	cvt.rn.f16.f32 	%rs117, %f781;
	cvt.rn.f16.f32 	%rs118, %f780;
	mov.b32 	%r1253, {%rs118, %rs117};
	cvt.rn.f16.f32 	%rs119, %f779;
	cvt.rn.f16.f32 	%rs120, %f778;
	mov.b32 	%r1252, {%rs120, %rs119};
	cvt.rn.f16.f32 	%rs121, %f777;
	cvt.rn.f16.f32 	%rs122, %f776;
	mov.b32 	%r1251, {%rs122, %rs121};
	cvt.rn.f16.f32 	%rs123, %f775;
	cvt.rn.f16.f32 	%rs124, %f774;
	mov.b32 	%r1250, {%rs124, %rs123};
	cvt.rn.f16.f32 	%rs125, %f773;
	cvt.rn.f16.f32 	%rs126, %f772;
	mov.b32 	%r1249, {%rs126, %rs125};
	cvt.rn.f16.f32 	%rs127, %f771;
	cvt.rn.f16.f32 	%rs128, %f770;
	mov.b32 	%r1248, {%rs128, %rs127};
$L__BB0_4:
	.loc	1 313 51
	or.b32  	%r1163, %r1, %r4;
	.loc	1 313 38
	or.b32  	%r1164, %r1163, 60;
	or.b32  	%r1165, %r1163, 56;
	or.b32  	%r1166, %r1163, 52;
	or.b32  	%r1167, %r1163, 48;
	or.b32  	%r1168, %r1163, 44;
	or.b32  	%r1169, %r1163, 40;
	or.b32  	%r1170, %r1163, 36;
	or.b32  	%r1171, %r1163, 32;
	or.b32  	%r1172, %r1, %r11;
	or.b32  	%r1173, %r1, %r10;
	or.b32  	%r1174, %r1, %r9;
	or.b32  	%r1175, %r1, %r8;
	or.b32  	%r1176, %r1, %r7;
	or.b32  	%r1177, %r1, %r6;
	or.b32  	%r1178, %r1, %r5;
	.loc	1 327 22
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 347 33
	mul.lo.s32 	%r1179, %r1163, %r275;
	mul.lo.s32 	%r1180, %r1178, %r275;
	mul.lo.s32 	%r1181, %r1177, %r275;
	mul.lo.s32 	%r1182, %r1176, %r275;
	mul.lo.s32 	%r1183, %r1175, %r275;
	mul.lo.s32 	%r1184, %r1174, %r275;
	mul.lo.s32 	%r1185, %r1173, %r275;
	mul.lo.s32 	%r1186, %r1172, %r275;
	shl.b32 	%r1187, %r275, 5;
	add.s32 	%r1188, %r1179, %r1187;
	shl.b32 	%r1189, %r275, 2;
	add.s32 	%r1190, %r1188, %r1189;
	add.s32 	%r1191, %r1190, %r1189;
	add.s32 	%r1192, %r1191, %r1189;
	add.s32 	%r1193, %r1192, %r1189;
	add.s32 	%r1194, %r1193, %r1189;
	add.s32 	%r1195, %r1194, %r1189;
	add.s32 	%r1196, %r1195, %r1189;
	.loc	1 347 21
	mul.wide.s32 	%rd105, %r1179, 2;
	add.s64 	%rd106, %rd32, %rd105;
	mul.wide.s32 	%rd107, %r1180, 2;
	add.s64 	%rd108, %rd32, %rd107;
	mul.wide.s32 	%rd109, %r1181, 2;
	add.s64 	%rd110, %rd32, %rd109;
	mul.wide.s32 	%rd111, %r1182, 2;
	add.s64 	%rd112, %rd32, %rd111;
	mul.wide.s32 	%rd113, %r1183, 2;
	add.s64 	%rd114, %rd32, %rd113;
	mul.wide.s32 	%rd115, %r1184, 2;
	add.s64 	%rd116, %rd32, %rd115;
	mul.wide.s32 	%rd117, %r1185, 2;
	add.s64 	%rd118, %rd32, %rd117;
	mul.wide.s32 	%rd119, %r1186, 2;
	add.s64 	%rd120, %rd32, %rd119;
	mul.wide.s32 	%rd121, %r1188, 2;
	add.s64 	%rd122, %rd32, %rd121;
	mul.wide.s32 	%rd123, %r1190, 2;
	add.s64 	%rd124, %rd32, %rd123;
	mul.wide.s32 	%rd125, %r1191, 2;
	add.s64 	%rd126, %rd32, %rd125;
	mul.wide.s32 	%rd127, %r1192, 2;
	add.s64 	%rd128, %rd32, %rd127;
	mul.wide.s32 	%rd129, %r1193, 2;
	add.s64 	%rd130, %rd32, %rd129;
	mul.wide.s32 	%rd131, %r1194, 2;
	add.s64 	%rd132, %rd32, %rd131;
	mul.wide.s32 	%rd133, %r1195, 2;
	add.s64 	%rd134, %rd32, %rd133;
	mul.wide.s32 	%rd135, %r1196, 2;
	add.s64 	%rd136, %rd32, %rd135;
	.loc	1 347 52
	mul.wide.s32 	%rd137, %r13, 2;
	add.s64 	%rd89, %rd106, %rd137;
	add.s64 	%rd90, %rd108, %rd137;
	add.s64 	%rd91, %rd110, %rd137;
	add.s64 	%rd92, %rd112, %rd137;
	add.s64 	%rd93, %rd114, %rd137;
	add.s64 	%rd94, %rd116, %rd137;
	add.s64 	%rd95, %rd118, %rd137;
	add.s64 	%rd96, %rd120, %rd137;
	add.s64 	%rd97, %rd122, %rd137;
	add.s64 	%rd98, %rd124, %rd137;
	add.s64 	%rd99, %rd126, %rd137;
	add.s64 	%rd100, %rd128, %rd137;
	add.s64 	%rd101, %rd130, %rd137;
	add.s64 	%rd102, %rd132, %rd137;
	add.s64 	%rd103, %rd134, %rd137;
	add.s64 	%rd104, %rd136, %rd137;
	.loc	1 348 33
	setp.lt.s32 	%p101, %r1163, %r272;
	setp.lt.s32 	%p102, %r1178, %r272;
	setp.lt.s32 	%p103, %r1177, %r272;
	setp.lt.s32 	%p104, %r1176, %r272;
	setp.lt.s32 	%p105, %r1175, %r272;
	setp.lt.s32 	%p106, %r1174, %r272;
	setp.lt.s32 	%p107, %r1173, %r272;
	setp.lt.s32 	%p108, %r1172, %r272;
	setp.lt.s32 	%p109, %r1171, %r272;
	setp.lt.s32 	%p110, %r1170, %r272;
	setp.lt.s32 	%p111, %r1169, %r272;
	setp.lt.s32 	%p112, %r1168, %r272;
	setp.lt.s32 	%p113, %r1167, %r272;
	setp.lt.s32 	%p114, %r1166, %r272;
	setp.lt.s32 	%p115, %r1165, %r272;
	setp.lt.s32 	%p116, %r1164, %r272;
	.loc	1 348 58
	setp.lt.s32 	%p117, %r13, %r273;
	.loc	1 348 39
	and.pred  	%p85, %p101, %p117;
	and.pred  	%p86, %p102, %p117;
	and.pred  	%p87, %p103, %p117;
	and.pred  	%p88, %p104, %p117;
	and.pred  	%p89, %p105, %p117;
	and.pred  	%p90, %p106, %p117;
	and.pred  	%p91, %p107, %p117;
	and.pred  	%p92, %p108, %p117;
	and.pred  	%p93, %p109, %p117;
	and.pred  	%p94, %p110, %p117;
	and.pred  	%p95, %p111, %p117;
	and.pred  	%p96, %p112, %p117;
	and.pred  	%p97, %p113, %p117;
	and.pred  	%p98, %p114, %p117;
	and.pred  	%p99, %p115, %p117;
	and.pred  	%p100, %p116, %p117;
	.loc	1 349 21
	shr.u32 	%r1197, %r3, 2;
	shl.b32 	%r1198, %r2, 1;
	and.b32  	%r1199, %r1198, 6;
	shl.b32 	%r1200, %r4, 3;
	or.b32  	%r1201, %r1200, %r1199;
	mad.lo.s32 	%r1202, %r1197, 264, %r1201;
	shl.b32 	%r1203, %r1202, 1;
	add.s32 	%r1205, %r496, %r1203;
	st.shared.b32 	[%r1205], %r1248;
	st.shared.b32 	[%r1205+4224], %r1249;
	st.shared.b32 	[%r1205+64], %r1250;
	st.shared.b32 	[%r1205+4288], %r1251;
	st.shared.b32 	[%r1205+128], %r1252;
	st.shared.b32 	[%r1205+4352], %r1253;
	st.shared.b32 	[%r1205+192], %r1254;
	st.shared.b32 	[%r1205+4416], %r1255;
	st.shared.b32 	[%r1205+256], %r1256;
	st.shared.b32 	[%r1205+4480], %r1257;
	st.shared.b32 	[%r1205+320], %r1258;
	st.shared.b32 	[%r1205+4544], %r1259;
	st.shared.b32 	[%r1205+384], %r1260;
	st.shared.b32 	[%r1205+4608], %r1261;
	st.shared.b32 	[%r1205+448], %r1262;
	st.shared.b32 	[%r1205+4672], %r1263;
	bar.sync 	0;
	shl.b32 	%r1206, %r3, 3;
	mad.lo.s32 	%r1207, %r4, 264, %r1206;
	shl.b32 	%r1208, %r1207, 1;
	add.s32 	%r1209, %r496, %r1208;
	ld.shared.v4.u32 	{%r1099, %r1100, %r1101, %r1102}, [%r1209];
	ld.shared.v4.u32 	{%r1103, %r1104, %r1105, %r1106}, [%r1209+2112];
	ld.shared.v4.u32 	{%r1107, %r1108, %r1109, %r1110}, [%r1209+4224];
	ld.shared.v4.u32 	{%r1111, %r1112, %r1113, %r1114}, [%r1209+6336];
	bar.sync 	0;
	st.shared.b32 	[%r1205], %r1264;
	st.shared.b32 	[%r1205+4224], %r1265;
	st.shared.b32 	[%r1205+64], %r1266;
	st.shared.b32 	[%r1205+4288], %r1267;
	st.shared.b32 	[%r1205+128], %r1268;
	st.shared.b32 	[%r1205+4352], %r1269;
	st.shared.b32 	[%r1205+192], %r1270;
	st.shared.b32 	[%r1205+4416], %r1271;
	st.shared.b32 	[%r1205+256], %r1272;
	st.shared.b32 	[%r1205+4480], %r1273;
	st.shared.b32 	[%r1205+320], %r1274;
	st.shared.b32 	[%r1205+4544], %r1275;
	st.shared.b32 	[%r1205+384], %r1276;
	st.shared.b32 	[%r1205+4608], %r1277;
	st.shared.b32 	[%r1205+448], %r1278;
	st.shared.b32 	[%r1205+4672], %r1279;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r1115, %r1116, %r1117, %r1118}, [%r1209];
	ld.shared.v4.u32 	{%r1119, %r1120, %r1121, %r1122}, [%r1209+2112];
	ld.shared.v4.u32 	{%r1123, %r1124, %r1125, %r1126}, [%r1209+4224];
	ld.shared.v4.u32 	{%r1127, %r1128, %r1129, %r1130}, [%r1209+6336];
	bar.sync 	0;
	st.shared.b32 	[%r1205], %r1280;
	st.shared.b32 	[%r1205+4224], %r1281;
	st.shared.b32 	[%r1205+64], %r1282;
	st.shared.b32 	[%r1205+4288], %r1283;
	st.shared.b32 	[%r1205+128], %r1284;
	st.shared.b32 	[%r1205+4352], %r1285;
	st.shared.b32 	[%r1205+192], %r1286;
	st.shared.b32 	[%r1205+4416], %r1287;
	st.shared.b32 	[%r1205+256], %r1288;
	st.shared.b32 	[%r1205+4480], %r1289;
	st.shared.b32 	[%r1205+320], %r1290;
	st.shared.b32 	[%r1205+4544], %r1291;
	st.shared.b32 	[%r1205+384], %r1292;
	st.shared.b32 	[%r1205+4608], %r1293;
	st.shared.b32 	[%r1205+448], %r1294;
	st.shared.b32 	[%r1205+4672], %r1295;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r1131, %r1132, %r1133, %r1134}, [%r1209];
	ld.shared.v4.u32 	{%r1135, %r1136, %r1137, %r1138}, [%r1209+2112];
	ld.shared.v4.u32 	{%r1139, %r1140, %r1141, %r1142}, [%r1209+4224];
	ld.shared.v4.u32 	{%r1143, %r1144, %r1145, %r1146}, [%r1209+6336];
	bar.sync 	0;
	st.shared.b32 	[%r1205], %r1296;
	st.shared.b32 	[%r1205+4224], %r1297;
	st.shared.b32 	[%r1205+64], %r1298;
	st.shared.b32 	[%r1205+4288], %r1299;
	st.shared.b32 	[%r1205+128], %r1300;
	st.shared.b32 	[%r1205+4352], %r1301;
	st.shared.b32 	[%r1205+192], %r1302;
	st.shared.b32 	[%r1205+4416], %r1303;
	st.shared.b32 	[%r1205+256], %r1304;
	st.shared.b32 	[%r1205+4480], %r1305;
	st.shared.b32 	[%r1205+320], %r1306;
	st.shared.b32 	[%r1205+4544], %r1307;
	st.shared.b32 	[%r1205+384], %r1308;
	st.shared.b32 	[%r1205+4608], %r1309;
	st.shared.b32 	[%r1205+448], %r1310;
	st.shared.b32 	[%r1205+4672], %r1311;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r1147, %r1148, %r1149, %r1150}, [%r1209];
	ld.shared.v4.u32 	{%r1151, %r1152, %r1153, %r1154}, [%r1209+2112];
	ld.shared.v4.u32 	{%r1155, %r1156, %r1157, %r1158}, [%r1209+4224];
	ld.shared.v4.u32 	{%r1159, %r1160, %r1161, %r1162}, [%r1209+6336];
	// begin inline asm
	@%p85 st.global.v4.b32 [ %rd89 + 0 ], { %r1099, %r1100, %r1101, %r1102 };
	// end inline asm
	// begin inline asm
	@%p86 st.global.v4.b32 [ %rd90 + 0 ], { %r1103, %r1104, %r1105, %r1106 };
	// end inline asm
	// begin inline asm
	@%p87 st.global.v4.b32 [ %rd91 + 0 ], { %r1107, %r1108, %r1109, %r1110 };
	// end inline asm
	// begin inline asm
	@%p88 st.global.v4.b32 [ %rd92 + 0 ], { %r1111, %r1112, %r1113, %r1114 };
	// end inline asm
	// begin inline asm
	@%p89 st.global.v4.b32 [ %rd93 + 0 ], { %r1115, %r1116, %r1117, %r1118 };
	// end inline asm
	// begin inline asm
	@%p90 st.global.v4.b32 [ %rd94 + 0 ], { %r1119, %r1120, %r1121, %r1122 };
	// end inline asm
	// begin inline asm
	@%p91 st.global.v4.b32 [ %rd95 + 0 ], { %r1123, %r1124, %r1125, %r1126 };
	// end inline asm
	// begin inline asm
	@%p92 st.global.v4.b32 [ %rd96 + 0 ], { %r1127, %r1128, %r1129, %r1130 };
	// end inline asm
	// begin inline asm
	@%p93 st.global.v4.b32 [ %rd97 + 0 ], { %r1131, %r1132, %r1133, %r1134 };
	// end inline asm
	// begin inline asm
	@%p94 st.global.v4.b32 [ %rd98 + 0 ], { %r1135, %r1136, %r1137, %r1138 };
	// end inline asm
	// begin inline asm
	@%p95 st.global.v4.b32 [ %rd99 + 0 ], { %r1139, %r1140, %r1141, %r1142 };
	// end inline asm
	// begin inline asm
	@%p96 st.global.v4.b32 [ %rd100 + 0 ], { %r1143, %r1144, %r1145, %r1146 };
	// end inline asm
	// begin inline asm
	@%p97 st.global.v4.b32 [ %rd101 + 0 ], { %r1147, %r1148, %r1149, %r1150 };
	// end inline asm
	// begin inline asm
	@%p98 st.global.v4.b32 [ %rd102 + 0 ], { %r1151, %r1152, %r1153, %r1154 };
	// end inline asm
	// begin inline asm
	@%p99 st.global.v4.b32 [ %rd103 + 0 ], { %r1155, %r1156, %r1157, %r1158 };
	// end inline asm
	// begin inline asm
	@%p100 st.global.v4.b32 [ %rd104 + 0 ], { %r1159, %r1160, %r1161, %r1162 };
	// end inline asm
	.loc	1 349 4
	ret;
$L__tmp6:
$L__func_end0:

}
	.file	1 "/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/02.matrix-multiplication/matrix_multiplication.py"
	.file	2 "/data_ssd1/zjy_home/frameworks/cuda/triton/python/triton/language/standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 5
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 265
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 109
.b8 97
.b8 116
.b8 114
.b8 105
.b8 120
.b8 95
.b8 109
.b8 117
.b8 108
.b8 116
.b8 105
.b8 112
.b8 108
.b8 105
.b8 99
.b8 97
.b8 116
.b8 105
.b8 111
.b8 110
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 100
.b8 97
.b8 116
.b8 97
.b8 95
.b8 115
.b8 115
.b8 100
.b8 49
.b8 47
.b8 122
.b8 106
.b8 121
.b8 95
.b8 104
.b8 111
.b8 109
.b8 101
.b8 47
.b8 109
.b8 121
.b8 95
.b8 99
.b8 111
.b8 100
.b8 101
.b8 47
.b8 77
.b8 76
.b8 67
.b8 45
.b8 76
.b8 101
.b8 97
.b8 114
.b8 110
.b8 105
.b8 110
.b8 103
.b8 47
.b8 84
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 47
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 45
.b8 101
.b8 120
.b8 97
.b8 109
.b8 112
.b8 108
.b8 101
.b8 115
.b8 47
.b8 48
.b8 50
.b8 46
.b8 109
.b8 97
.b8 116
.b8 114
.b8 105
.b8 120
.b8 45
.b8 109
.b8 117
.b8 108
.b8 116
.b8 105
.b8 112
.b8 108
.b8 105
.b8 99
.b8 97
.b8 116
.b8 105
.b8 111
.b8 110
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 109
.b8 97
.b8 116
.b8 109
.b8 117
.b8 108
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 155
.b8 4
.b32 155
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 37
.b8 1
.b8 27
.b8 4
.b32 155
.b64 $L__tmp2
.b64 $L__tmp3
.b8 1
.b8 38
.b8 1
.b8 27
.b8 4
.b32 155
.b64 $L__tmp4
.b64 $L__tmp5
.b8 1
.b8 71
.b8 1
.b8 33
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}

//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_80
.address_size 64

	// .globl	matmul_kernel
.extern .shared .align 16 .b8 global_smem[];

.visible .entry matmul_kernel(
	.param .u64 matmul_kernel_param_0,
	.param .u64 matmul_kernel_param_1,
	.param .u64 matmul_kernel_param_2,
	.param .u32 matmul_kernel_param_3,
	.param .u32 matmul_kernel_param_4,
	.param .u32 matmul_kernel_param_5,
	.param .u32 matmul_kernel_param_6,
	.param .u32 matmul_kernel_param_7,
	.param .u32 matmul_kernel_param_8
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<140>;
	.reg .b16 	%rs<129>;
	.reg .b32 	%r<2840>;
	.reg .f32 	%f<2434>;
	.reg .b64 	%rd<224>;
	.loc	1 260 0
$L__func_begin0:
	.loc	1 260 0

	ld.param.u32 	%r304, [matmul_kernel_param_8];
	ld.param.u32 	%r303, [matmul_kernel_param_5];
	ld.param.u32 	%r302, [matmul_kernel_param_4];
	ld.param.u32 	%r301, [matmul_kernel_param_3];
	ld.param.u64 	%rd58, [matmul_kernel_param_2];
	ld.param.u64 	%rd57, [matmul_kernel_param_1];
	ld.param.u64 	%rd222, [matmul_kernel_param_0];
$L__tmp0:
	.loc	1 292 24
	// begin inline asm
	mov.u32 %r305, %ctaid.x;
	// end inline asm
$L__tmp1:
	.loc	2 44 22
	add.s32 	%r506, %r301, 255;
	.loc	2 44 28
	shr.s32 	%r507, %r506, 31;
	shr.u32 	%r508, %r507, 24;
	add.s32 	%r509, %r506, %r508;
	shr.s32 	%r510, %r509, 8;
$L__tmp2:
	.loc	2 44 22
	add.s32 	%r511, %r302, 127;
	.loc	2 44 28
	shr.s32 	%r512, %r511, 31;
	shr.u32 	%r513, %r512, 25;
	add.s32 	%r514, %r511, %r513;
	shr.s32 	%r515, %r514, 7;
$L__tmp3:
	.loc	1 295 38
	shl.b32 	%r517, %r515, 3;
	ld.param.u32 	%r518, [matmul_kernel_param_6];
	ld.param.u32 	%r519, [matmul_kernel_param_7];
	.loc	1 296 22
	div.s32 	%r520, %r305, %r517;
	.loc	1 297 29
	shl.b32 	%r521, %r520, 3;
	.loc	1 299 20
	sub.s32 	%r522, %r510, %r521;
	.loc	1 299 33
	min.s32 	%r524, %r522, 8;
	mul.lo.s32 	%r525, %r520, %r517;
	sub.s32 	%r526, %r305, %r525;
	.loc	1 304 40
	div.s32 	%r527, %r526, %r524;
	mul.lo.s32 	%r528, %r527, %r524;
	sub.s32 	%r529, %r526, %r528;
	.loc	1 302 8
	add.s32 	%r530, %r529, %r521;
	.loc	1 313 23
	shl.b32 	%r531, %r530, 8;
	.loc	1 313 51
	mov.u32 	%r1, %tid.x;
	shr.u32 	%r2, %r1, 5;
	bfe.u32 	%r3, %r1, 4, 4;
	or.b32  	%r4, %r3, 16;
	or.b32  	%r5, %r3, 32;
	or.b32  	%r6, %r3, 48;
	or.b32  	%r7, %r3, 64;
	or.b32  	%r8, %r3, 80;
	or.b32  	%r9, %r3, 96;
	or.b32  	%r10, %r3, 112;
	or.b32  	%r532, %r3, 128;
	or.b32  	%r533, %r3, 144;
	or.b32  	%r534, %r3, 160;
	or.b32  	%r535, %r3, 176;
	or.b32  	%r536, %r3, 192;
	or.b32  	%r537, %r3, 208;
	or.b32  	%r538, %r3, 224;
	or.b32  	%r539, %r3, 240;
	.loc	1 313 38
	or.b32  	%r11, %r531, %r3;
	or.b32  	%r12, %r531, %r4;
	or.b32  	%r13, %r531, %r5;
	or.b32  	%r14, %r531, %r6;
	or.b32  	%r15, %r531, %r7;
	or.b32  	%r16, %r531, %r8;
	or.b32  	%r17, %r531, %r9;
	or.b32  	%r18, %r531, %r10;
	or.b32  	%r19, %r531, %r532;
	or.b32  	%r20, %r531, %r533;
	or.b32  	%r21, %r531, %r534;
	or.b32  	%r22, %r531, %r535;
	or.b32  	%r23, %r531, %r536;
	or.b32  	%r24, %r531, %r537;
	or.b32  	%r25, %r531, %r538;
	or.b32  	%r26, %r531, %r539;
	.loc	1 313 68
	rem.s32 	%r540, %r11, %r301;
	rem.s32 	%r541, %r12, %r301;
	rem.s32 	%r542, %r13, %r301;
	rem.s32 	%r543, %r14, %r301;
	rem.s32 	%r544, %r15, %r301;
	rem.s32 	%r545, %r16, %r301;
	rem.s32 	%r546, %r17, %r301;
	rem.s32 	%r547, %r18, %r301;
	rem.s32 	%r548, %r19, %r301;
	rem.s32 	%r549, %r20, %r301;
	rem.s32 	%r550, %r21, %r301;
	rem.s32 	%r551, %r22, %r301;
	rem.s32 	%r552, %r23, %r301;
	rem.s32 	%r553, %r24, %r301;
	rem.s32 	%r554, %r25, %r301;
	rem.s32 	%r555, %r26, %r301;
	.loc	1 314 23
	shl.b32 	%r556, %r527, 7;
	.loc	1 314 51
	shl.b32 	%r557, %r1, 3;
	and.b32  	%r27, %r557, 120;
	.loc	1 314 38
	or.b32  	%r28, %r556, %r27;
	.loc	1 314 68
	rem.s32 	%r558, %r28, %r302;
	.loc	1 316 53
	mad.lo.s32 	%r559, %r540, %r518, %r27;
	mad.lo.s32 	%r560, %r541, %r518, %r27;
	mad.lo.s32 	%r561, %r542, %r518, %r27;
	mad.lo.s32 	%r562, %r543, %r518, %r27;
	mad.lo.s32 	%r563, %r544, %r518, %r27;
	mad.lo.s32 	%r564, %r545, %r518, %r27;
	mad.lo.s32 	%r565, %r546, %r518, %r27;
	mad.lo.s32 	%r566, %r547, %r518, %r27;
	mad.lo.s32 	%r567, %r548, %r518, %r27;
	mad.lo.s32 	%r568, %r549, %r518, %r27;
	mad.lo.s32 	%r569, %r550, %r518, %r27;
	mad.lo.s32 	%r570, %r551, %r518, %r27;
	mad.lo.s32 	%r571, %r552, %r518, %r27;
	mad.lo.s32 	%r572, %r553, %r518, %r27;
	mad.lo.s32 	%r573, %r554, %r518, %r27;
	mad.lo.s32 	%r574, %r555, %r518, %r27;
	.loc	1 316 22
	mul.wide.s32 	%rd107, %r559, 2;
	add.s64 	%rd59, %rd222, %rd107;
	mul.wide.s32 	%rd108, %r560, 2;
	add.s64 	%rd60, %rd222, %rd108;
	mul.wide.s32 	%rd109, %r561, 2;
	add.s64 	%rd61, %rd222, %rd109;
	mul.wide.s32 	%rd110, %r562, 2;
	add.s64 	%rd62, %rd222, %rd110;
	mul.wide.s32 	%rd111, %r563, 2;
	add.s64 	%rd63, %rd222, %rd111;
	mul.wide.s32 	%rd112, %r564, 2;
	add.s64 	%rd64, %rd222, %rd112;
	mul.wide.s32 	%rd113, %r565, 2;
	add.s64 	%rd65, %rd222, %rd113;
	mul.wide.s32 	%rd114, %r566, 2;
	add.s64 	%rd66, %rd222, %rd114;
	mul.wide.s32 	%rd115, %r567, 2;
	add.s64 	%rd67, %rd222, %rd115;
	mul.wide.s32 	%rd116, %r568, 2;
	add.s64 	%rd68, %rd222, %rd116;
	mul.wide.s32 	%rd117, %r569, 2;
	add.s64 	%rd69, %rd222, %rd117;
	mul.wide.s32 	%rd118, %r570, 2;
	add.s64 	%rd70, %rd222, %rd118;
	mul.wide.s32 	%rd119, %r571, 2;
	add.s64 	%rd71, %rd222, %rd119;
	mul.wide.s32 	%rd120, %r572, 2;
	add.s64 	%rd72, %rd222, %rd120;
	mul.wide.s32 	%rd121, %r573, 2;
	add.s64 	%rd73, %rd222, %rd121;
	mul.wide.s32 	%rd122, %r574, 2;
	add.s64 	%rd74, %rd222, %rd122;
	.loc	1 318 40
	shl.b32 	%r575, %r519, 4;
	.loc	1 318 52
	mad.lo.s32 	%r576, %r3, %r519, %r558;
	add.s32 	%r577, %r576, %r575;
	add.s32 	%r578, %r577, %r575;
	add.s32 	%r579, %r578, %r575;
	add.s32 	%r580, %r579, %r575;
	add.s32 	%r581, %r580, %r575;
	add.s32 	%r582, %r581, %r575;
	add.s32 	%r583, %r582, %r575;
	.loc	1 318 22
	mul.wide.s32 	%rd123, %r576, 2;
	add.s64 	%rd75, %rd57, %rd123;
	mul.wide.s32 	%rd124, %r577, 2;
	add.s64 	%rd76, %rd57, %rd124;
	mul.wide.s32 	%rd125, %r578, 2;
	add.s64 	%rd77, %rd57, %rd125;
	mul.wide.s32 	%rd126, %r579, 2;
	add.s64 	%rd78, %rd57, %rd126;
	mul.wide.s32 	%rd127, %r580, 2;
	add.s64 	%rd79, %rd57, %rd127;
	mul.wide.s32 	%rd128, %r581, 2;
	add.s64 	%rd80, %rd57, %rd128;
	mul.wide.s32 	%rd129, %r582, 2;
	add.s64 	%rd81, %rd57, %rd129;
	mul.wide.s32 	%rd130, %r583, 2;
	add.s64 	%rd82, %rd57, %rd130;
$L__tmp4:
	.loc	2 44 22
	add.s32 	%r584, %r303, 127;
$L__tmp5:
	.loc	1 336 33
	shl.b32 	%r588, %r519, 7;
	.loc	1 327 22
	setp.lt.s32 	%p49, %r584, 128;
	setp.gt.s32 	%p50, %r584, 127;
	.loc	1 330 51
	setp.lt.s32 	%p51, %r27, %r303;
	.loc	1 330 20
	shl.b32 	%r589, %r3, 7;
	shr.u32 	%r590, %r1, 1;
	and.b32  	%r591, %r590, 56;
	xor.b32  	%r592, %r591, %r27;
	or.b32  	%r30, %r592, %r589;
	shl.b32 	%r593, %r30, 1;
	mov.u32 	%r594, global_smem;
	add.s32 	%r306, %r594, %r593;
	shl.b32 	%r595, %r4, 7;
	or.b32  	%r31, %r595, %r592;
	shl.b32 	%r596, %r31, 1;
	add.s32 	%r308, %r594, %r596;
	shl.b32 	%r597, %r5, 7;
	or.b32  	%r32, %r597, %r592;
	shl.b32 	%r598, %r32, 1;
	add.s32 	%r310, %r594, %r598;
	shl.b32 	%r599, %r6, 7;
	or.b32  	%r33, %r599, %r592;
	shl.b32 	%r600, %r33, 1;
	add.s32 	%r312, %r594, %r600;
	shl.b32 	%r601, %r7, 7;
	or.b32  	%r34, %r601, %r592;
	shl.b32 	%r602, %r34, 1;
	add.s32 	%r314, %r594, %r602;
	shl.b32 	%r603, %r8, 7;
	or.b32  	%r35, %r603, %r592;
	shl.b32 	%r604, %r35, 1;
	add.s32 	%r316, %r594, %r604;
	shl.b32 	%r605, %r9, 7;
	or.b32  	%r36, %r605, %r592;
	shl.b32 	%r606, %r36, 1;
	add.s32 	%r318, %r594, %r606;
	shl.b32 	%r607, %r10, 7;
	or.b32  	%r37, %r607, %r592;
	shl.b32 	%r608, %r37, 1;
	add.s32 	%r320, %r594, %r608;
	shl.b32 	%r609, %r532, 7;
	or.b32  	%r38, %r609, %r592;
	shl.b32 	%r610, %r38, 1;
	add.s32 	%r322, %r594, %r610;
	shl.b32 	%r611, %r533, 7;
	or.b32  	%r39, %r611, %r592;
	shl.b32 	%r612, %r39, 1;
	add.s32 	%r324, %r594, %r612;
	shl.b32 	%r613, %r534, 7;
	or.b32  	%r40, %r613, %r592;
	shl.b32 	%r614, %r40, 1;
	add.s32 	%r326, %r594, %r614;
	shl.b32 	%r615, %r535, 7;
	or.b32  	%r41, %r615, %r592;
	shl.b32 	%r616, %r41, 1;
	add.s32 	%r328, %r594, %r616;
	shl.b32 	%r617, %r536, 7;
	or.b32  	%r42, %r617, %r592;
	shl.b32 	%r618, %r42, 1;
	add.s32 	%r330, %r594, %r618;
	shl.b32 	%r619, %r537, 7;
	or.b32  	%r43, %r619, %r592;
	shl.b32 	%r620, %r43, 1;
	add.s32 	%r332, %r594, %r620;
	shl.b32 	%r621, %r538, 7;
	or.b32  	%r44, %r621, %r592;
	shl.b32 	%r622, %r44, 1;
	add.s32 	%r334, %r594, %r622;
	shl.b32 	%r623, %r539, 7;
	or.b32  	%r45, %r623, %r592;
	shl.b32 	%r624, %r45, 1;
	add.s32 	%r336, %r594, %r624;
	selp.b32 	%r625, 16, 0, %p50;
	selp.b32 	%r309, %r625, 0, %p51;
	mov.pred 	%p1, -1;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r306 + 0 ], [ %rd59 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r308 + 0 ], [ %rd60 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r310 + 0 ], [ %rd61 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r312 + 0 ], [ %rd62 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r314 + 0 ], [ %rd63 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r316 + 0 ], [ %rd64 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r318 + 0 ], [ %rd65 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r320 + 0 ], [ %rd66 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r322 + 0 ], [ %rd67 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r324 + 0 ], [ %rd68 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r326 + 0 ], [ %rd69 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r328 + 0 ], [ %rd70 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r330 + 0 ], [ %rd71 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r332 + 0 ], [ %rd72 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r334 + 0 ], [ %rd73 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r336 + 0 ], [ %rd74 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 51
	setp.lt.s32 	%p52, %r3, %r303;
	setp.lt.s32 	%p53, %r4, %r303;
	setp.lt.s32 	%p54, %r5, %r303;
	setp.lt.s32 	%p55, %r6, %r303;
	setp.lt.s32 	%p56, %r7, %r303;
	setp.lt.s32 	%p57, %r8, %r303;
	setp.lt.s32 	%p58, %r9, %r303;
	setp.lt.s32 	%p59, %r10, %r303;
	.loc	1 331 20
	add.s32 	%r626, %r594, 131072;
	add.s32 	%r338, %r626, %r593;
	add.s32 	%r340, %r626, %r596;
	add.s32 	%r342, %r626, %r598;
	add.s32 	%r344, %r626, %r600;
	add.s32 	%r346, %r626, %r602;
	add.s32 	%r348, %r626, %r604;
	add.s32 	%r350, %r626, %r606;
	add.s32 	%r352, %r626, %r608;
	selp.b32 	%r339, %r625, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r338 + 0 ], [ %rd75 + 0 ], 0x10, %r339;
	// end inline asm
	selp.b32 	%r341, %r625, 0, %p53;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r340 + 0 ], [ %rd76 + 0 ], 0x10, %r341;
	// end inline asm
	selp.b32 	%r343, %r625, 0, %p54;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r342 + 0 ], [ %rd77 + 0 ], 0x10, %r343;
	// end inline asm
	selp.b32 	%r345, %r625, 0, %p55;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r344 + 0 ], [ %rd78 + 0 ], 0x10, %r345;
	// end inline asm
	selp.b32 	%r347, %r625, 0, %p56;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r346 + 0 ], [ %rd79 + 0 ], 0x10, %r347;
	// end inline asm
	selp.b32 	%r349, %r625, 0, %p57;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r348 + 0 ], [ %rd80 + 0 ], 0x10, %r349;
	// end inline asm
	selp.b32 	%r351, %r625, 0, %p58;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r350 + 0 ], [ %rd81 + 0 ], 0x10, %r351;
	// end inline asm
	selp.b32 	%r353, %r625, 0, %p59;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r352 + 0 ], [ %rd82 + 0 ], 0x10, %r353;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 327 22
	setp.gt.s32 	%p60, %r584, 255;
	.loc	1 335 18
	add.s64 	%rd83, %rd59, 256;
	add.s64 	%rd84, %rd60, 256;
	add.s64 	%rd85, %rd61, 256;
	add.s64 	%rd86, %rd62, 256;
	add.s64 	%rd87, %rd63, 256;
	add.s64 	%rd88, %rd64, 256;
	add.s64 	%rd89, %rd65, 256;
	add.s64 	%rd90, %rd66, 256;
	add.s64 	%rd91, %rd67, 256;
	add.s64 	%rd92, %rd68, 256;
	add.s64 	%rd93, %rd69, 256;
	add.s64 	%rd94, %rd70, 256;
	add.s64 	%rd95, %rd71, 256;
	add.s64 	%rd96, %rd72, 256;
	add.s64 	%rd97, %rd73, 256;
	add.s64 	%rd98, %rd74, 256;
	.loc	1 336 18
	mul.wide.s32 	%rd131, %r588, 2;
	add.s64 	%rd99, %rd75, %rd131;
	add.s64 	%rd100, %rd76, %rd131;
	add.s64 	%rd101, %rd77, %rd131;
	add.s64 	%rd102, %rd78, %rd131;
	add.s64 	%rd103, %rd79, %rd131;
	add.s64 	%rd104, %rd80, %rd131;
	add.s64 	%rd105, %rd81, %rd131;
	add.s64 	%rd106, %rd82, %rd131;
	.loc	1 330 55
	add.s32 	%r627, %r303, -128;
	.loc	1 330 51
	setp.lt.s32 	%p61, %r27, %r627;
	.loc	1 330 20
	bar.sync 	0;
	add.s32 	%r628, %r594, 65536;
	add.s32 	%r354, %r628, %r593;
	add.s32 	%r356, %r628, %r596;
	add.s32 	%r358, %r628, %r598;
	add.s32 	%r360, %r628, %r600;
	add.s32 	%r362, %r628, %r602;
	add.s32 	%r364, %r628, %r604;
	add.s32 	%r366, %r628, %r606;
	add.s32 	%r368, %r628, %r608;
	add.s32 	%r370, %r628, %r610;
	add.s32 	%r372, %r628, %r612;
	add.s32 	%r374, %r628, %r614;
	add.s32 	%r376, %r628, %r616;
	add.s32 	%r378, %r628, %r618;
	add.s32 	%r380, %r628, %r620;
	add.s32 	%r382, %r628, %r622;
	add.s32 	%r384, %r628, %r624;
	selp.b32 	%r629, 16, 0, %p61;
	selp.b32 	%r357, %r629, 0, %p60;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r354 + 0 ], [ %rd83 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r356 + 0 ], [ %rd84 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r358 + 0 ], [ %rd85 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r360 + 0 ], [ %rd86 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r362 + 0 ], [ %rd87 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r364 + 0 ], [ %rd88 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r366 + 0 ], [ %rd89 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r368 + 0 ], [ %rd90 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r370 + 0 ], [ %rd91 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r372 + 0 ], [ %rd92 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r374 + 0 ], [ %rd93 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r376 + 0 ], [ %rd94 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r378 + 0 ], [ %rd95 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r380 + 0 ], [ %rd96 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r382 + 0 ], [ %rd97 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r384 + 0 ], [ %rd98 + 0 ], 0x10, %r357;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 51
	setp.lt.s32 	%p62, %r3, %r627;
	setp.lt.s32 	%p63, %r4, %r627;
	setp.lt.s32 	%p64, %r5, %r627;
	setp.lt.s32 	%p65, %r6, %r627;
	setp.lt.s32 	%p66, %r7, %r627;
	setp.lt.s32 	%p67, %r8, %r627;
	setp.lt.s32 	%p68, %r9, %r627;
	setp.lt.s32 	%p69, %r10, %r627;
	.loc	1 331 20
	add.s32 	%r630, %r594, 163840;
	add.s32 	%r386, %r630, %r593;
	add.s32 	%r388, %r630, %r596;
	add.s32 	%r390, %r630, %r598;
	add.s32 	%r392, %r630, %r600;
	add.s32 	%r394, %r630, %r602;
	add.s32 	%r396, %r630, %r604;
	add.s32 	%r398, %r630, %r606;
	add.s32 	%r400, %r630, %r608;
	selp.b32 	%r631, 16, 0, %p62;
	selp.b32 	%r387, %r631, 0, %p60;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r386 + 0 ], [ %rd99 + 0 ], 0x10, %r387;
	// end inline asm
	selp.b32 	%r632, 16, 0, %p63;
	selp.b32 	%r389, %r632, 0, %p60;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r388 + 0 ], [ %rd100 + 0 ], 0x10, %r389;
	// end inline asm
	selp.b32 	%r633, 16, 0, %p64;
	selp.b32 	%r391, %r633, 0, %p60;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r390 + 0 ], [ %rd101 + 0 ], 0x10, %r391;
	// end inline asm
	selp.b32 	%r634, 16, 0, %p65;
	selp.b32 	%r393, %r634, 0, %p60;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r392 + 0 ], [ %rd102 + 0 ], 0x10, %r393;
	// end inline asm
	selp.b32 	%r635, 16, 0, %p66;
	selp.b32 	%r395, %r635, 0, %p60;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r394 + 0 ], [ %rd103 + 0 ], 0x10, %r395;
	// end inline asm
	selp.b32 	%r636, 16, 0, %p67;
	selp.b32 	%r397, %r636, 0, %p60;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r396 + 0 ], [ %rd104 + 0 ], 0x10, %r397;
	// end inline asm
	selp.b32 	%r637, 16, 0, %p68;
	selp.b32 	%r399, %r637, 0, %p60;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r398 + 0 ], [ %rd105 + 0 ], 0x10, %r399;
	// end inline asm
	selp.b32 	%r638, 16, 0, %p69;
	selp.b32 	%r401, %r638, 0, %p60;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r400 + 0 ], [ %rd106 + 0 ], 0x10, %r401;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 330 20
	// begin inline asm
	cp.async.wait_group 0x2;
	// end inline asm
	bar.sync 	0;
	and.b32  	%r46, %r1, 7;
	bfe.u32 	%r47, %r1, 4, 1;
	shr.u32 	%r48, %r1, 2;
	and.b32  	%r639, %r48, 48;
	and.b32  	%r640, %r1, 15;
	or.b32  	%r641, %r640, %r639;
	xor.b32  	%r642, %r47, %r46;
	shl.b32 	%r49, %r641, 7;
	shl.b32 	%r643, %r642, 3;
	or.b32  	%r50, %r49, %r643;
	shl.b32 	%r644, %r50, 1;
	add.s32 	%r406, %r594, %r644;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2767, %r2768, %r2769, %r2770 }, [ %r406 + 0 ];
	// end inline asm
	add.s32 	%r411, %r406, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2763, %r2764, %r2765, %r2766 }, [ %r411 + 0 ];
	// end inline asm
	add.s32 	%r416, %r406, 32768;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2759, %r2760, %r2761, %r2762 }, [ %r416 + 0 ];
	// end inline asm
	add.s32 	%r421, %r406, 49152;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2755, %r2756, %r2757, %r2758 }, [ %r421 + 0 ];
	// end inline asm
	.loc	1 331 20
	bfe.u32 	%r67, %r1, 5, 1;
	shl.b32 	%r645, %r47, 1;
	or.b32  	%r646, %r645, %r67;
	xor.b32  	%r647, %r646, %r46;
	shl.b32 	%r648, %r640, 7;
	shl.b32 	%r649, %r647, 3;
	or.b32  	%r68, %r649, %r648;
	shl.b32 	%r650, %r68, 1;
	add.s32 	%r426, %r626, %r650;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2751, %r2752, %r2753, %r2754 }, [ %r426 + 0 ];
	// end inline asm
	or.b32  	%r651, %r646, 4;
	xor.b32  	%r652, %r651, %r46;
	shl.b32 	%r653, %r652, 3;
	or.b32  	%r73, %r653, %r648;
	shl.b32 	%r654, %r73, 1;
	add.s32 	%r431, %r626, %r654;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2747, %r2748, %r2749, %r2750 }, [ %r431 + 0 ];
	// end inline asm
	or.b32  	%r655, %r646, 8;
	xor.b32  	%r656, %r655, %r46;
	shl.b32 	%r657, %r656, 3;
	or.b32  	%r78, %r657, %r648;
	shl.b32 	%r658, %r78, 1;
	add.s32 	%r436, %r626, %r658;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2743, %r2744, %r2745, %r2746 }, [ %r436 + 0 ];
	// end inline asm
	or.b32  	%r659, %r646, 12;
	xor.b32  	%r660, %r659, %r46;
	shl.b32 	%r661, %r660, 3;
	or.b32  	%r83, %r661, %r648;
	shl.b32 	%r662, %r83, 1;
	add.s32 	%r441, %r626, %r662;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2739, %r2740, %r2741, %r2742 }, [ %r441 + 0 ];
	// end inline asm
	mov.b32 	%r2776, 0;
	mov.u32 	%r2777, %r2776;
	mov.u32 	%r2778, %r2776;
	mov.u32 	%r2779, %r2776;
	mov.u32 	%r2780, %r2776;
	mov.u32 	%r2781, %r2776;
	mov.u32 	%r2782, %r2776;
	mov.u32 	%r2783, %r2776;
	mov.u32 	%r2784, %r2776;
	mov.u32 	%r2785, %r2776;
	mov.u32 	%r2786, %r2776;
	mov.u32 	%r2787, %r2776;
	mov.u32 	%r2788, %r2776;
	mov.u32 	%r2789, %r2776;
	mov.u32 	%r2790, %r2776;
	mov.u32 	%r2791, %r2776;
	mov.u32 	%r2792, %r2776;
	mov.u32 	%r2793, %r2776;
	mov.u32 	%r2794, %r2776;
	mov.u32 	%r2795, %r2776;
	mov.u32 	%r2796, %r2776;
	mov.u32 	%r2797, %r2776;
	mov.u32 	%r2798, %r2776;
	mov.u32 	%r2799, %r2776;
	mov.u32 	%r2800, %r2776;
	mov.u32 	%r2801, %r2776;
	mov.u32 	%r2802, %r2776;
	mov.u32 	%r2803, %r2776;
	mov.u32 	%r2804, %r2776;
	mov.u32 	%r2805, %r2776;
	mov.u32 	%r2806, %r2776;
	mov.u32 	%r2807, %r2776;
	mov.u32 	%r2808, %r2776;
	mov.u32 	%r2809, %r2776;
	mov.u32 	%r2810, %r2776;
	mov.u32 	%r2811, %r2776;
	mov.u32 	%r2812, %r2776;
	mov.u32 	%r2813, %r2776;
	mov.u32 	%r2814, %r2776;
	mov.u32 	%r2815, %r2776;
	mov.u32 	%r2816, %r2776;
	mov.u32 	%r2817, %r2776;
	mov.u32 	%r2818, %r2776;
	mov.u32 	%r2819, %r2776;
	mov.u32 	%r2820, %r2776;
	mov.u32 	%r2821, %r2776;
	mov.u32 	%r2822, %r2776;
	mov.u32 	%r2823, %r2776;
	mov.u32 	%r2824, %r2776;
	mov.u32 	%r2825, %r2776;
	mov.u32 	%r2826, %r2776;
	mov.u32 	%r2827, %r2776;
	mov.u32 	%r2828, %r2776;
	mov.u32 	%r2829, %r2776;
	mov.u32 	%r2830, %r2776;
	mov.u32 	%r2831, %r2776;
	mov.u32 	%r2832, %r2776;
	mov.u32 	%r2833, %r2776;
	mov.u32 	%r2834, %r2776;
	mov.u32 	%r2835, %r2776;
	mov.u32 	%r2836, %r2776;
	mov.u32 	%r2837, %r2776;
	mov.u32 	%r2838, %r2776;
	mov.u32 	%r2839, %r2776;
	.loc	1 327 22
	@%p49 bra 	$L__BB0_4;
	.loc	1 0 22
	cvt.s64.s32 	%rd1, %r559;
	cvt.s64.s32 	%rd2, %r560;
	cvt.s64.s32 	%rd3, %r561;
	cvt.s64.s32 	%rd4, %r562;
	cvt.s64.s32 	%rd5, %r563;
	cvt.s64.s32 	%rd6, %r564;
	cvt.s64.s32 	%rd7, %r565;
	cvt.s64.s32 	%rd8, %r566;
	cvt.s64.s32 	%rd9, %r567;
	cvt.s64.s32 	%rd10, %r568;
	cvt.s64.s32 	%rd11, %r569;
	cvt.s64.s32 	%rd12, %r570;
	cvt.s64.s32 	%rd13, %r571;
	cvt.s64.s32 	%rd14, %r572;
	cvt.s64.s32 	%rd15, %r573;
	cvt.s64.s32 	%rd16, %r574;
	cvt.s64.s32 	%rd17, %r576;
	cvt.s64.s32 	%rd18, %r577;
	cvt.s64.s32 	%rd19, %r578;
	cvt.s64.s32 	%rd20, %r579;
	cvt.s64.s32 	%rd21, %r580;
	cvt.s64.s32 	%rd22, %r581;
	cvt.s64.s32 	%rd23, %r582;
	cvt.s64.s32 	%rd24, %r583;
	shr.s32 	%r585, %r584, 31;
	shr.u32 	%r586, %r585, 25;
	add.s32 	%r587, %r584, %r586;
	shr.s32 	%r29, %r587, 7;
	cvt.s64.s32 	%rd25, %r588;
	add.s32 	%r88, %r29, -2;
	or.b32  	%r667, %r47, 2;
	xor.b32  	%r668, %r667, %r46;
	shl.b32 	%r669, %r668, 3;
	or.b32  	%r670, %r47, 4;
	xor.b32  	%r671, %r670, %r46;
	shl.b32 	%r672, %r671, 3;
	or.b32  	%r673, %r47, 6;
	xor.b32  	%r674, %r673, %r46;
	shl.b32 	%r675, %r674, 3;
	or.b32  	%r676, %r47, 8;
	xor.b32  	%r677, %r676, %r46;
	shl.b32 	%r678, %r677, 3;
	or.b32  	%r679, %r47, 10;
	xor.b32  	%r680, %r679, %r46;
	shl.b32 	%r681, %r680, 3;
	or.b32  	%r682, %r47, 12;
	xor.b32  	%r683, %r682, %r46;
	shl.b32 	%r684, %r683, 3;
	or.b32  	%r685, %r47, 14;
	xor.b32  	%r686, %r685, %r46;
	shl.b32 	%r687, %r686, 3;
	add.s32 	%r2738, %r303, -256;
	or.b32  	%r90, %r49, %r669;
	or.b32  	%r91, %r49, %r672;
	or.b32  	%r92, %r49, %r675;
	or.b32  	%r93, %r49, %r678;
	or.b32  	%r94, %r49, %r681;
	or.b32  	%r95, %r49, %r684;
	or.b32  	%r96, %r49, %r687;
	.loc	1 327 22
	shl.b64 	%rd26, %rd24, 1;
	shl.b64 	%rd132, %rd25, 2;
	add.s64 	%rd223, %rd57, %rd132;
	shl.b64 	%rd28, %rd25, 1;
	shl.b64 	%rd29, %rd23, 1;
	shl.b64 	%rd30, %rd22, 1;
	shl.b64 	%rd31, %rd21, 1;
	shl.b64 	%rd32, %rd20, 1;
	shl.b64 	%rd33, %rd19, 1;
	shl.b64 	%rd34, %rd18, 1;
	shl.b64 	%rd35, %rd17, 1;
	shl.b64 	%rd133, %rd16, 1;
	add.s64 	%rd36, %rd133, 512;
	shl.b64 	%rd134, %rd15, 1;
	add.s64 	%rd37, %rd134, 512;
	shl.b64 	%rd135, %rd14, 1;
	add.s64 	%rd38, %rd135, 512;
	shl.b64 	%rd136, %rd13, 1;
	add.s64 	%rd39, %rd136, 512;
	shl.b64 	%rd137, %rd12, 1;
	add.s64 	%rd40, %rd137, 512;
	shl.b64 	%rd138, %rd11, 1;
	add.s64 	%rd41, %rd138, 512;
	shl.b64 	%rd139, %rd10, 1;
	add.s64 	%rd42, %rd139, 512;
	shl.b64 	%rd140, %rd9, 1;
	add.s64 	%rd43, %rd140, 512;
	shl.b64 	%rd141, %rd8, 1;
	add.s64 	%rd44, %rd141, 512;
	shl.b64 	%rd142, %rd7, 1;
	add.s64 	%rd45, %rd142, 512;
	shl.b64 	%rd143, %rd6, 1;
	add.s64 	%rd46, %rd143, 512;
	shl.b64 	%rd144, %rd5, 1;
	add.s64 	%rd47, %rd144, 512;
	shl.b64 	%rd145, %rd4, 1;
	add.s64 	%rd48, %rd145, 512;
	shl.b64 	%rd146, %rd3, 1;
	add.s64 	%rd49, %rd146, 512;
	shl.b64 	%rd147, %rd2, 1;
	add.s64 	%rd50, %rd147, 512;
	shl.b64 	%rd148, %rd1, 1;
	add.s64 	%rd51, %rd148, 512;
	mov.f32 	%f2306, 0f00000000;
	mov.b32 	%r2774, 1;
	mov.b32 	%r2773, 0;
	shl.b32 	%r2592, %r90, 1;
	shl.b32 	%r2601, %r91, 1;
	shl.b32 	%r2602, %r92, 1;
	shl.b32 	%r2603, %r93, 1;
	shl.b32 	%r2604, %r94, 1;
	shl.b32 	%r2605, %r95, 1;
	shl.b32 	%r2606, %r96, 1;
	mov.u32 	%r2771, %r626;
	mov.u32 	%r2772, %r594;
	mov.f32 	%f2307, %f2306;
	mov.f32 	%f2308, %f2306;
	mov.f32 	%f2309, %f2306;
	mov.f32 	%f2310, %f2306;
	mov.f32 	%f2311, %f2306;
	mov.f32 	%f2312, %f2306;
	mov.f32 	%f2313, %f2306;
	mov.f32 	%f2314, %f2306;
	mov.f32 	%f2315, %f2306;
	mov.f32 	%f2316, %f2306;
	mov.f32 	%f2317, %f2306;
	mov.f32 	%f2318, %f2306;
	mov.f32 	%f2319, %f2306;
	mov.f32 	%f2320, %f2306;
	mov.f32 	%f2321, %f2306;
	mov.f32 	%f2322, %f2306;
	mov.f32 	%f2323, %f2306;
	mov.f32 	%f2324, %f2306;
	mov.f32 	%f2325, %f2306;
	mov.f32 	%f2326, %f2306;
	mov.f32 	%f2327, %f2306;
	mov.f32 	%f2328, %f2306;
	mov.f32 	%f2329, %f2306;
	mov.f32 	%f2330, %f2306;
	mov.f32 	%f2331, %f2306;
	mov.f32 	%f2332, %f2306;
	mov.f32 	%f2333, %f2306;
	mov.f32 	%f2334, %f2306;
	mov.f32 	%f2335, %f2306;
	mov.f32 	%f2336, %f2306;
	mov.f32 	%f2337, %f2306;
	mov.f32 	%f2338, %f2306;
	mov.f32 	%f2339, %f2306;
	mov.f32 	%f2340, %f2306;
	mov.f32 	%f2341, %f2306;
	mov.f32 	%f2342, %f2306;
	mov.f32 	%f2343, %f2306;
	mov.f32 	%f2344, %f2306;
	mov.f32 	%f2345, %f2306;
	mov.f32 	%f2346, %f2306;
	mov.f32 	%f2347, %f2306;
	mov.f32 	%f2348, %f2306;
	mov.f32 	%f2349, %f2306;
	mov.f32 	%f2350, %f2306;
	mov.f32 	%f2351, %f2306;
	mov.f32 	%f2352, %f2306;
	mov.f32 	%f2353, %f2306;
	mov.f32 	%f2354, %f2306;
	mov.f32 	%f2355, %f2306;
	mov.f32 	%f2356, %f2306;
	mov.f32 	%f2357, %f2306;
	mov.f32 	%f2358, %f2306;
	mov.f32 	%f2359, %f2306;
	mov.f32 	%f2360, %f2306;
	mov.f32 	%f2361, %f2306;
	mov.f32 	%f2362, %f2306;
	mov.f32 	%f2363, %f2306;
	mov.f32 	%f2364, %f2306;
	mov.f32 	%f2365, %f2306;
	mov.f32 	%f2366, %f2306;
	mov.f32 	%f2367, %f2306;
	mov.f32 	%f2368, %f2306;
	mov.f32 	%f2369, %f2306;
	mov.f32 	%f2370, %f2306;
	mov.f32 	%f2371, %f2306;
	mov.f32 	%f2372, %f2306;
	mov.f32 	%f2373, %f2306;
	mov.f32 	%f2374, %f2306;
	mov.f32 	%f2375, %f2306;
	mov.f32 	%f2376, %f2306;
	mov.f32 	%f2377, %f2306;
	mov.f32 	%f2378, %f2306;
	mov.f32 	%f2379, %f2306;
	mov.f32 	%f2380, %f2306;
	mov.f32 	%f2381, %f2306;
	mov.f32 	%f2382, %f2306;
	mov.f32 	%f2383, %f2306;
	mov.f32 	%f2384, %f2306;
	mov.f32 	%f2385, %f2306;
	mov.f32 	%f2386, %f2306;
	mov.f32 	%f2387, %f2306;
	mov.f32 	%f2388, %f2306;
	mov.f32 	%f2389, %f2306;
	mov.f32 	%f2390, %f2306;
	mov.f32 	%f2391, %f2306;
	mov.f32 	%f2392, %f2306;
	mov.f32 	%f2393, %f2306;
	mov.f32 	%f2394, %f2306;
	mov.f32 	%f2395, %f2306;
	mov.f32 	%f2396, %f2306;
	mov.f32 	%f2397, %f2306;
	mov.f32 	%f2398, %f2306;
	mov.f32 	%f2399, %f2306;
	mov.f32 	%f2400, %f2306;
	mov.f32 	%f2401, %f2306;
	mov.f32 	%f2402, %f2306;
	mov.f32 	%f2403, %f2306;
	mov.f32 	%f2404, %f2306;
	mov.f32 	%f2405, %f2306;
	mov.f32 	%f2406, %f2306;
	mov.f32 	%f2407, %f2306;
	mov.f32 	%f2408, %f2306;
	mov.f32 	%f2409, %f2306;
	mov.f32 	%f2410, %f2306;
	mov.f32 	%f2411, %f2306;
	mov.f32 	%f2412, %f2306;
	mov.f32 	%f2413, %f2306;
	mov.f32 	%f2414, %f2306;
	mov.f32 	%f2415, %f2306;
	mov.f32 	%f2416, %f2306;
	mov.f32 	%f2417, %f2306;
	mov.f32 	%f2418, %f2306;
	mov.f32 	%f2419, %f2306;
	mov.f32 	%f2420, %f2306;
	mov.f32 	%f2421, %f2306;
	mov.f32 	%f2422, %f2306;
	mov.f32 	%f2423, %f2306;
	mov.f32 	%f2424, %f2306;
	mov.f32 	%f2425, %f2306;
	mov.f32 	%f2426, %f2306;
	mov.f32 	%f2427, %f2306;
	mov.f32 	%f2428, %f2306;
	mov.f32 	%f2429, %f2306;
	mov.f32 	%f2430, %f2306;
	mov.f32 	%f2431, %f2306;
	mov.f32 	%f2432, %f2306;
	mov.f32 	%f2433, %f2306;
	mov.u32 	%r2775, %r2773;
$L__BB0_2:
	setp.lt.s32 	%p94, %r2775, %r88;
	.loc	1 330 20
	add.s32 	%r692, %r2772, %r2592;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r960, %r961, %r962, %r963 }, [ %r692 + 0 ];
	// end inline asm
	add.s32 	%r697, %r692, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1008, %r1009, %r1010, %r1011 }, [ %r697 + 0 ];
	// end inline asm
	add.s32 	%r702, %r692, 32768;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1056, %r1057, %r1058, %r1059 }, [ %r702 + 0 ];
	// end inline asm
	add.s32 	%r707, %r692, 49152;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1104, %r1105, %r1106, %r1107 }, [ %r707 + 0 ];
	// end inline asm
	.loc	1 331 20
	add.s32 	%r2594, %r2771, %r650;
	add.s32 	%r712, %r2594, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r964, %r965, %r970, %r971 }, [ %r712 + 0 ];
	// end inline asm
	add.s32 	%r2596, %r2771, %r654;
	add.s32 	%r717, %r2596, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r976, %r977, %r982, %r983 }, [ %r717 + 0 ];
	// end inline asm
	add.s32 	%r2598, %r2771, %r658;
	add.s32 	%r722, %r2598, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r988, %r989, %r994, %r995 }, [ %r722 + 0 ];
	// end inline asm
	add.s32 	%r2600, %r2771, %r662;
	add.s32 	%r727, %r2600, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1000, %r1001, %r1006, %r1007 }, [ %r727 + 0 ];
	// end inline asm
	.loc	1 333 35
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2306, %f2307, %f2308, %f2309 }, { %r2767, %r2768, %r2769, %r2770 }, { %r2751, %r2752 }, { %f2306, %f2307, %f2308, %f2309 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2310, %f2311, %f2312, %f2313 }, { %r2767, %r2768, %r2769, %r2770 }, { %r2753, %r2754 }, { %f2310, %f2311, %f2312, %f2313 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2314, %f2315, %f2316, %f2317 }, { %r2767, %r2768, %r2769, %r2770 }, { %r2747, %r2748 }, { %f2314, %f2315, %f2316, %f2317 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2318, %f2319, %f2320, %f2321 }, { %r2767, %r2768, %r2769, %r2770 }, { %r2749, %r2750 }, { %f2318, %f2319, %f2320, %f2321 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2322, %f2323, %f2324, %f2325 }, { %r2767, %r2768, %r2769, %r2770 }, { %r2743, %r2744 }, { %f2322, %f2323, %f2324, %f2325 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2326, %f2327, %f2328, %f2329 }, { %r2767, %r2768, %r2769, %r2770 }, { %r2745, %r2746 }, { %f2326, %f2327, %f2328, %f2329 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2330, %f2331, %f2332, %f2333 }, { %r2767, %r2768, %r2769, %r2770 }, { %r2739, %r2740 }, { %f2330, %f2331, %f2332, %f2333 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2334, %f2335, %f2336, %f2337 }, { %r2767, %r2768, %r2769, %r2770 }, { %r2741, %r2742 }, { %f2334, %f2335, %f2336, %f2337 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2338, %f2339, %f2340, %f2341 }, { %r2763, %r2764, %r2765, %r2766 }, { %r2751, %r2752 }, { %f2338, %f2339, %f2340, %f2341 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2342, %f2343, %f2344, %f2345 }, { %r2763, %r2764, %r2765, %r2766 }, { %r2753, %r2754 }, { %f2342, %f2343, %f2344, %f2345 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2346, %f2347, %f2348, %f2349 }, { %r2763, %r2764, %r2765, %r2766 }, { %r2747, %r2748 }, { %f2346, %f2347, %f2348, %f2349 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2350, %f2351, %f2352, %f2353 }, { %r2763, %r2764, %r2765, %r2766 }, { %r2749, %r2750 }, { %f2350, %f2351, %f2352, %f2353 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2354, %f2355, %f2356, %f2357 }, { %r2763, %r2764, %r2765, %r2766 }, { %r2743, %r2744 }, { %f2354, %f2355, %f2356, %f2357 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2358, %f2359, %f2360, %f2361 }, { %r2763, %r2764, %r2765, %r2766 }, { %r2745, %r2746 }, { %f2358, %f2359, %f2360, %f2361 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2362, %f2363, %f2364, %f2365 }, { %r2763, %r2764, %r2765, %r2766 }, { %r2739, %r2740 }, { %f2362, %f2363, %f2364, %f2365 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2366, %f2367, %f2368, %f2369 }, { %r2763, %r2764, %r2765, %r2766 }, { %r2741, %r2742 }, { %f2366, %f2367, %f2368, %f2369 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2370, %f2371, %f2372, %f2373 }, { %r2759, %r2760, %r2761, %r2762 }, { %r2751, %r2752 }, { %f2370, %f2371, %f2372, %f2373 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2374, %f2375, %f2376, %f2377 }, { %r2759, %r2760, %r2761, %r2762 }, { %r2753, %r2754 }, { %f2374, %f2375, %f2376, %f2377 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2378, %f2379, %f2380, %f2381 }, { %r2759, %r2760, %r2761, %r2762 }, { %r2747, %r2748 }, { %f2378, %f2379, %f2380, %f2381 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2382, %f2383, %f2384, %f2385 }, { %r2759, %r2760, %r2761, %r2762 }, { %r2749, %r2750 }, { %f2382, %f2383, %f2384, %f2385 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2386, %f2387, %f2388, %f2389 }, { %r2759, %r2760, %r2761, %r2762 }, { %r2743, %r2744 }, { %f2386, %f2387, %f2388, %f2389 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2390, %f2391, %f2392, %f2393 }, { %r2759, %r2760, %r2761, %r2762 }, { %r2745, %r2746 }, { %f2390, %f2391, %f2392, %f2393 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2394, %f2395, %f2396, %f2397 }, { %r2759, %r2760, %r2761, %r2762 }, { %r2739, %r2740 }, { %f2394, %f2395, %f2396, %f2397 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2398, %f2399, %f2400, %f2401 }, { %r2759, %r2760, %r2761, %r2762 }, { %r2741, %r2742 }, { %f2398, %f2399, %f2400, %f2401 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2402, %f2403, %f2404, %f2405 }, { %r2755, %r2756, %r2757, %r2758 }, { %r2751, %r2752 }, { %f2402, %f2403, %f2404, %f2405 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2406, %f2407, %f2408, %f2409 }, { %r2755, %r2756, %r2757, %r2758 }, { %r2753, %r2754 }, { %f2406, %f2407, %f2408, %f2409 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2410, %f2411, %f2412, %f2413 }, { %r2755, %r2756, %r2757, %r2758 }, { %r2747, %r2748 }, { %f2410, %f2411, %f2412, %f2413 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2414, %f2415, %f2416, %f2417 }, { %r2755, %r2756, %r2757, %r2758 }, { %r2749, %r2750 }, { %f2414, %f2415, %f2416, %f2417 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2418, %f2419, %f2420, %f2421 }, { %r2755, %r2756, %r2757, %r2758 }, { %r2743, %r2744 }, { %f2418, %f2419, %f2420, %f2421 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2422, %f2423, %f2424, %f2425 }, { %r2755, %r2756, %r2757, %r2758 }, { %r2745, %r2746 }, { %f2422, %f2423, %f2424, %f2425 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2426, %f2427, %f2428, %f2429 }, { %r2755, %r2756, %r2757, %r2758 }, { %r2739, %r2740 }, { %f2426, %f2427, %f2428, %f2429 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2430, %f2431, %f2432, %f2433 }, { %r2755, %r2756, %r2757, %r2758 }, { %r2741, %r2742 }, { %f2430, %f2431, %f2432, %f2433 };
	// end inline asm
	.loc	1 330 20
	add.s32 	%r924, %r2772, %r2601;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1192, %r1193, %r1194, %r1195 }, [ %r924 + 0 ];
	// end inline asm
	add.s32 	%r929, %r924, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1240, %r1241, %r1242, %r1243 }, [ %r929 + 0 ];
	// end inline asm
	add.s32 	%r934, %r924, 32768;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1288, %r1289, %r1290, %r1291 }, [ %r934 + 0 ];
	// end inline asm
	add.s32 	%r939, %r924, 49152;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1336, %r1337, %r1338, %r1339 }, [ %r939 + 0 ];
	// end inline asm
	.loc	1 331 20
	add.s32 	%r944, %r2594, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1196, %r1197, %r1202, %r1203 }, [ %r944 + 0 ];
	// end inline asm
	add.s32 	%r949, %r2596, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1208, %r1209, %r1214, %r1215 }, [ %r949 + 0 ];
	// end inline asm
	add.s32 	%r954, %r2598, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1220, %r1221, %r1226, %r1227 }, [ %r954 + 0 ];
	// end inline asm
	add.s32 	%r959, %r2600, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1232, %r1233, %r1238, %r1239 }, [ %r959 + 0 ];
	// end inline asm
	.loc	1 333 35
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2306, %f2307, %f2308, %f2309 }, { %r960, %r961, %r962, %r963 }, { %r964, %r965 }, { %f2306, %f2307, %f2308, %f2309 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2310, %f2311, %f2312, %f2313 }, { %r960, %r961, %r962, %r963 }, { %r970, %r971 }, { %f2310, %f2311, %f2312, %f2313 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2314, %f2315, %f2316, %f2317 }, { %r960, %r961, %r962, %r963 }, { %r976, %r977 }, { %f2314, %f2315, %f2316, %f2317 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2318, %f2319, %f2320, %f2321 }, { %r960, %r961, %r962, %r963 }, { %r982, %r983 }, { %f2318, %f2319, %f2320, %f2321 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2322, %f2323, %f2324, %f2325 }, { %r960, %r961, %r962, %r963 }, { %r988, %r989 }, { %f2322, %f2323, %f2324, %f2325 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2326, %f2327, %f2328, %f2329 }, { %r960, %r961, %r962, %r963 }, { %r994, %r995 }, { %f2326, %f2327, %f2328, %f2329 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2330, %f2331, %f2332, %f2333 }, { %r960, %r961, %r962, %r963 }, { %r1000, %r1001 }, { %f2330, %f2331, %f2332, %f2333 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2334, %f2335, %f2336, %f2337 }, { %r960, %r961, %r962, %r963 }, { %r1006, %r1007 }, { %f2334, %f2335, %f2336, %f2337 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2338, %f2339, %f2340, %f2341 }, { %r1008, %r1009, %r1010, %r1011 }, { %r964, %r965 }, { %f2338, %f2339, %f2340, %f2341 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2342, %f2343, %f2344, %f2345 }, { %r1008, %r1009, %r1010, %r1011 }, { %r970, %r971 }, { %f2342, %f2343, %f2344, %f2345 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2346, %f2347, %f2348, %f2349 }, { %r1008, %r1009, %r1010, %r1011 }, { %r976, %r977 }, { %f2346, %f2347, %f2348, %f2349 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2350, %f2351, %f2352, %f2353 }, { %r1008, %r1009, %r1010, %r1011 }, { %r982, %r983 }, { %f2350, %f2351, %f2352, %f2353 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2354, %f2355, %f2356, %f2357 }, { %r1008, %r1009, %r1010, %r1011 }, { %r988, %r989 }, { %f2354, %f2355, %f2356, %f2357 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2358, %f2359, %f2360, %f2361 }, { %r1008, %r1009, %r1010, %r1011 }, { %r994, %r995 }, { %f2358, %f2359, %f2360, %f2361 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2362, %f2363, %f2364, %f2365 }, { %r1008, %r1009, %r1010, %r1011 }, { %r1000, %r1001 }, { %f2362, %f2363, %f2364, %f2365 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2366, %f2367, %f2368, %f2369 }, { %r1008, %r1009, %r1010, %r1011 }, { %r1006, %r1007 }, { %f2366, %f2367, %f2368, %f2369 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2370, %f2371, %f2372, %f2373 }, { %r1056, %r1057, %r1058, %r1059 }, { %r964, %r965 }, { %f2370, %f2371, %f2372, %f2373 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2374, %f2375, %f2376, %f2377 }, { %r1056, %r1057, %r1058, %r1059 }, { %r970, %r971 }, { %f2374, %f2375, %f2376, %f2377 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2378, %f2379, %f2380, %f2381 }, { %r1056, %r1057, %r1058, %r1059 }, { %r976, %r977 }, { %f2378, %f2379, %f2380, %f2381 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2382, %f2383, %f2384, %f2385 }, { %r1056, %r1057, %r1058, %r1059 }, { %r982, %r983 }, { %f2382, %f2383, %f2384, %f2385 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2386, %f2387, %f2388, %f2389 }, { %r1056, %r1057, %r1058, %r1059 }, { %r988, %r989 }, { %f2386, %f2387, %f2388, %f2389 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2390, %f2391, %f2392, %f2393 }, { %r1056, %r1057, %r1058, %r1059 }, { %r994, %r995 }, { %f2390, %f2391, %f2392, %f2393 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2394, %f2395, %f2396, %f2397 }, { %r1056, %r1057, %r1058, %r1059 }, { %r1000, %r1001 }, { %f2394, %f2395, %f2396, %f2397 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2398, %f2399, %f2400, %f2401 }, { %r1056, %r1057, %r1058, %r1059 }, { %r1006, %r1007 }, { %f2398, %f2399, %f2400, %f2401 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2402, %f2403, %f2404, %f2405 }, { %r1104, %r1105, %r1106, %r1107 }, { %r964, %r965 }, { %f2402, %f2403, %f2404, %f2405 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2406, %f2407, %f2408, %f2409 }, { %r1104, %r1105, %r1106, %r1107 }, { %r970, %r971 }, { %f2406, %f2407, %f2408, %f2409 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2410, %f2411, %f2412, %f2413 }, { %r1104, %r1105, %r1106, %r1107 }, { %r976, %r977 }, { %f2410, %f2411, %f2412, %f2413 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2414, %f2415, %f2416, %f2417 }, { %r1104, %r1105, %r1106, %r1107 }, { %r982, %r983 }, { %f2414, %f2415, %f2416, %f2417 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2418, %f2419, %f2420, %f2421 }, { %r1104, %r1105, %r1106, %r1107 }, { %r988, %r989 }, { %f2418, %f2419, %f2420, %f2421 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2422, %f2423, %f2424, %f2425 }, { %r1104, %r1105, %r1106, %r1107 }, { %r994, %r995 }, { %f2422, %f2423, %f2424, %f2425 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2426, %f2427, %f2428, %f2429 }, { %r1104, %r1105, %r1106, %r1107 }, { %r1000, %r1001 }, { %f2426, %f2427, %f2428, %f2429 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2430, %f2431, %f2432, %f2433 }, { %r1104, %r1105, %r1106, %r1107 }, { %r1006, %r1007 }, { %f2430, %f2431, %f2432, %f2433 };
	// end inline asm
	.loc	1 330 20
	add.s32 	%r1156, %r2772, %r2602;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1424, %r1425, %r1426, %r1427 }, [ %r1156 + 0 ];
	// end inline asm
	add.s32 	%r1161, %r1156, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1472, %r1473, %r1474, %r1475 }, [ %r1161 + 0 ];
	// end inline asm
	add.s32 	%r1166, %r1156, 32768;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1520, %r1521, %r1522, %r1523 }, [ %r1166 + 0 ];
	// end inline asm
	add.s32 	%r1171, %r1156, 49152;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1568, %r1569, %r1570, %r1571 }, [ %r1171 + 0 ];
	// end inline asm
	.loc	1 331 20
	add.s32 	%r1176, %r2594, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1428, %r1429, %r1434, %r1435 }, [ %r1176 + 0 ];
	// end inline asm
	add.s32 	%r1181, %r2596, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1440, %r1441, %r1446, %r1447 }, [ %r1181 + 0 ];
	// end inline asm
	add.s32 	%r1186, %r2598, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1452, %r1453, %r1458, %r1459 }, [ %r1186 + 0 ];
	// end inline asm
	add.s32 	%r1191, %r2600, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1464, %r1465, %r1470, %r1471 }, [ %r1191 + 0 ];
	// end inline asm
	.loc	1 333 35
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2306, %f2307, %f2308, %f2309 }, { %r1192, %r1193, %r1194, %r1195 }, { %r1196, %r1197 }, { %f2306, %f2307, %f2308, %f2309 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2310, %f2311, %f2312, %f2313 }, { %r1192, %r1193, %r1194, %r1195 }, { %r1202, %r1203 }, { %f2310, %f2311, %f2312, %f2313 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2314, %f2315, %f2316, %f2317 }, { %r1192, %r1193, %r1194, %r1195 }, { %r1208, %r1209 }, { %f2314, %f2315, %f2316, %f2317 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2318, %f2319, %f2320, %f2321 }, { %r1192, %r1193, %r1194, %r1195 }, { %r1214, %r1215 }, { %f2318, %f2319, %f2320, %f2321 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2322, %f2323, %f2324, %f2325 }, { %r1192, %r1193, %r1194, %r1195 }, { %r1220, %r1221 }, { %f2322, %f2323, %f2324, %f2325 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2326, %f2327, %f2328, %f2329 }, { %r1192, %r1193, %r1194, %r1195 }, { %r1226, %r1227 }, { %f2326, %f2327, %f2328, %f2329 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2330, %f2331, %f2332, %f2333 }, { %r1192, %r1193, %r1194, %r1195 }, { %r1232, %r1233 }, { %f2330, %f2331, %f2332, %f2333 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2334, %f2335, %f2336, %f2337 }, { %r1192, %r1193, %r1194, %r1195 }, { %r1238, %r1239 }, { %f2334, %f2335, %f2336, %f2337 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2338, %f2339, %f2340, %f2341 }, { %r1240, %r1241, %r1242, %r1243 }, { %r1196, %r1197 }, { %f2338, %f2339, %f2340, %f2341 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2342, %f2343, %f2344, %f2345 }, { %r1240, %r1241, %r1242, %r1243 }, { %r1202, %r1203 }, { %f2342, %f2343, %f2344, %f2345 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2346, %f2347, %f2348, %f2349 }, { %r1240, %r1241, %r1242, %r1243 }, { %r1208, %r1209 }, { %f2346, %f2347, %f2348, %f2349 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2350, %f2351, %f2352, %f2353 }, { %r1240, %r1241, %r1242, %r1243 }, { %r1214, %r1215 }, { %f2350, %f2351, %f2352, %f2353 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2354, %f2355, %f2356, %f2357 }, { %r1240, %r1241, %r1242, %r1243 }, { %r1220, %r1221 }, { %f2354, %f2355, %f2356, %f2357 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2358, %f2359, %f2360, %f2361 }, { %r1240, %r1241, %r1242, %r1243 }, { %r1226, %r1227 }, { %f2358, %f2359, %f2360, %f2361 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2362, %f2363, %f2364, %f2365 }, { %r1240, %r1241, %r1242, %r1243 }, { %r1232, %r1233 }, { %f2362, %f2363, %f2364, %f2365 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2366, %f2367, %f2368, %f2369 }, { %r1240, %r1241, %r1242, %r1243 }, { %r1238, %r1239 }, { %f2366, %f2367, %f2368, %f2369 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2370, %f2371, %f2372, %f2373 }, { %r1288, %r1289, %r1290, %r1291 }, { %r1196, %r1197 }, { %f2370, %f2371, %f2372, %f2373 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2374, %f2375, %f2376, %f2377 }, { %r1288, %r1289, %r1290, %r1291 }, { %r1202, %r1203 }, { %f2374, %f2375, %f2376, %f2377 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2378, %f2379, %f2380, %f2381 }, { %r1288, %r1289, %r1290, %r1291 }, { %r1208, %r1209 }, { %f2378, %f2379, %f2380, %f2381 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2382, %f2383, %f2384, %f2385 }, { %r1288, %r1289, %r1290, %r1291 }, { %r1214, %r1215 }, { %f2382, %f2383, %f2384, %f2385 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2386, %f2387, %f2388, %f2389 }, { %r1288, %r1289, %r1290, %r1291 }, { %r1220, %r1221 }, { %f2386, %f2387, %f2388, %f2389 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2390, %f2391, %f2392, %f2393 }, { %r1288, %r1289, %r1290, %r1291 }, { %r1226, %r1227 }, { %f2390, %f2391, %f2392, %f2393 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2394, %f2395, %f2396, %f2397 }, { %r1288, %r1289, %r1290, %r1291 }, { %r1232, %r1233 }, { %f2394, %f2395, %f2396, %f2397 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2398, %f2399, %f2400, %f2401 }, { %r1288, %r1289, %r1290, %r1291 }, { %r1238, %r1239 }, { %f2398, %f2399, %f2400, %f2401 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2402, %f2403, %f2404, %f2405 }, { %r1336, %r1337, %r1338, %r1339 }, { %r1196, %r1197 }, { %f2402, %f2403, %f2404, %f2405 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2406, %f2407, %f2408, %f2409 }, { %r1336, %r1337, %r1338, %r1339 }, { %r1202, %r1203 }, { %f2406, %f2407, %f2408, %f2409 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2410, %f2411, %f2412, %f2413 }, { %r1336, %r1337, %r1338, %r1339 }, { %r1208, %r1209 }, { %f2410, %f2411, %f2412, %f2413 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2414, %f2415, %f2416, %f2417 }, { %r1336, %r1337, %r1338, %r1339 }, { %r1214, %r1215 }, { %f2414, %f2415, %f2416, %f2417 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2418, %f2419, %f2420, %f2421 }, { %r1336, %r1337, %r1338, %r1339 }, { %r1220, %r1221 }, { %f2418, %f2419, %f2420, %f2421 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2422, %f2423, %f2424, %f2425 }, { %r1336, %r1337, %r1338, %r1339 }, { %r1226, %r1227 }, { %f2422, %f2423, %f2424, %f2425 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2426, %f2427, %f2428, %f2429 }, { %r1336, %r1337, %r1338, %r1339 }, { %r1232, %r1233 }, { %f2426, %f2427, %f2428, %f2429 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2430, %f2431, %f2432, %f2433 }, { %r1336, %r1337, %r1338, %r1339 }, { %r1238, %r1239 }, { %f2430, %f2431, %f2432, %f2433 };
	// end inline asm
	.loc	1 330 20
	add.s32 	%r1388, %r2772, %r2603;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1656, %r1657, %r1658, %r1659 }, [ %r1388 + 0 ];
	// end inline asm
	add.s32 	%r1393, %r1388, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1704, %r1705, %r1706, %r1707 }, [ %r1393 + 0 ];
	// end inline asm
	add.s32 	%r1398, %r1388, 32768;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1752, %r1753, %r1754, %r1755 }, [ %r1398 + 0 ];
	// end inline asm
	add.s32 	%r1403, %r1388, 49152;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1800, %r1801, %r1802, %r1803 }, [ %r1403 + 0 ];
	// end inline asm
	.loc	1 331 20
	add.s32 	%r1408, %r2594, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1660, %r1661, %r1666, %r1667 }, [ %r1408 + 0 ];
	// end inline asm
	add.s32 	%r1413, %r2596, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1672, %r1673, %r1678, %r1679 }, [ %r1413 + 0 ];
	// end inline asm
	add.s32 	%r1418, %r2598, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1684, %r1685, %r1690, %r1691 }, [ %r1418 + 0 ];
	// end inline asm
	add.s32 	%r1423, %r2600, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1696, %r1697, %r1702, %r1703 }, [ %r1423 + 0 ];
	// end inline asm
	.loc	1 333 35
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2306, %f2307, %f2308, %f2309 }, { %r1424, %r1425, %r1426, %r1427 }, { %r1428, %r1429 }, { %f2306, %f2307, %f2308, %f2309 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2310, %f2311, %f2312, %f2313 }, { %r1424, %r1425, %r1426, %r1427 }, { %r1434, %r1435 }, { %f2310, %f2311, %f2312, %f2313 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2314, %f2315, %f2316, %f2317 }, { %r1424, %r1425, %r1426, %r1427 }, { %r1440, %r1441 }, { %f2314, %f2315, %f2316, %f2317 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2318, %f2319, %f2320, %f2321 }, { %r1424, %r1425, %r1426, %r1427 }, { %r1446, %r1447 }, { %f2318, %f2319, %f2320, %f2321 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2322, %f2323, %f2324, %f2325 }, { %r1424, %r1425, %r1426, %r1427 }, { %r1452, %r1453 }, { %f2322, %f2323, %f2324, %f2325 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2326, %f2327, %f2328, %f2329 }, { %r1424, %r1425, %r1426, %r1427 }, { %r1458, %r1459 }, { %f2326, %f2327, %f2328, %f2329 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2330, %f2331, %f2332, %f2333 }, { %r1424, %r1425, %r1426, %r1427 }, { %r1464, %r1465 }, { %f2330, %f2331, %f2332, %f2333 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2334, %f2335, %f2336, %f2337 }, { %r1424, %r1425, %r1426, %r1427 }, { %r1470, %r1471 }, { %f2334, %f2335, %f2336, %f2337 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2338, %f2339, %f2340, %f2341 }, { %r1472, %r1473, %r1474, %r1475 }, { %r1428, %r1429 }, { %f2338, %f2339, %f2340, %f2341 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2342, %f2343, %f2344, %f2345 }, { %r1472, %r1473, %r1474, %r1475 }, { %r1434, %r1435 }, { %f2342, %f2343, %f2344, %f2345 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2346, %f2347, %f2348, %f2349 }, { %r1472, %r1473, %r1474, %r1475 }, { %r1440, %r1441 }, { %f2346, %f2347, %f2348, %f2349 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2350, %f2351, %f2352, %f2353 }, { %r1472, %r1473, %r1474, %r1475 }, { %r1446, %r1447 }, { %f2350, %f2351, %f2352, %f2353 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2354, %f2355, %f2356, %f2357 }, { %r1472, %r1473, %r1474, %r1475 }, { %r1452, %r1453 }, { %f2354, %f2355, %f2356, %f2357 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2358, %f2359, %f2360, %f2361 }, { %r1472, %r1473, %r1474, %r1475 }, { %r1458, %r1459 }, { %f2358, %f2359, %f2360, %f2361 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2362, %f2363, %f2364, %f2365 }, { %r1472, %r1473, %r1474, %r1475 }, { %r1464, %r1465 }, { %f2362, %f2363, %f2364, %f2365 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2366, %f2367, %f2368, %f2369 }, { %r1472, %r1473, %r1474, %r1475 }, { %r1470, %r1471 }, { %f2366, %f2367, %f2368, %f2369 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2370, %f2371, %f2372, %f2373 }, { %r1520, %r1521, %r1522, %r1523 }, { %r1428, %r1429 }, { %f2370, %f2371, %f2372, %f2373 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2374, %f2375, %f2376, %f2377 }, { %r1520, %r1521, %r1522, %r1523 }, { %r1434, %r1435 }, { %f2374, %f2375, %f2376, %f2377 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2378, %f2379, %f2380, %f2381 }, { %r1520, %r1521, %r1522, %r1523 }, { %r1440, %r1441 }, { %f2378, %f2379, %f2380, %f2381 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2382, %f2383, %f2384, %f2385 }, { %r1520, %r1521, %r1522, %r1523 }, { %r1446, %r1447 }, { %f2382, %f2383, %f2384, %f2385 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2386, %f2387, %f2388, %f2389 }, { %r1520, %r1521, %r1522, %r1523 }, { %r1452, %r1453 }, { %f2386, %f2387, %f2388, %f2389 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2390, %f2391, %f2392, %f2393 }, { %r1520, %r1521, %r1522, %r1523 }, { %r1458, %r1459 }, { %f2390, %f2391, %f2392, %f2393 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2394, %f2395, %f2396, %f2397 }, { %r1520, %r1521, %r1522, %r1523 }, { %r1464, %r1465 }, { %f2394, %f2395, %f2396, %f2397 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2398, %f2399, %f2400, %f2401 }, { %r1520, %r1521, %r1522, %r1523 }, { %r1470, %r1471 }, { %f2398, %f2399, %f2400, %f2401 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2402, %f2403, %f2404, %f2405 }, { %r1568, %r1569, %r1570, %r1571 }, { %r1428, %r1429 }, { %f2402, %f2403, %f2404, %f2405 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2406, %f2407, %f2408, %f2409 }, { %r1568, %r1569, %r1570, %r1571 }, { %r1434, %r1435 }, { %f2406, %f2407, %f2408, %f2409 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2410, %f2411, %f2412, %f2413 }, { %r1568, %r1569, %r1570, %r1571 }, { %r1440, %r1441 }, { %f2410, %f2411, %f2412, %f2413 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2414, %f2415, %f2416, %f2417 }, { %r1568, %r1569, %r1570, %r1571 }, { %r1446, %r1447 }, { %f2414, %f2415, %f2416, %f2417 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2418, %f2419, %f2420, %f2421 }, { %r1568, %r1569, %r1570, %r1571 }, { %r1452, %r1453 }, { %f2418, %f2419, %f2420, %f2421 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2422, %f2423, %f2424, %f2425 }, { %r1568, %r1569, %r1570, %r1571 }, { %r1458, %r1459 }, { %f2422, %f2423, %f2424, %f2425 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2426, %f2427, %f2428, %f2429 }, { %r1568, %r1569, %r1570, %r1571 }, { %r1464, %r1465 }, { %f2426, %f2427, %f2428, %f2429 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2430, %f2431, %f2432, %f2433 }, { %r1568, %r1569, %r1570, %r1571 }, { %r1470, %r1471 }, { %f2430, %f2431, %f2432, %f2433 };
	// end inline asm
	.loc	1 330 20
	add.s32 	%r1620, %r2772, %r2604;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1888, %r1889, %r1890, %r1891 }, [ %r1620 + 0 ];
	// end inline asm
	add.s32 	%r1625, %r1620, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1936, %r1937, %r1938, %r1939 }, [ %r1625 + 0 ];
	// end inline asm
	add.s32 	%r1630, %r1620, 32768;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1984, %r1985, %r1986, %r1987 }, [ %r1630 + 0 ];
	// end inline asm
	add.s32 	%r1635, %r1620, 49152;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2032, %r2033, %r2034, %r2035 }, [ %r1635 + 0 ];
	// end inline asm
	.loc	1 331 20
	add.s32 	%r1640, %r2594, 20480;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1892, %r1893, %r1898, %r1899 }, [ %r1640 + 0 ];
	// end inline asm
	add.s32 	%r1645, %r2596, 20480;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1904, %r1905, %r1910, %r1911 }, [ %r1645 + 0 ];
	// end inline asm
	add.s32 	%r1650, %r2598, 20480;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1916, %r1917, %r1922, %r1923 }, [ %r1650 + 0 ];
	// end inline asm
	add.s32 	%r1655, %r2600, 20480;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1928, %r1929, %r1934, %r1935 }, [ %r1655 + 0 ];
	// end inline asm
	.loc	1 333 35
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2306, %f2307, %f2308, %f2309 }, { %r1656, %r1657, %r1658, %r1659 }, { %r1660, %r1661 }, { %f2306, %f2307, %f2308, %f2309 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2310, %f2311, %f2312, %f2313 }, { %r1656, %r1657, %r1658, %r1659 }, { %r1666, %r1667 }, { %f2310, %f2311, %f2312, %f2313 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2314, %f2315, %f2316, %f2317 }, { %r1656, %r1657, %r1658, %r1659 }, { %r1672, %r1673 }, { %f2314, %f2315, %f2316, %f2317 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2318, %f2319, %f2320, %f2321 }, { %r1656, %r1657, %r1658, %r1659 }, { %r1678, %r1679 }, { %f2318, %f2319, %f2320, %f2321 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2322, %f2323, %f2324, %f2325 }, { %r1656, %r1657, %r1658, %r1659 }, { %r1684, %r1685 }, { %f2322, %f2323, %f2324, %f2325 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2326, %f2327, %f2328, %f2329 }, { %r1656, %r1657, %r1658, %r1659 }, { %r1690, %r1691 }, { %f2326, %f2327, %f2328, %f2329 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2330, %f2331, %f2332, %f2333 }, { %r1656, %r1657, %r1658, %r1659 }, { %r1696, %r1697 }, { %f2330, %f2331, %f2332, %f2333 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2334, %f2335, %f2336, %f2337 }, { %r1656, %r1657, %r1658, %r1659 }, { %r1702, %r1703 }, { %f2334, %f2335, %f2336, %f2337 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2338, %f2339, %f2340, %f2341 }, { %r1704, %r1705, %r1706, %r1707 }, { %r1660, %r1661 }, { %f2338, %f2339, %f2340, %f2341 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2342, %f2343, %f2344, %f2345 }, { %r1704, %r1705, %r1706, %r1707 }, { %r1666, %r1667 }, { %f2342, %f2343, %f2344, %f2345 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2346, %f2347, %f2348, %f2349 }, { %r1704, %r1705, %r1706, %r1707 }, { %r1672, %r1673 }, { %f2346, %f2347, %f2348, %f2349 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2350, %f2351, %f2352, %f2353 }, { %r1704, %r1705, %r1706, %r1707 }, { %r1678, %r1679 }, { %f2350, %f2351, %f2352, %f2353 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2354, %f2355, %f2356, %f2357 }, { %r1704, %r1705, %r1706, %r1707 }, { %r1684, %r1685 }, { %f2354, %f2355, %f2356, %f2357 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2358, %f2359, %f2360, %f2361 }, { %r1704, %r1705, %r1706, %r1707 }, { %r1690, %r1691 }, { %f2358, %f2359, %f2360, %f2361 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2362, %f2363, %f2364, %f2365 }, { %r1704, %r1705, %r1706, %r1707 }, { %r1696, %r1697 }, { %f2362, %f2363, %f2364, %f2365 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2366, %f2367, %f2368, %f2369 }, { %r1704, %r1705, %r1706, %r1707 }, { %r1702, %r1703 }, { %f2366, %f2367, %f2368, %f2369 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2370, %f2371, %f2372, %f2373 }, { %r1752, %r1753, %r1754, %r1755 }, { %r1660, %r1661 }, { %f2370, %f2371, %f2372, %f2373 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2374, %f2375, %f2376, %f2377 }, { %r1752, %r1753, %r1754, %r1755 }, { %r1666, %r1667 }, { %f2374, %f2375, %f2376, %f2377 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2378, %f2379, %f2380, %f2381 }, { %r1752, %r1753, %r1754, %r1755 }, { %r1672, %r1673 }, { %f2378, %f2379, %f2380, %f2381 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2382, %f2383, %f2384, %f2385 }, { %r1752, %r1753, %r1754, %r1755 }, { %r1678, %r1679 }, { %f2382, %f2383, %f2384, %f2385 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2386, %f2387, %f2388, %f2389 }, { %r1752, %r1753, %r1754, %r1755 }, { %r1684, %r1685 }, { %f2386, %f2387, %f2388, %f2389 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2390, %f2391, %f2392, %f2393 }, { %r1752, %r1753, %r1754, %r1755 }, { %r1690, %r1691 }, { %f2390, %f2391, %f2392, %f2393 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2394, %f2395, %f2396, %f2397 }, { %r1752, %r1753, %r1754, %r1755 }, { %r1696, %r1697 }, { %f2394, %f2395, %f2396, %f2397 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2398, %f2399, %f2400, %f2401 }, { %r1752, %r1753, %r1754, %r1755 }, { %r1702, %r1703 }, { %f2398, %f2399, %f2400, %f2401 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2402, %f2403, %f2404, %f2405 }, { %r1800, %r1801, %r1802, %r1803 }, { %r1660, %r1661 }, { %f2402, %f2403, %f2404, %f2405 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2406, %f2407, %f2408, %f2409 }, { %r1800, %r1801, %r1802, %r1803 }, { %r1666, %r1667 }, { %f2406, %f2407, %f2408, %f2409 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2410, %f2411, %f2412, %f2413 }, { %r1800, %r1801, %r1802, %r1803 }, { %r1672, %r1673 }, { %f2410, %f2411, %f2412, %f2413 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2414, %f2415, %f2416, %f2417 }, { %r1800, %r1801, %r1802, %r1803 }, { %r1678, %r1679 }, { %f2414, %f2415, %f2416, %f2417 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2418, %f2419, %f2420, %f2421 }, { %r1800, %r1801, %r1802, %r1803 }, { %r1684, %r1685 }, { %f2418, %f2419, %f2420, %f2421 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2422, %f2423, %f2424, %f2425 }, { %r1800, %r1801, %r1802, %r1803 }, { %r1690, %r1691 }, { %f2422, %f2423, %f2424, %f2425 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2426, %f2427, %f2428, %f2429 }, { %r1800, %r1801, %r1802, %r1803 }, { %r1696, %r1697 }, { %f2426, %f2427, %f2428, %f2429 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2430, %f2431, %f2432, %f2433 }, { %r1800, %r1801, %r1802, %r1803 }, { %r1702, %r1703 }, { %f2430, %f2431, %f2432, %f2433 };
	// end inline asm
	.loc	1 330 20
	add.s32 	%r1852, %r2772, %r2605;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2120, %r2121, %r2122, %r2123 }, [ %r1852 + 0 ];
	// end inline asm
	add.s32 	%r1857, %r1852, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2168, %r2169, %r2170, %r2171 }, [ %r1857 + 0 ];
	// end inline asm
	add.s32 	%r1862, %r1852, 32768;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2216, %r2217, %r2218, %r2219 }, [ %r1862 + 0 ];
	// end inline asm
	add.s32 	%r1867, %r1852, 49152;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2264, %r2265, %r2266, %r2267 }, [ %r1867 + 0 ];
	// end inline asm
	.loc	1 331 20
	add.s32 	%r1872, %r2594, 24576;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2124, %r2125, %r2130, %r2131 }, [ %r1872 + 0 ];
	// end inline asm
	add.s32 	%r1877, %r2596, 24576;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2136, %r2137, %r2142, %r2143 }, [ %r1877 + 0 ];
	// end inline asm
	add.s32 	%r1882, %r2598, 24576;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2148, %r2149, %r2154, %r2155 }, [ %r1882 + 0 ];
	// end inline asm
	add.s32 	%r1887, %r2600, 24576;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2160, %r2161, %r2166, %r2167 }, [ %r1887 + 0 ];
	// end inline asm
	.loc	1 333 35
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2306, %f2307, %f2308, %f2309 }, { %r1888, %r1889, %r1890, %r1891 }, { %r1892, %r1893 }, { %f2306, %f2307, %f2308, %f2309 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2310, %f2311, %f2312, %f2313 }, { %r1888, %r1889, %r1890, %r1891 }, { %r1898, %r1899 }, { %f2310, %f2311, %f2312, %f2313 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2314, %f2315, %f2316, %f2317 }, { %r1888, %r1889, %r1890, %r1891 }, { %r1904, %r1905 }, { %f2314, %f2315, %f2316, %f2317 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2318, %f2319, %f2320, %f2321 }, { %r1888, %r1889, %r1890, %r1891 }, { %r1910, %r1911 }, { %f2318, %f2319, %f2320, %f2321 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2322, %f2323, %f2324, %f2325 }, { %r1888, %r1889, %r1890, %r1891 }, { %r1916, %r1917 }, { %f2322, %f2323, %f2324, %f2325 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2326, %f2327, %f2328, %f2329 }, { %r1888, %r1889, %r1890, %r1891 }, { %r1922, %r1923 }, { %f2326, %f2327, %f2328, %f2329 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2330, %f2331, %f2332, %f2333 }, { %r1888, %r1889, %r1890, %r1891 }, { %r1928, %r1929 }, { %f2330, %f2331, %f2332, %f2333 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2334, %f2335, %f2336, %f2337 }, { %r1888, %r1889, %r1890, %r1891 }, { %r1934, %r1935 }, { %f2334, %f2335, %f2336, %f2337 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2338, %f2339, %f2340, %f2341 }, { %r1936, %r1937, %r1938, %r1939 }, { %r1892, %r1893 }, { %f2338, %f2339, %f2340, %f2341 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2342, %f2343, %f2344, %f2345 }, { %r1936, %r1937, %r1938, %r1939 }, { %r1898, %r1899 }, { %f2342, %f2343, %f2344, %f2345 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2346, %f2347, %f2348, %f2349 }, { %r1936, %r1937, %r1938, %r1939 }, { %r1904, %r1905 }, { %f2346, %f2347, %f2348, %f2349 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2350, %f2351, %f2352, %f2353 }, { %r1936, %r1937, %r1938, %r1939 }, { %r1910, %r1911 }, { %f2350, %f2351, %f2352, %f2353 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2354, %f2355, %f2356, %f2357 }, { %r1936, %r1937, %r1938, %r1939 }, { %r1916, %r1917 }, { %f2354, %f2355, %f2356, %f2357 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2358, %f2359, %f2360, %f2361 }, { %r1936, %r1937, %r1938, %r1939 }, { %r1922, %r1923 }, { %f2358, %f2359, %f2360, %f2361 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2362, %f2363, %f2364, %f2365 }, { %r1936, %r1937, %r1938, %r1939 }, { %r1928, %r1929 }, { %f2362, %f2363, %f2364, %f2365 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2366, %f2367, %f2368, %f2369 }, { %r1936, %r1937, %r1938, %r1939 }, { %r1934, %r1935 }, { %f2366, %f2367, %f2368, %f2369 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2370, %f2371, %f2372, %f2373 }, { %r1984, %r1985, %r1986, %r1987 }, { %r1892, %r1893 }, { %f2370, %f2371, %f2372, %f2373 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2374, %f2375, %f2376, %f2377 }, { %r1984, %r1985, %r1986, %r1987 }, { %r1898, %r1899 }, { %f2374, %f2375, %f2376, %f2377 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2378, %f2379, %f2380, %f2381 }, { %r1984, %r1985, %r1986, %r1987 }, { %r1904, %r1905 }, { %f2378, %f2379, %f2380, %f2381 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2382, %f2383, %f2384, %f2385 }, { %r1984, %r1985, %r1986, %r1987 }, { %r1910, %r1911 }, { %f2382, %f2383, %f2384, %f2385 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2386, %f2387, %f2388, %f2389 }, { %r1984, %r1985, %r1986, %r1987 }, { %r1916, %r1917 }, { %f2386, %f2387, %f2388, %f2389 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2390, %f2391, %f2392, %f2393 }, { %r1984, %r1985, %r1986, %r1987 }, { %r1922, %r1923 }, { %f2390, %f2391, %f2392, %f2393 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2394, %f2395, %f2396, %f2397 }, { %r1984, %r1985, %r1986, %r1987 }, { %r1928, %r1929 }, { %f2394, %f2395, %f2396, %f2397 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2398, %f2399, %f2400, %f2401 }, { %r1984, %r1985, %r1986, %r1987 }, { %r1934, %r1935 }, { %f2398, %f2399, %f2400, %f2401 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2402, %f2403, %f2404, %f2405 }, { %r2032, %r2033, %r2034, %r2035 }, { %r1892, %r1893 }, { %f2402, %f2403, %f2404, %f2405 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2406, %f2407, %f2408, %f2409 }, { %r2032, %r2033, %r2034, %r2035 }, { %r1898, %r1899 }, { %f2406, %f2407, %f2408, %f2409 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2410, %f2411, %f2412, %f2413 }, { %r2032, %r2033, %r2034, %r2035 }, { %r1904, %r1905 }, { %f2410, %f2411, %f2412, %f2413 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2414, %f2415, %f2416, %f2417 }, { %r2032, %r2033, %r2034, %r2035 }, { %r1910, %r1911 }, { %f2414, %f2415, %f2416, %f2417 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2418, %f2419, %f2420, %f2421 }, { %r2032, %r2033, %r2034, %r2035 }, { %r1916, %r1917 }, { %f2418, %f2419, %f2420, %f2421 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2422, %f2423, %f2424, %f2425 }, { %r2032, %r2033, %r2034, %r2035 }, { %r1922, %r1923 }, { %f2422, %f2423, %f2424, %f2425 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2426, %f2427, %f2428, %f2429 }, { %r2032, %r2033, %r2034, %r2035 }, { %r1928, %r1929 }, { %f2426, %f2427, %f2428, %f2429 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2430, %f2431, %f2432, %f2433 }, { %r2032, %r2033, %r2034, %r2035 }, { %r1934, %r1935 }, { %f2430, %f2431, %f2432, %f2433 };
	// end inline asm
	.loc	1 330 20
	add.s32 	%r2084, %r2772, %r2606;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2312, %r2313, %r2314, %r2315 }, [ %r2084 + 0 ];
	// end inline asm
	add.s32 	%r2089, %r2084, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2360, %r2361, %r2362, %r2363 }, [ %r2089 + 0 ];
	// end inline asm
	add.s32 	%r2094, %r2084, 32768;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2408, %r2409, %r2410, %r2411 }, [ %r2094 + 0 ];
	// end inline asm
	add.s32 	%r2099, %r2084, 49152;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2456, %r2457, %r2458, %r2459 }, [ %r2099 + 0 ];
	// end inline asm
	.loc	1 331 20
	add.s32 	%r2104, %r2594, 28672;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2316, %r2317, %r2322, %r2323 }, [ %r2104 + 0 ];
	// end inline asm
	add.s32 	%r2109, %r2596, 28672;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2328, %r2329, %r2334, %r2335 }, [ %r2109 + 0 ];
	// end inline asm
	add.s32 	%r2114, %r2598, 28672;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2340, %r2341, %r2346, %r2347 }, [ %r2114 + 0 ];
	// end inline asm
	add.s32 	%r2119, %r2600, 28672;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2352, %r2353, %r2358, %r2359 }, [ %r2119 + 0 ];
	// end inline asm
	.loc	1 333 35
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2306, %f2307, %f2308, %f2309 }, { %r2120, %r2121, %r2122, %r2123 }, { %r2124, %r2125 }, { %f2306, %f2307, %f2308, %f2309 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2310, %f2311, %f2312, %f2313 }, { %r2120, %r2121, %r2122, %r2123 }, { %r2130, %r2131 }, { %f2310, %f2311, %f2312, %f2313 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2314, %f2315, %f2316, %f2317 }, { %r2120, %r2121, %r2122, %r2123 }, { %r2136, %r2137 }, { %f2314, %f2315, %f2316, %f2317 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2318, %f2319, %f2320, %f2321 }, { %r2120, %r2121, %r2122, %r2123 }, { %r2142, %r2143 }, { %f2318, %f2319, %f2320, %f2321 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2322, %f2323, %f2324, %f2325 }, { %r2120, %r2121, %r2122, %r2123 }, { %r2148, %r2149 }, { %f2322, %f2323, %f2324, %f2325 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2326, %f2327, %f2328, %f2329 }, { %r2120, %r2121, %r2122, %r2123 }, { %r2154, %r2155 }, { %f2326, %f2327, %f2328, %f2329 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2330, %f2331, %f2332, %f2333 }, { %r2120, %r2121, %r2122, %r2123 }, { %r2160, %r2161 }, { %f2330, %f2331, %f2332, %f2333 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2334, %f2335, %f2336, %f2337 }, { %r2120, %r2121, %r2122, %r2123 }, { %r2166, %r2167 }, { %f2334, %f2335, %f2336, %f2337 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2338, %f2339, %f2340, %f2341 }, { %r2168, %r2169, %r2170, %r2171 }, { %r2124, %r2125 }, { %f2338, %f2339, %f2340, %f2341 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2342, %f2343, %f2344, %f2345 }, { %r2168, %r2169, %r2170, %r2171 }, { %r2130, %r2131 }, { %f2342, %f2343, %f2344, %f2345 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2346, %f2347, %f2348, %f2349 }, { %r2168, %r2169, %r2170, %r2171 }, { %r2136, %r2137 }, { %f2346, %f2347, %f2348, %f2349 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2350, %f2351, %f2352, %f2353 }, { %r2168, %r2169, %r2170, %r2171 }, { %r2142, %r2143 }, { %f2350, %f2351, %f2352, %f2353 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2354, %f2355, %f2356, %f2357 }, { %r2168, %r2169, %r2170, %r2171 }, { %r2148, %r2149 }, { %f2354, %f2355, %f2356, %f2357 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2358, %f2359, %f2360, %f2361 }, { %r2168, %r2169, %r2170, %r2171 }, { %r2154, %r2155 }, { %f2358, %f2359, %f2360, %f2361 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2362, %f2363, %f2364, %f2365 }, { %r2168, %r2169, %r2170, %r2171 }, { %r2160, %r2161 }, { %f2362, %f2363, %f2364, %f2365 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2366, %f2367, %f2368, %f2369 }, { %r2168, %r2169, %r2170, %r2171 }, { %r2166, %r2167 }, { %f2366, %f2367, %f2368, %f2369 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2370, %f2371, %f2372, %f2373 }, { %r2216, %r2217, %r2218, %r2219 }, { %r2124, %r2125 }, { %f2370, %f2371, %f2372, %f2373 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2374, %f2375, %f2376, %f2377 }, { %r2216, %r2217, %r2218, %r2219 }, { %r2130, %r2131 }, { %f2374, %f2375, %f2376, %f2377 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2378, %f2379, %f2380, %f2381 }, { %r2216, %r2217, %r2218, %r2219 }, { %r2136, %r2137 }, { %f2378, %f2379, %f2380, %f2381 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2382, %f2383, %f2384, %f2385 }, { %r2216, %r2217, %r2218, %r2219 }, { %r2142, %r2143 }, { %f2382, %f2383, %f2384, %f2385 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2386, %f2387, %f2388, %f2389 }, { %r2216, %r2217, %r2218, %r2219 }, { %r2148, %r2149 }, { %f2386, %f2387, %f2388, %f2389 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2390, %f2391, %f2392, %f2393 }, { %r2216, %r2217, %r2218, %r2219 }, { %r2154, %r2155 }, { %f2390, %f2391, %f2392, %f2393 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2394, %f2395, %f2396, %f2397 }, { %r2216, %r2217, %r2218, %r2219 }, { %r2160, %r2161 }, { %f2394, %f2395, %f2396, %f2397 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2398, %f2399, %f2400, %f2401 }, { %r2216, %r2217, %r2218, %r2219 }, { %r2166, %r2167 }, { %f2398, %f2399, %f2400, %f2401 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2402, %f2403, %f2404, %f2405 }, { %r2264, %r2265, %r2266, %r2267 }, { %r2124, %r2125 }, { %f2402, %f2403, %f2404, %f2405 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2406, %f2407, %f2408, %f2409 }, { %r2264, %r2265, %r2266, %r2267 }, { %r2130, %r2131 }, { %f2406, %f2407, %f2408, %f2409 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2410, %f2411, %f2412, %f2413 }, { %r2264, %r2265, %r2266, %r2267 }, { %r2136, %r2137 }, { %f2410, %f2411, %f2412, %f2413 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2414, %f2415, %f2416, %f2417 }, { %r2264, %r2265, %r2266, %r2267 }, { %r2142, %r2143 }, { %f2414, %f2415, %f2416, %f2417 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2418, %f2419, %f2420, %f2421 }, { %r2264, %r2265, %r2266, %r2267 }, { %r2148, %r2149 }, { %f2418, %f2419, %f2420, %f2421 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2422, %f2423, %f2424, %f2425 }, { %r2264, %r2265, %r2266, %r2267 }, { %r2154, %r2155 }, { %f2422, %f2423, %f2424, %f2425 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2426, %f2427, %f2428, %f2429 }, { %r2264, %r2265, %r2266, %r2267 }, { %r2160, %r2161 }, { %f2426, %f2427, %f2428, %f2429 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2430, %f2431, %f2432, %f2433 }, { %r2264, %r2265, %r2266, %r2267 }, { %r2166, %r2167 }, { %f2430, %f2431, %f2432, %f2433 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2306, %f2307, %f2308, %f2309 }, { %r2312, %r2313, %r2314, %r2315 }, { %r2316, %r2317 }, { %f2306, %f2307, %f2308, %f2309 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2310, %f2311, %f2312, %f2313 }, { %r2312, %r2313, %r2314, %r2315 }, { %r2322, %r2323 }, { %f2310, %f2311, %f2312, %f2313 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2314, %f2315, %f2316, %f2317 }, { %r2312, %r2313, %r2314, %r2315 }, { %r2328, %r2329 }, { %f2314, %f2315, %f2316, %f2317 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2318, %f2319, %f2320, %f2321 }, { %r2312, %r2313, %r2314, %r2315 }, { %r2334, %r2335 }, { %f2318, %f2319, %f2320, %f2321 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2322, %f2323, %f2324, %f2325 }, { %r2312, %r2313, %r2314, %r2315 }, { %r2340, %r2341 }, { %f2322, %f2323, %f2324, %f2325 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2326, %f2327, %f2328, %f2329 }, { %r2312, %r2313, %r2314, %r2315 }, { %r2346, %r2347 }, { %f2326, %f2327, %f2328, %f2329 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2330, %f2331, %f2332, %f2333 }, { %r2312, %r2313, %r2314, %r2315 }, { %r2352, %r2353 }, { %f2330, %f2331, %f2332, %f2333 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2334, %f2335, %f2336, %f2337 }, { %r2312, %r2313, %r2314, %r2315 }, { %r2358, %r2359 }, { %f2334, %f2335, %f2336, %f2337 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2338, %f2339, %f2340, %f2341 }, { %r2360, %r2361, %r2362, %r2363 }, { %r2316, %r2317 }, { %f2338, %f2339, %f2340, %f2341 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2342, %f2343, %f2344, %f2345 }, { %r2360, %r2361, %r2362, %r2363 }, { %r2322, %r2323 }, { %f2342, %f2343, %f2344, %f2345 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2346, %f2347, %f2348, %f2349 }, { %r2360, %r2361, %r2362, %r2363 }, { %r2328, %r2329 }, { %f2346, %f2347, %f2348, %f2349 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2350, %f2351, %f2352, %f2353 }, { %r2360, %r2361, %r2362, %r2363 }, { %r2334, %r2335 }, { %f2350, %f2351, %f2352, %f2353 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2354, %f2355, %f2356, %f2357 }, { %r2360, %r2361, %r2362, %r2363 }, { %r2340, %r2341 }, { %f2354, %f2355, %f2356, %f2357 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2358, %f2359, %f2360, %f2361 }, { %r2360, %r2361, %r2362, %r2363 }, { %r2346, %r2347 }, { %f2358, %f2359, %f2360, %f2361 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2362, %f2363, %f2364, %f2365 }, { %r2360, %r2361, %r2362, %r2363 }, { %r2352, %r2353 }, { %f2362, %f2363, %f2364, %f2365 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2366, %f2367, %f2368, %f2369 }, { %r2360, %r2361, %r2362, %r2363 }, { %r2358, %r2359 }, { %f2366, %f2367, %f2368, %f2369 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2370, %f2371, %f2372, %f2373 }, { %r2408, %r2409, %r2410, %r2411 }, { %r2316, %r2317 }, { %f2370, %f2371, %f2372, %f2373 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2374, %f2375, %f2376, %f2377 }, { %r2408, %r2409, %r2410, %r2411 }, { %r2322, %r2323 }, { %f2374, %f2375, %f2376, %f2377 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2378, %f2379, %f2380, %f2381 }, { %r2408, %r2409, %r2410, %r2411 }, { %r2328, %r2329 }, { %f2378, %f2379, %f2380, %f2381 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2382, %f2383, %f2384, %f2385 }, { %r2408, %r2409, %r2410, %r2411 }, { %r2334, %r2335 }, { %f2382, %f2383, %f2384, %f2385 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2386, %f2387, %f2388, %f2389 }, { %r2408, %r2409, %r2410, %r2411 }, { %r2340, %r2341 }, { %f2386, %f2387, %f2388, %f2389 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2390, %f2391, %f2392, %f2393 }, { %r2408, %r2409, %r2410, %r2411 }, { %r2346, %r2347 }, { %f2390, %f2391, %f2392, %f2393 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2394, %f2395, %f2396, %f2397 }, { %r2408, %r2409, %r2410, %r2411 }, { %r2352, %r2353 }, { %f2394, %f2395, %f2396, %f2397 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2398, %f2399, %f2400, %f2401 }, { %r2408, %r2409, %r2410, %r2411 }, { %r2358, %r2359 }, { %f2398, %f2399, %f2400, %f2401 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2402, %f2403, %f2404, %f2405 }, { %r2456, %r2457, %r2458, %r2459 }, { %r2316, %r2317 }, { %f2402, %f2403, %f2404, %f2405 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2406, %f2407, %f2408, %f2409 }, { %r2456, %r2457, %r2458, %r2459 }, { %r2322, %r2323 }, { %f2406, %f2407, %f2408, %f2409 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2410, %f2411, %f2412, %f2413 }, { %r2456, %r2457, %r2458, %r2459 }, { %r2328, %r2329 }, { %f2410, %f2411, %f2412, %f2413 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2414, %f2415, %f2416, %f2417 }, { %r2456, %r2457, %r2458, %r2459 }, { %r2334, %r2335 }, { %f2414, %f2415, %f2416, %f2417 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2418, %f2419, %f2420, %f2421 }, { %r2456, %r2457, %r2458, %r2459 }, { %r2340, %r2341 }, { %f2418, %f2419, %f2420, %f2421 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2422, %f2423, %f2424, %f2425 }, { %r2456, %r2457, %r2458, %r2459 }, { %r2346, %r2347 }, { %f2422, %f2423, %f2424, %f2425 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2426, %f2427, %f2428, %f2429 }, { %r2456, %r2457, %r2458, %r2459 }, { %r2352, %r2353 }, { %f2426, %f2427, %f2428, %f2429 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f2430, %f2431, %f2432, %f2433 }, { %r2456, %r2457, %r2458, %r2459 }, { %r2358, %r2359 }, { %f2430, %f2431, %f2432, %f2433 };
	// end inline asm
	.loc	1 335 18
	add.s64 	%rd149, %rd222, %rd51;
	add.s64 	%rd150, %rd222, %rd50;
	add.s64 	%rd151, %rd222, %rd49;
	add.s64 	%rd152, %rd222, %rd48;
	add.s64 	%rd153, %rd222, %rd47;
	add.s64 	%rd154, %rd222, %rd46;
	add.s64 	%rd155, %rd222, %rd45;
	add.s64 	%rd156, %rd222, %rd44;
	add.s64 	%rd157, %rd222, %rd43;
	add.s64 	%rd158, %rd222, %rd42;
	add.s64 	%rd159, %rd222, %rd41;
	add.s64 	%rd160, %rd222, %rd40;
	add.s64 	%rd161, %rd222, %rd39;
	add.s64 	%rd162, %rd222, %rd38;
	add.s64 	%rd163, %rd222, %rd37;
	.loc	1 336 18
	add.s64 	%rd164, %rd222, %rd36;
	add.s64 	%rd165, %rd223, %rd35;
	add.s64 	%rd166, %rd223, %rd34;
	add.s64 	%rd167, %rd223, %rd33;
	add.s64 	%rd168, %rd223, %rd32;
	add.s64 	%rd169, %rd223, %rd31;
	add.s64 	%rd170, %rd223, %rd30;
	add.s64 	%rd171, %rd223, %rd29;
	.loc	1 327 22
	add.s64 	%rd172, %rd223, %rd26;
	add.s32 	%r2607, %r2774, 1;
	setp.lt.s32 	%p95, %r2607, 2;
	selp.b32 	%r2774, %r2607, 0, %p95;
	.loc	1 330 51
	setp.lt.s32 	%p96, %r27, %r2738;
	.loc	1 330 20
	shl.b32 	%r2608, %r2774, 15;
	shl.b32 	%r2609, %r2774, 16;
	add.s32 	%r2611, %r594, %r2609;
	bar.sync 	0;
	add.s32 	%r2504, %r2611, %r593;
	add.s32 	%r2506, %r2611, %r596;
	add.s32 	%r2508, %r2611, %r598;
	add.s32 	%r2510, %r2611, %r600;
	add.s32 	%r2512, %r2611, %r602;
	add.s32 	%r2514, %r2611, %r604;
	add.s32 	%r2516, %r2611, %r606;
	add.s32 	%r2518, %r2611, %r608;
	add.s32 	%r2520, %r2611, %r610;
	add.s32 	%r2522, %r2611, %r612;
	add.s32 	%r2524, %r2611, %r614;
	add.s32 	%r2526, %r2611, %r616;
	add.s32 	%r2528, %r2611, %r618;
	add.s32 	%r2530, %r2611, %r620;
	add.s32 	%r2532, %r2611, %r622;
	add.s32 	%r2534, %r2611, %r624;
	selp.b32 	%r2628, 16, 0, %p96;
	selp.b32 	%r2507, %r2628, 0, %p94;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2504 + 0 ], [ %rd149 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2506 + 0 ], [ %rd150 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2508 + 0 ], [ %rd151 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2510 + 0 ], [ %rd152 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2512 + 0 ], [ %rd153 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2514 + 0 ], [ %rd154 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2516 + 0 ], [ %rd155 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2518 + 0 ], [ %rd156 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2520 + 0 ], [ %rd157 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2522 + 0 ], [ %rd158 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2524 + 0 ], [ %rd159 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2526 + 0 ], [ %rd160 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2528 + 0 ], [ %rd161 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2530 + 0 ], [ %rd162 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2532 + 0 ], [ %rd163 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2534 + 0 ], [ %rd164 + 0 ], 0x10, %r2507;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 51
	setp.lt.s32 	%p97, %r3, %r2738;
	setp.lt.s32 	%p98, %r4, %r2738;
	setp.lt.s32 	%p99, %r5, %r2738;
	setp.lt.s32 	%p100, %r6, %r2738;
	setp.lt.s32 	%p101, %r7, %r2738;
	setp.lt.s32 	%p102, %r8, %r2738;
	setp.lt.s32 	%p103, %r9, %r2738;
	setp.lt.s32 	%p104, %r10, %r2738;
	.loc	1 331 20
	add.s32 	%r2630, %r626, %r2608;
	add.s32 	%r2536, %r2630, %r593;
	add.s32 	%r2538, %r2630, %r596;
	add.s32 	%r2540, %r2630, %r598;
	add.s32 	%r2542, %r2630, %r600;
	add.s32 	%r2544, %r2630, %r602;
	add.s32 	%r2546, %r2630, %r604;
	add.s32 	%r2548, %r2630, %r606;
	add.s32 	%r2550, %r2630, %r608;
	selp.b32 	%r2631, 16, 0, %p97;
	selp.b32 	%r2537, %r2631, 0, %p94;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2536 + 0 ], [ %rd165 + 0 ], 0x10, %r2537;
	// end inline asm
	selp.b32 	%r2632, 16, 0, %p98;
	selp.b32 	%r2539, %r2632, 0, %p94;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2538 + 0 ], [ %rd166 + 0 ], 0x10, %r2539;
	// end inline asm
	selp.b32 	%r2633, 16, 0, %p99;
	selp.b32 	%r2541, %r2633, 0, %p94;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2540 + 0 ], [ %rd167 + 0 ], 0x10, %r2541;
	// end inline asm
	selp.b32 	%r2634, 16, 0, %p100;
	selp.b32 	%r2543, %r2634, 0, %p94;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2542 + 0 ], [ %rd168 + 0 ], 0x10, %r2543;
	// end inline asm
	selp.b32 	%r2635, 16, 0, %p101;
	selp.b32 	%r2545, %r2635, 0, %p94;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2544 + 0 ], [ %rd169 + 0 ], 0x10, %r2545;
	// end inline asm
	selp.b32 	%r2636, 16, 0, %p102;
	selp.b32 	%r2547, %r2636, 0, %p94;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2546 + 0 ], [ %rd170 + 0 ], 0x10, %r2547;
	// end inline asm
	selp.b32 	%r2637, 16, 0, %p103;
	selp.b32 	%r2549, %r2637, 0, %p94;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2548 + 0 ], [ %rd171 + 0 ], 0x10, %r2549;
	// end inline asm
	selp.b32 	%r2638, 16, 0, %p104;
	selp.b32 	%r2551, %r2638, 0, %p94;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r2550 + 0 ], [ %rd172 + 0 ], 0x10, %r2551;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 327 22
	add.s32 	%r2639, %r2773, 1;
	setp.lt.s32 	%p105, %r2639, 2;
	selp.b32 	%r2773, %r2639, 0, %p105;
	.loc	1 330 20
	shl.b32 	%r2640, %r2773, 15;
	shl.b32 	%r2641, %r2773, 16;
	add.s32 	%r2772, %r594, %r2641;
	// begin inline asm
	cp.async.wait_group 0x2;
	// end inline asm
	bar.sync 	0;
	.loc	1 331 20
	add.s32 	%r2771, %r626, %r2640;
	.loc	1 330 20
	add.s32 	%r2556, %r2772, %r644;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2767, %r2768, %r2769, %r2770 }, [ %r2556 + 0 ];
	// end inline asm
	add.s32 	%r2561, %r2556, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2763, %r2764, %r2765, %r2766 }, [ %r2561 + 0 ];
	// end inline asm
	add.s32 	%r2566, %r2556, 32768;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2759, %r2760, %r2761, %r2762 }, [ %r2566 + 0 ];
	// end inline asm
	add.s32 	%r2571, %r2556, 49152;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r2755, %r2756, %r2757, %r2758 }, [ %r2571 + 0 ];
	// end inline asm
	.loc	1 331 20
	add.s32 	%r2576, %r2771, %r650;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2751, %r2752, %r2753, %r2754 }, [ %r2576 + 0 ];
	// end inline asm
	add.s32 	%r2581, %r2771, %r654;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2747, %r2748, %r2749, %r2750 }, [ %r2581 + 0 ];
	// end inline asm
	add.s32 	%r2586, %r2771, %r658;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2743, %r2744, %r2745, %r2746 }, [ %r2586 + 0 ];
	// end inline asm
	add.s32 	%r2591, %r2771, %r662;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r2739, %r2740, %r2741, %r2742 }, [ %r2591 + 0 ];
	// end inline asm
	.loc	1 327 22
	add.s32 	%r2775, %r2775, 1;
	add.s64 	%rd223, %rd223, %rd28;
	add.s64 	%rd222, %rd222, 256;
	add.s32 	%r2738, %r2738, -128;
	setp.lt.s32 	%p106, %r2775, %r29;
	@%p106 bra 	$L__BB0_2;
	.loc	1 341 23
	cvt.rn.f16.f32 	%rs1, %f2433;
	cvt.rn.f16.f32 	%rs2, %f2432;
	mov.b32 	%r2839, {%rs2, %rs1};
	cvt.rn.f16.f32 	%rs3, %f2431;
	cvt.rn.f16.f32 	%rs4, %f2430;
	mov.b32 	%r2838, {%rs4, %rs3};
	cvt.rn.f16.f32 	%rs5, %f2429;
	cvt.rn.f16.f32 	%rs6, %f2428;
	mov.b32 	%r2837, {%rs6, %rs5};
	cvt.rn.f16.f32 	%rs7, %f2427;
	cvt.rn.f16.f32 	%rs8, %f2426;
	mov.b32 	%r2836, {%rs8, %rs7};
	cvt.rn.f16.f32 	%rs9, %f2425;
	cvt.rn.f16.f32 	%rs10, %f2424;
	mov.b32 	%r2835, {%rs10, %rs9};
	cvt.rn.f16.f32 	%rs11, %f2423;
	cvt.rn.f16.f32 	%rs12, %f2422;
	mov.b32 	%r2834, {%rs12, %rs11};
	cvt.rn.f16.f32 	%rs13, %f2421;
	cvt.rn.f16.f32 	%rs14, %f2420;
	mov.b32 	%r2833, {%rs14, %rs13};
	cvt.rn.f16.f32 	%rs15, %f2419;
	cvt.rn.f16.f32 	%rs16, %f2418;
	mov.b32 	%r2832, {%rs16, %rs15};
	cvt.rn.f16.f32 	%rs17, %f2417;
	cvt.rn.f16.f32 	%rs18, %f2416;
	mov.b32 	%r2831, {%rs18, %rs17};
	cvt.rn.f16.f32 	%rs19, %f2415;
	cvt.rn.f16.f32 	%rs20, %f2414;
	mov.b32 	%r2830, {%rs20, %rs19};
	cvt.rn.f16.f32 	%rs21, %f2413;
	cvt.rn.f16.f32 	%rs22, %f2412;
	mov.b32 	%r2829, {%rs22, %rs21};
	cvt.rn.f16.f32 	%rs23, %f2411;
	cvt.rn.f16.f32 	%rs24, %f2410;
	mov.b32 	%r2828, {%rs24, %rs23};
	cvt.rn.f16.f32 	%rs25, %f2409;
	cvt.rn.f16.f32 	%rs26, %f2408;
	mov.b32 	%r2827, {%rs26, %rs25};
	cvt.rn.f16.f32 	%rs27, %f2407;
	cvt.rn.f16.f32 	%rs28, %f2406;
	mov.b32 	%r2826, {%rs28, %rs27};
	cvt.rn.f16.f32 	%rs29, %f2405;
	cvt.rn.f16.f32 	%rs30, %f2404;
	mov.b32 	%r2825, {%rs30, %rs29};
	cvt.rn.f16.f32 	%rs31, %f2403;
	cvt.rn.f16.f32 	%rs32, %f2402;
	mov.b32 	%r2824, {%rs32, %rs31};
	cvt.rn.f16.f32 	%rs33, %f2401;
	cvt.rn.f16.f32 	%rs34, %f2400;
	mov.b32 	%r2823, {%rs34, %rs33};
	cvt.rn.f16.f32 	%rs35, %f2399;
	cvt.rn.f16.f32 	%rs36, %f2398;
	mov.b32 	%r2822, {%rs36, %rs35};
	cvt.rn.f16.f32 	%rs37, %f2397;
	cvt.rn.f16.f32 	%rs38, %f2396;
	mov.b32 	%r2821, {%rs38, %rs37};
	cvt.rn.f16.f32 	%rs39, %f2395;
	cvt.rn.f16.f32 	%rs40, %f2394;
	mov.b32 	%r2820, {%rs40, %rs39};
	cvt.rn.f16.f32 	%rs41, %f2393;
	cvt.rn.f16.f32 	%rs42, %f2392;
	mov.b32 	%r2819, {%rs42, %rs41};
	cvt.rn.f16.f32 	%rs43, %f2391;
	cvt.rn.f16.f32 	%rs44, %f2390;
	mov.b32 	%r2818, {%rs44, %rs43};
	cvt.rn.f16.f32 	%rs45, %f2389;
	cvt.rn.f16.f32 	%rs46, %f2388;
	mov.b32 	%r2817, {%rs46, %rs45};
	cvt.rn.f16.f32 	%rs47, %f2387;
	cvt.rn.f16.f32 	%rs48, %f2386;
	mov.b32 	%r2816, {%rs48, %rs47};
	cvt.rn.f16.f32 	%rs49, %f2385;
	cvt.rn.f16.f32 	%rs50, %f2384;
	mov.b32 	%r2815, {%rs50, %rs49};
	cvt.rn.f16.f32 	%rs51, %f2383;
	cvt.rn.f16.f32 	%rs52, %f2382;
	mov.b32 	%r2814, {%rs52, %rs51};
	cvt.rn.f16.f32 	%rs53, %f2381;
	cvt.rn.f16.f32 	%rs54, %f2380;
	mov.b32 	%r2813, {%rs54, %rs53};
	cvt.rn.f16.f32 	%rs55, %f2379;
	cvt.rn.f16.f32 	%rs56, %f2378;
	mov.b32 	%r2812, {%rs56, %rs55};
	cvt.rn.f16.f32 	%rs57, %f2377;
	cvt.rn.f16.f32 	%rs58, %f2376;
	mov.b32 	%r2811, {%rs58, %rs57};
	cvt.rn.f16.f32 	%rs59, %f2375;
	cvt.rn.f16.f32 	%rs60, %f2374;
	mov.b32 	%r2810, {%rs60, %rs59};
	cvt.rn.f16.f32 	%rs61, %f2373;
	cvt.rn.f16.f32 	%rs62, %f2372;
	mov.b32 	%r2809, {%rs62, %rs61};
	cvt.rn.f16.f32 	%rs63, %f2371;
	cvt.rn.f16.f32 	%rs64, %f2370;
	mov.b32 	%r2808, {%rs64, %rs63};
	cvt.rn.f16.f32 	%rs65, %f2369;
	cvt.rn.f16.f32 	%rs66, %f2368;
	mov.b32 	%r2807, {%rs66, %rs65};
	cvt.rn.f16.f32 	%rs67, %f2367;
	cvt.rn.f16.f32 	%rs68, %f2366;
	mov.b32 	%r2806, {%rs68, %rs67};
	cvt.rn.f16.f32 	%rs69, %f2365;
	cvt.rn.f16.f32 	%rs70, %f2364;
	mov.b32 	%r2805, {%rs70, %rs69};
	cvt.rn.f16.f32 	%rs71, %f2363;
	cvt.rn.f16.f32 	%rs72, %f2362;
	mov.b32 	%r2804, {%rs72, %rs71};
	cvt.rn.f16.f32 	%rs73, %f2361;
	cvt.rn.f16.f32 	%rs74, %f2360;
	mov.b32 	%r2803, {%rs74, %rs73};
	cvt.rn.f16.f32 	%rs75, %f2359;
	cvt.rn.f16.f32 	%rs76, %f2358;
	mov.b32 	%r2802, {%rs76, %rs75};
	cvt.rn.f16.f32 	%rs77, %f2357;
	cvt.rn.f16.f32 	%rs78, %f2356;
	mov.b32 	%r2801, {%rs78, %rs77};
	cvt.rn.f16.f32 	%rs79, %f2355;
	cvt.rn.f16.f32 	%rs80, %f2354;
	mov.b32 	%r2800, {%rs80, %rs79};
	cvt.rn.f16.f32 	%rs81, %f2353;
	cvt.rn.f16.f32 	%rs82, %f2352;
	mov.b32 	%r2799, {%rs82, %rs81};
	cvt.rn.f16.f32 	%rs83, %f2351;
	cvt.rn.f16.f32 	%rs84, %f2350;
	mov.b32 	%r2798, {%rs84, %rs83};
	cvt.rn.f16.f32 	%rs85, %f2349;
	cvt.rn.f16.f32 	%rs86, %f2348;
	mov.b32 	%r2797, {%rs86, %rs85};
	cvt.rn.f16.f32 	%rs87, %f2347;
	cvt.rn.f16.f32 	%rs88, %f2346;
	mov.b32 	%r2796, {%rs88, %rs87};
	cvt.rn.f16.f32 	%rs89, %f2345;
	cvt.rn.f16.f32 	%rs90, %f2344;
	mov.b32 	%r2795, {%rs90, %rs89};
	cvt.rn.f16.f32 	%rs91, %f2343;
	cvt.rn.f16.f32 	%rs92, %f2342;
	mov.b32 	%r2794, {%rs92, %rs91};
	cvt.rn.f16.f32 	%rs93, %f2341;
	cvt.rn.f16.f32 	%rs94, %f2340;
	mov.b32 	%r2793, {%rs94, %rs93};
	cvt.rn.f16.f32 	%rs95, %f2339;
	cvt.rn.f16.f32 	%rs96, %f2338;
	mov.b32 	%r2792, {%rs96, %rs95};
	cvt.rn.f16.f32 	%rs97, %f2337;
	cvt.rn.f16.f32 	%rs98, %f2336;
	mov.b32 	%r2791, {%rs98, %rs97};
	cvt.rn.f16.f32 	%rs99, %f2335;
	cvt.rn.f16.f32 	%rs100, %f2334;
	mov.b32 	%r2790, {%rs100, %rs99};
	cvt.rn.f16.f32 	%rs101, %f2333;
	cvt.rn.f16.f32 	%rs102, %f2332;
	mov.b32 	%r2789, {%rs102, %rs101};
	cvt.rn.f16.f32 	%rs103, %f2331;
	cvt.rn.f16.f32 	%rs104, %f2330;
	mov.b32 	%r2788, {%rs104, %rs103};
	cvt.rn.f16.f32 	%rs105, %f2329;
	cvt.rn.f16.f32 	%rs106, %f2328;
	mov.b32 	%r2787, {%rs106, %rs105};
	cvt.rn.f16.f32 	%rs107, %f2327;
	cvt.rn.f16.f32 	%rs108, %f2326;
	mov.b32 	%r2786, {%rs108, %rs107};
	cvt.rn.f16.f32 	%rs109, %f2325;
	cvt.rn.f16.f32 	%rs110, %f2324;
	mov.b32 	%r2785, {%rs110, %rs109};
	cvt.rn.f16.f32 	%rs111, %f2323;
	cvt.rn.f16.f32 	%rs112, %f2322;
	mov.b32 	%r2784, {%rs112, %rs111};
	cvt.rn.f16.f32 	%rs113, %f2321;
	cvt.rn.f16.f32 	%rs114, %f2320;
	mov.b32 	%r2783, {%rs114, %rs113};
	cvt.rn.f16.f32 	%rs115, %f2319;
	cvt.rn.f16.f32 	%rs116, %f2318;
	mov.b32 	%r2782, {%rs116, %rs115};
	cvt.rn.f16.f32 	%rs117, %f2317;
	cvt.rn.f16.f32 	%rs118, %f2316;
	mov.b32 	%r2781, {%rs118, %rs117};
	cvt.rn.f16.f32 	%rs119, %f2315;
	cvt.rn.f16.f32 	%rs120, %f2314;
	mov.b32 	%r2780, {%rs120, %rs119};
	cvt.rn.f16.f32 	%rs121, %f2313;
	cvt.rn.f16.f32 	%rs122, %f2312;
	mov.b32 	%r2779, {%rs122, %rs121};
	cvt.rn.f16.f32 	%rs123, %f2311;
	cvt.rn.f16.f32 	%rs124, %f2310;
	mov.b32 	%r2778, {%rs124, %rs123};
	cvt.rn.f16.f32 	%rs125, %f2309;
	cvt.rn.f16.f32 	%rs126, %f2308;
	mov.b32 	%r2777, {%rs126, %rs125};
	cvt.rn.f16.f32 	%rs127, %f2307;
	cvt.rn.f16.f32 	%rs128, %f2306;
	mov.b32 	%r2776, {%rs128, %rs127};
$L__BB0_4:
	.loc	1 327 22
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 347 33
	mul.lo.s32 	%r2707, %r11, %r304;
	mul.lo.s32 	%r2708, %r12, %r304;
	mul.lo.s32 	%r2709, %r13, %r304;
	mul.lo.s32 	%r2710, %r14, %r304;
	mul.lo.s32 	%r2711, %r15, %r304;
	mul.lo.s32 	%r2712, %r16, %r304;
	mul.lo.s32 	%r2713, %r17, %r304;
	mul.lo.s32 	%r2714, %r18, %r304;
	mul.lo.s32 	%r2715, %r19, %r304;
	mul.lo.s32 	%r2716, %r20, %r304;
	mul.lo.s32 	%r2717, %r21, %r304;
	mul.lo.s32 	%r2718, %r22, %r304;
	mul.lo.s32 	%r2719, %r23, %r304;
	mul.lo.s32 	%r2720, %r24, %r304;
	mul.lo.s32 	%r2721, %r25, %r304;
	mul.lo.s32 	%r2722, %r26, %r304;
	.loc	1 347 21
	mul.wide.s32 	%rd189, %r2707, 2;
	add.s64 	%rd190, %rd58, %rd189;
	mul.wide.s32 	%rd191, %r2708, 2;
	add.s64 	%rd192, %rd58, %rd191;
	mul.wide.s32 	%rd193, %r2709, 2;
	add.s64 	%rd194, %rd58, %rd193;
	mul.wide.s32 	%rd195, %r2710, 2;
	add.s64 	%rd196, %rd58, %rd195;
	mul.wide.s32 	%rd197, %r2711, 2;
	add.s64 	%rd198, %rd58, %rd197;
	mul.wide.s32 	%rd199, %r2712, 2;
	add.s64 	%rd200, %rd58, %rd199;
	mul.wide.s32 	%rd201, %r2713, 2;
	add.s64 	%rd202, %rd58, %rd201;
	mul.wide.s32 	%rd203, %r2714, 2;
	add.s64 	%rd204, %rd58, %rd203;
	mul.wide.s32 	%rd205, %r2715, 2;
	add.s64 	%rd206, %rd58, %rd205;
	mul.wide.s32 	%rd207, %r2716, 2;
	add.s64 	%rd208, %rd58, %rd207;
	mul.wide.s32 	%rd209, %r2717, 2;
	add.s64 	%rd210, %rd58, %rd209;
	mul.wide.s32 	%rd211, %r2718, 2;
	add.s64 	%rd212, %rd58, %rd211;
	mul.wide.s32 	%rd213, %r2719, 2;
	add.s64 	%rd214, %rd58, %rd213;
	mul.wide.s32 	%rd215, %r2720, 2;
	add.s64 	%rd216, %rd58, %rd215;
	mul.wide.s32 	%rd217, %r2721, 2;
	add.s64 	%rd218, %rd58, %rd217;
	mul.wide.s32 	%rd219, %r2722, 2;
	add.s64 	%rd220, %rd58, %rd219;
	.loc	1 347 52
	mul.wide.s32 	%rd221, %r28, 2;
	add.s64 	%rd173, %rd190, %rd221;
	add.s64 	%rd174, %rd192, %rd221;
	add.s64 	%rd175, %rd194, %rd221;
	add.s64 	%rd176, %rd196, %rd221;
	add.s64 	%rd177, %rd198, %rd221;
	add.s64 	%rd178, %rd200, %rd221;
	add.s64 	%rd179, %rd202, %rd221;
	add.s64 	%rd180, %rd204, %rd221;
	add.s64 	%rd181, %rd206, %rd221;
	add.s64 	%rd182, %rd208, %rd221;
	add.s64 	%rd183, %rd210, %rd221;
	add.s64 	%rd184, %rd212, %rd221;
	add.s64 	%rd185, %rd214, %rd221;
	add.s64 	%rd186, %rd216, %rd221;
	add.s64 	%rd187, %rd218, %rd221;
	add.s64 	%rd188, %rd220, %rd221;
	.loc	1 348 33
	setp.lt.s32 	%p123, %r11, %r301;
	setp.lt.s32 	%p124, %r12, %r301;
	setp.lt.s32 	%p125, %r13, %r301;
	setp.lt.s32 	%p126, %r14, %r301;
	setp.lt.s32 	%p127, %r15, %r301;
	setp.lt.s32 	%p128, %r16, %r301;
	setp.lt.s32 	%p129, %r17, %r301;
	setp.lt.s32 	%p130, %r18, %r301;
	setp.lt.s32 	%p131, %r19, %r301;
	setp.lt.s32 	%p132, %r20, %r301;
	setp.lt.s32 	%p133, %r21, %r301;
	setp.lt.s32 	%p134, %r22, %r301;
	setp.lt.s32 	%p135, %r23, %r301;
	setp.lt.s32 	%p136, %r24, %r301;
	setp.lt.s32 	%p137, %r25, %r301;
	setp.lt.s32 	%p138, %r26, %r301;
	.loc	1 348 58
	setp.lt.s32 	%p139, %r28, %r302;
	.loc	1 348 39
	and.pred  	%p107, %p123, %p139;
	and.pred  	%p108, %p124, %p139;
	and.pred  	%p109, %p125, %p139;
	and.pred  	%p110, %p126, %p139;
	and.pred  	%p111, %p127, %p139;
	and.pred  	%p112, %p128, %p139;
	and.pred  	%p113, %p129, %p139;
	and.pred  	%p114, %p130, %p139;
	and.pred  	%p115, %p131, %p139;
	and.pred  	%p116, %p132, %p139;
	and.pred  	%p117, %p133, %p139;
	and.pred  	%p118, %p134, %p139;
	and.pred  	%p119, %p135, %p139;
	and.pred  	%p120, %p136, %p139;
	and.pred  	%p121, %p137, %p139;
	and.pred  	%p122, %p138, %p139;
	.loc	1 349 21
	shl.b32 	%r2723, %r1, 1;
	and.b32  	%r2724, %r2723, 6;
	and.b32  	%r2725, %r48, 55;
	shl.b32 	%r2726, %r67, 3;
	or.b32  	%r2727, %r2726, %r2724;
	mad.lo.s32 	%r2728, %r2725, 136, %r2727;
	shl.b32 	%r2729, %r2728, 1;
	add.s32 	%r2731, %r594, %r2729;
	st.shared.b32 	[%r2731], %r2776;
	st.shared.b32 	[%r2731+2176], %r2777;
	st.shared.b32 	[%r2731+32], %r2778;
	st.shared.b32 	[%r2731+2208], %r2779;
	st.shared.b32 	[%r2731+64], %r2780;
	st.shared.b32 	[%r2731+2240], %r2781;
	st.shared.b32 	[%r2731+96], %r2782;
	st.shared.b32 	[%r2731+2272], %r2783;
	st.shared.b32 	[%r2731+128], %r2784;
	st.shared.b32 	[%r2731+2304], %r2785;
	st.shared.b32 	[%r2731+160], %r2786;
	st.shared.b32 	[%r2731+2336], %r2787;
	st.shared.b32 	[%r2731+192], %r2788;
	st.shared.b32 	[%r2731+2368], %r2789;
	st.shared.b32 	[%r2731+224], %r2790;
	st.shared.b32 	[%r2731+2400], %r2791;
	bar.sync 	0;
	shl.b32 	%r2732, %r2, 1;
	and.b32  	%r2733, %r2732, 14;
	or.b32  	%r2734, %r2733, %r47;
	mad.lo.s32 	%r2735, %r2734, 136, %r27;
	shl.b32 	%r2736, %r2735, 1;
	add.s32 	%r2737, %r594, %r2736;
	ld.shared.v4.u32 	{%r2643, %r2644, %r2645, %r2646}, [%r2737];
	ld.shared.v4.u32 	{%r2647, %r2648, %r2649, %r2650}, [%r2737+4352];
	ld.shared.v4.u32 	{%r2651, %r2652, %r2653, %r2654}, [%r2737+8704];
	ld.shared.v4.u32 	{%r2655, %r2656, %r2657, %r2658}, [%r2737+13056];
	bar.sync 	0;
	st.shared.b32 	[%r2731], %r2792;
	st.shared.b32 	[%r2731+2176], %r2793;
	st.shared.b32 	[%r2731+32], %r2794;
	st.shared.b32 	[%r2731+2208], %r2795;
	st.shared.b32 	[%r2731+64], %r2796;
	st.shared.b32 	[%r2731+2240], %r2797;
	st.shared.b32 	[%r2731+96], %r2798;
	st.shared.b32 	[%r2731+2272], %r2799;
	st.shared.b32 	[%r2731+128], %r2800;
	st.shared.b32 	[%r2731+2304], %r2801;
	st.shared.b32 	[%r2731+160], %r2802;
	st.shared.b32 	[%r2731+2336], %r2803;
	st.shared.b32 	[%r2731+192], %r2804;
	st.shared.b32 	[%r2731+2368], %r2805;
	st.shared.b32 	[%r2731+224], %r2806;
	st.shared.b32 	[%r2731+2400], %r2807;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r2659, %r2660, %r2661, %r2662}, [%r2737];
	ld.shared.v4.u32 	{%r2663, %r2664, %r2665, %r2666}, [%r2737+4352];
	ld.shared.v4.u32 	{%r2667, %r2668, %r2669, %r2670}, [%r2737+8704];
	ld.shared.v4.u32 	{%r2671, %r2672, %r2673, %r2674}, [%r2737+13056];
	bar.sync 	0;
	st.shared.b32 	[%r2731], %r2808;
	st.shared.b32 	[%r2731+2176], %r2809;
	st.shared.b32 	[%r2731+32], %r2810;
	st.shared.b32 	[%r2731+2208], %r2811;
	st.shared.b32 	[%r2731+64], %r2812;
	st.shared.b32 	[%r2731+2240], %r2813;
	st.shared.b32 	[%r2731+96], %r2814;
	st.shared.b32 	[%r2731+2272], %r2815;
	st.shared.b32 	[%r2731+128], %r2816;
	st.shared.b32 	[%r2731+2304], %r2817;
	st.shared.b32 	[%r2731+160], %r2818;
	st.shared.b32 	[%r2731+2336], %r2819;
	st.shared.b32 	[%r2731+192], %r2820;
	st.shared.b32 	[%r2731+2368], %r2821;
	st.shared.b32 	[%r2731+224], %r2822;
	st.shared.b32 	[%r2731+2400], %r2823;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r2675, %r2676, %r2677, %r2678}, [%r2737];
	ld.shared.v4.u32 	{%r2679, %r2680, %r2681, %r2682}, [%r2737+4352];
	ld.shared.v4.u32 	{%r2683, %r2684, %r2685, %r2686}, [%r2737+8704];
	ld.shared.v4.u32 	{%r2687, %r2688, %r2689, %r2690}, [%r2737+13056];
	bar.sync 	0;
	st.shared.b32 	[%r2731], %r2824;
	st.shared.b32 	[%r2731+2176], %r2825;
	st.shared.b32 	[%r2731+32], %r2826;
	st.shared.b32 	[%r2731+2208], %r2827;
	st.shared.b32 	[%r2731+64], %r2828;
	st.shared.b32 	[%r2731+2240], %r2829;
	st.shared.b32 	[%r2731+96], %r2830;
	st.shared.b32 	[%r2731+2272], %r2831;
	st.shared.b32 	[%r2731+128], %r2832;
	st.shared.b32 	[%r2731+2304], %r2833;
	st.shared.b32 	[%r2731+160], %r2834;
	st.shared.b32 	[%r2731+2336], %r2835;
	st.shared.b32 	[%r2731+192], %r2836;
	st.shared.b32 	[%r2731+2368], %r2837;
	st.shared.b32 	[%r2731+224], %r2838;
	st.shared.b32 	[%r2731+2400], %r2839;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r2691, %r2692, %r2693, %r2694}, [%r2737];
	ld.shared.v4.u32 	{%r2695, %r2696, %r2697, %r2698}, [%r2737+4352];
	ld.shared.v4.u32 	{%r2699, %r2700, %r2701, %r2702}, [%r2737+8704];
	ld.shared.v4.u32 	{%r2703, %r2704, %r2705, %r2706}, [%r2737+13056];
	// begin inline asm
	@%p107 st.global.v4.b32 [ %rd173 + 0 ], { %r2643, %r2644, %r2645, %r2646 };
	// end inline asm
	// begin inline asm
	@%p108 st.global.v4.b32 [ %rd174 + 0 ], { %r2647, %r2648, %r2649, %r2650 };
	// end inline asm
	// begin inline asm
	@%p109 st.global.v4.b32 [ %rd175 + 0 ], { %r2651, %r2652, %r2653, %r2654 };
	// end inline asm
	// begin inline asm
	@%p110 st.global.v4.b32 [ %rd176 + 0 ], { %r2655, %r2656, %r2657, %r2658 };
	// end inline asm
	// begin inline asm
	@%p111 st.global.v4.b32 [ %rd177 + 0 ], { %r2659, %r2660, %r2661, %r2662 };
	// end inline asm
	// begin inline asm
	@%p112 st.global.v4.b32 [ %rd178 + 0 ], { %r2663, %r2664, %r2665, %r2666 };
	// end inline asm
	// begin inline asm
	@%p113 st.global.v4.b32 [ %rd179 + 0 ], { %r2667, %r2668, %r2669, %r2670 };
	// end inline asm
	// begin inline asm
	@%p114 st.global.v4.b32 [ %rd180 + 0 ], { %r2671, %r2672, %r2673, %r2674 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd181 + 0 ], { %r2675, %r2676, %r2677, %r2678 };
	// end inline asm
	// begin inline asm
	@%p116 st.global.v4.b32 [ %rd182 + 0 ], { %r2679, %r2680, %r2681, %r2682 };
	// end inline asm
	// begin inline asm
	@%p117 st.global.v4.b32 [ %rd183 + 0 ], { %r2683, %r2684, %r2685, %r2686 };
	// end inline asm
	// begin inline asm
	@%p118 st.global.v4.b32 [ %rd184 + 0 ], { %r2687, %r2688, %r2689, %r2690 };
	// end inline asm
	// begin inline asm
	@%p119 st.global.v4.b32 [ %rd185 + 0 ], { %r2691, %r2692, %r2693, %r2694 };
	// end inline asm
	// begin inline asm
	@%p120 st.global.v4.b32 [ %rd186 + 0 ], { %r2695, %r2696, %r2697, %r2698 };
	// end inline asm
	// begin inline asm
	@%p121 st.global.v4.b32 [ %rd187 + 0 ], { %r2699, %r2700, %r2701, %r2702 };
	// end inline asm
	// begin inline asm
	@%p122 st.global.v4.b32 [ %rd188 + 0 ], { %r2703, %r2704, %r2705, %r2706 };
	// end inline asm
	.loc	1 349 4
	ret;
$L__tmp6:
$L__func_end0:

}
	.file	1 "/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/02.matrix-multiplication/matrix_multiplication.py"
	.file	2 "/data_ssd1/zjy_home/frameworks/cuda/triton/python/triton/language/standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 5
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 265
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 109
.b8 97
.b8 116
.b8 114
.b8 105
.b8 120
.b8 95
.b8 109
.b8 117
.b8 108
.b8 116
.b8 105
.b8 112
.b8 108
.b8 105
.b8 99
.b8 97
.b8 116
.b8 105
.b8 111
.b8 110
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 100
.b8 97
.b8 116
.b8 97
.b8 95
.b8 115
.b8 115
.b8 100
.b8 49
.b8 47
.b8 122
.b8 106
.b8 121
.b8 95
.b8 104
.b8 111
.b8 109
.b8 101
.b8 47
.b8 109
.b8 121
.b8 95
.b8 99
.b8 111
.b8 100
.b8 101
.b8 47
.b8 77
.b8 76
.b8 67
.b8 45
.b8 76
.b8 101
.b8 97
.b8 114
.b8 110
.b8 105
.b8 110
.b8 103
.b8 47
.b8 84
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 47
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 45
.b8 101
.b8 120
.b8 97
.b8 109
.b8 112
.b8 108
.b8 101
.b8 115
.b8 47
.b8 48
.b8 50
.b8 46
.b8 109
.b8 97
.b8 116
.b8 114
.b8 105
.b8 120
.b8 45
.b8 109
.b8 117
.b8 108
.b8 116
.b8 105
.b8 112
.b8 108
.b8 105
.b8 99
.b8 97
.b8 116
.b8 105
.b8 111
.b8 110
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 109
.b8 97
.b8 116
.b8 109
.b8 117
.b8 108
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 155
.b8 4
.b32 155
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 37
.b8 1
.b8 27
.b8 4
.b32 155
.b64 $L__tmp2
.b64 $L__tmp3
.b8 1
.b8 38
.b8 1
.b8 27
.b8 4
.b32 155
.b64 $L__tmp4
.b64 $L__tmp5
.b8 1
.b8 71
.b8 1
.b8 33
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}

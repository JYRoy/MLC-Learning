//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_80
.address_size 64

	// .globl	matmul_kernel
.extern .shared .align 16 .b8 global_smem[];

.visible .entry matmul_kernel(
	.param .u64 matmul_kernel_param_0,
	.param .u64 matmul_kernel_param_1,
	.param .u64 matmul_kernel_param_2,
	.param .u32 matmul_kernel_param_3,
	.param .u32 matmul_kernel_param_4,
	.param .u32 matmul_kernel_param_5,
	.param .u32 matmul_kernel_param_6,
	.param .u32 matmul_kernel_param_7,
	.param .u32 matmul_kernel_param_8
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<94>;
	.reg .b16 	%rs<129>;
	.reg .b32 	%r<1290>;
	.reg .f32 	%f<898>;
	.reg .b64 	%rd<133>;
	.loc	1 260 0
$L__func_begin0:
	.loc	1 260 0

	ld.param.u32 	%r276, [matmul_kernel_param_8];
	ld.param.u32 	%r275, [matmul_kernel_param_7];
	ld.param.u32 	%r274, [matmul_kernel_param_5];
	ld.param.u32 	%r273, [matmul_kernel_param_4];
	ld.param.u32 	%r272, [matmul_kernel_param_3];
	ld.param.u64 	%rd28, [matmul_kernel_param_2];
	ld.param.u64 	%rd27, [matmul_kernel_param_1];
	ld.param.u64 	%rd26, [matmul_kernel_param_0];
$L__tmp0:
	.loc	1 292 24
	// begin inline asm
	mov.u32 %r277, %ctaid.x;
	// end inline asm
$L__tmp1:
	.loc	2 44 22
	add.s32 	%r430, %r272, 127;
	.loc	2 44 28
	shr.s32 	%r431, %r430, 31;
	shr.u32 	%r432, %r431, 25;
	add.s32 	%r433, %r430, %r432;
	shr.s32 	%r434, %r433, 7;
$L__tmp2:
	.loc	2 44 22
	add.s32 	%r435, %r273, 127;
	.loc	2 44 28
	shr.s32 	%r436, %r435, 31;
	shr.u32 	%r437, %r436, 25;
	add.s32 	%r438, %r435, %r437;
	shr.s32 	%r439, %r438, 7;
$L__tmp3:
	.loc	1 295 38
	shl.b32 	%r441, %r439, 3;
	ld.param.u32 	%r442, [matmul_kernel_param_6];
	.loc	1 296 22
	div.s32 	%r443, %r277, %r441;
	.loc	1 297 29
	shl.b32 	%r444, %r443, 3;
	.loc	1 299 20
	sub.s32 	%r445, %r434, %r444;
	.loc	1 299 33
	min.s32 	%r447, %r445, 8;
	mul.lo.s32 	%r448, %r443, %r441;
	sub.s32 	%r449, %r277, %r448;
	.loc	1 304 40
	div.s32 	%r450, %r449, %r447;
	mul.lo.s32 	%r451, %r450, %r447;
	sub.s32 	%r452, %r449, %r451;
	.loc	1 302 8
	add.s32 	%r453, %r452, %r444;
	.loc	1 313 23
	shl.b32 	%r1, %r453, 7;
	.loc	1 313 51
	mov.u32 	%r2, %tid.x;
	shr.u32 	%r3, %r2, 5;
	shr.u32 	%r4, %r2, 2;
	bfe.u32 	%r454, %r2, 2, 4;
	and.b32  	%r455, %r2, 64;
	shr.u32 	%r456, %r455, 2;
	or.b32  	%r457, %r454, %r456;
	or.b32  	%r458, %r457, 32;
	or.b32  	%r459, %r457, 64;
	or.b32  	%r460, %r457, 96;
	bfe.u32 	%r5, %r2, 4, 1;
	bfe.u32 	%r6, %r2, 4, 2;
	shr.u32 	%r7, %r455, 4;
	or.b32  	%r8, %r6, %r7;
	or.b32  	%r9, %r8, 8;
	or.b32  	%r10, %r8, 16;
	or.b32  	%r11, %r8, 24;
	shl.b32 	%r461, %r2, 3;
	and.b32  	%r12, %r461, 24;
	and.b32  	%r13, %r461, 120;
	.loc	1 313 38
	or.b32  	%r462, %r1, %r457;
	or.b32  	%r463, %r1, %r458;
	or.b32  	%r464, %r1, %r459;
	or.b32  	%r465, %r1, %r460;
	.loc	1 313 68
	rem.s32 	%r466, %r462, %r272;
	rem.s32 	%r467, %r463, %r272;
	rem.s32 	%r468, %r464, %r272;
	rem.s32 	%r469, %r465, %r272;
	.loc	1 314 23
	shl.b32 	%r470, %r450, 7;
	.loc	1 314 38
	or.b32  	%r14, %r470, %r13;
	.loc	1 314 68
	rem.s32 	%r15, %r14, %r273;
	.loc	1 316 53
	mad.lo.s32 	%r471, %r466, %r442, %r12;
	mad.lo.s32 	%r472, %r467, %r442, %r12;
	mad.lo.s32 	%r473, %r468, %r442, %r12;
	mad.lo.s32 	%r474, %r469, %r442, %r12;
	.loc	1 316 22
	mul.wide.s32 	%rd53, %r471, 2;
	add.s64 	%rd29, %rd26, %rd53;
	mul.wide.s32 	%rd54, %r472, 2;
	add.s64 	%rd30, %rd26, %rd54;
	mul.wide.s32 	%rd55, %r473, 2;
	add.s64 	%rd31, %rd26, %rd55;
	mul.wide.s32 	%rd56, %r474, 2;
	add.s64 	%rd32, %rd26, %rd56;
	.loc	1 318 40
	shl.b32 	%r475, %r275, 3;
	.loc	1 318 52
	mad.lo.s32 	%r476, %r8, %r275, %r15;
	add.s32 	%r477, %r476, %r475;
	add.s32 	%r478, %r477, %r475;
	add.s32 	%r479, %r478, %r475;
	.loc	1 318 22
	mul.wide.s32 	%rd57, %r476, 2;
	add.s64 	%rd33, %rd27, %rd57;
	mul.wide.s32 	%rd58, %r477, 2;
	add.s64 	%rd34, %rd27, %rd58;
	mul.wide.s32 	%rd59, %r478, 2;
	add.s64 	%rd35, %rd27, %rd59;
	mul.wide.s32 	%rd60, %r479, 2;
	add.s64 	%rd36, %rd27, %rd60;
$L__tmp4:
	.loc	2 44 22
	add.s32 	%r480, %r274, 31;
$L__tmp5:
	.loc	1 336 33
	shl.b32 	%r484, %r275, 5;
	.loc	1 327 22
	setp.lt.s32 	%p25, %r480, 32;
	setp.gt.s32 	%p26, %r480, 31;
	.loc	1 330 51
	setp.lt.s32 	%p27, %r12, %r274;
	.loc	1 330 20
	shl.b32 	%r485, %r457, 5;
	xor.b32  	%r486, %r461, %r2;
	and.b32  	%r487, %r486, 24;
	or.b32  	%r17, %r485, %r487;
	shl.b32 	%r488, %r17, 1;
	mov.u32 	%r489, global_smem;
	add.s32 	%r278, %r489, %r488;
	shl.b32 	%r490, %r458, 5;
	or.b32  	%r18, %r490, %r487;
	shl.b32 	%r491, %r18, 1;
	add.s32 	%r280, %r489, %r491;
	shl.b32 	%r492, %r459, 5;
	or.b32  	%r19, %r492, %r487;
	shl.b32 	%r493, %r19, 1;
	add.s32 	%r282, %r489, %r493;
	shl.b32 	%r494, %r460, 5;
	or.b32  	%r20, %r494, %r487;
	shl.b32 	%r495, %r20, 1;
	add.s32 	%r284, %r489, %r495;
	selp.b32 	%r496, 16, 0, %p26;
	selp.b32 	%r281, %r496, 0, %p27;
	mov.pred 	%p1, -1;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r278 + 0 ], [ %rd29 + 0 ], 0x10, %r281;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r280 + 0 ], [ %rd30 + 0 ], 0x10, %r281;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r282 + 0 ], [ %rd31 + 0 ], 0x10, %r281;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r284 + 0 ], [ %rd32 + 0 ], 0x10, %r281;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 51
	setp.lt.s32 	%p28, %r8, %r274;
	setp.lt.s32 	%p29, %r9, %r274;
	setp.lt.s32 	%p30, %r10, %r274;
	setp.lt.s32 	%p31, %r11, %r274;
	.loc	1 331 20
	shl.b32 	%r497, %r8, 7;
	shl.b32 	%r498, %r8, 3;
	xor.b32  	%r499, %r498, %r13;
	or.b32  	%r21, %r499, %r497;
	shl.b32 	%r500, %r21, 1;
	add.s32 	%r501, %r489, 24576;
	add.s32 	%r286, %r501, %r500;
	shl.b32 	%r502, %r9, 7;
	or.b32  	%r22, %r502, %r499;
	shl.b32 	%r503, %r22, 1;
	add.s32 	%r288, %r501, %r503;
	shl.b32 	%r504, %r10, 7;
	or.b32  	%r23, %r504, %r499;
	shl.b32 	%r505, %r23, 1;
	add.s32 	%r290, %r501, %r505;
	shl.b32 	%r506, %r11, 7;
	or.b32  	%r24, %r506, %r499;
	shl.b32 	%r507, %r24, 1;
	add.s32 	%r292, %r501, %r507;
	selp.b32 	%r287, %r496, 0, %p28;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r286 + 0 ], [ %rd33 + 0 ], 0x10, %r287;
	// end inline asm
	selp.b32 	%r289, %r496, 0, %p29;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r288 + 0 ], [ %rd34 + 0 ], 0x10, %r289;
	// end inline asm
	selp.b32 	%r291, %r496, 0, %p30;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r290 + 0 ], [ %rd35 + 0 ], 0x10, %r291;
	// end inline asm
	selp.b32 	%r293, %r496, 0, %p31;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r292 + 0 ], [ %rd36 + 0 ], 0x10, %r293;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 327 22
	setp.gt.s32 	%p32, %r480, 63;
	.loc	1 335 18
	add.s64 	%rd37, %rd29, 64;
	add.s64 	%rd38, %rd30, 64;
	add.s64 	%rd39, %rd31, 64;
	add.s64 	%rd40, %rd32, 64;
	.loc	1 336 18
	mul.wide.s32 	%rd61, %r484, 2;
	add.s64 	%rd41, %rd33, %rd61;
	add.s64 	%rd42, %rd34, %rd61;
	add.s64 	%rd43, %rd35, %rd61;
	add.s64 	%rd44, %rd36, %rd61;
	.loc	1 330 55
	add.s32 	%r508, %r274, -32;
	.loc	1 330 51
	setp.lt.s32 	%p33, %r12, %r508;
	.loc	1 330 20
	bar.sync 	0;
	add.s32 	%r509, %r489, 8192;
	add.s32 	%r294, %r509, %r488;
	add.s32 	%r296, %r509, %r491;
	add.s32 	%r298, %r509, %r493;
	add.s32 	%r300, %r509, %r495;
	selp.b32 	%r510, 16, 0, %p33;
	selp.b32 	%r297, %r510, 0, %p32;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r294 + 0 ], [ %rd37 + 0 ], 0x10, %r297;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r296 + 0 ], [ %rd38 + 0 ], 0x10, %r297;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r298 + 0 ], [ %rd39 + 0 ], 0x10, %r297;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r300 + 0 ], [ %rd40 + 0 ], 0x10, %r297;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 51
	setp.lt.s32 	%p34, %r8, %r508;
	setp.lt.s32 	%p35, %r9, %r508;
	setp.lt.s32 	%p36, %r10, %r508;
	setp.lt.s32 	%p37, %r11, %r508;
	.loc	1 331 20
	add.s32 	%r511, %r489, 32768;
	add.s32 	%r302, %r511, %r500;
	add.s32 	%r304, %r511, %r503;
	add.s32 	%r306, %r511, %r505;
	add.s32 	%r308, %r511, %r507;
	selp.b32 	%r512, 16, 0, %p34;
	selp.b32 	%r303, %r512, 0, %p32;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r302 + 0 ], [ %rd41 + 0 ], 0x10, %r303;
	// end inline asm
	selp.b32 	%r513, 16, 0, %p35;
	selp.b32 	%r305, %r513, 0, %p32;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r304 + 0 ], [ %rd42 + 0 ], 0x10, %r305;
	// end inline asm
	selp.b32 	%r514, 16, 0, %p36;
	selp.b32 	%r307, %r514, 0, %p32;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r306 + 0 ], [ %rd43 + 0 ], 0x10, %r307;
	// end inline asm
	selp.b32 	%r515, 16, 0, %p37;
	selp.b32 	%r309, %r515, 0, %p32;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r308 + 0 ], [ %rd44 + 0 ], 0x10, %r309;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 327 22
	setp.gt.s32 	%p38, %r480, 95;
	.loc	1 335 18
	add.s64 	%rd45, %rd29, 128;
	add.s64 	%rd46, %rd30, 128;
	add.s64 	%rd47, %rd31, 128;
	add.s64 	%rd48, %rd32, 128;
	.loc	1 336 18
	add.s64 	%rd49, %rd41, %rd61;
	add.s64 	%rd50, %rd42, %rd61;
	add.s64 	%rd51, %rd43, %rd61;
	add.s64 	%rd52, %rd44, %rd61;
	.loc	1 330 55
	add.s32 	%r516, %r274, -64;
	.loc	1 330 51
	setp.lt.s32 	%p39, %r12, %r516;
	.loc	1 330 20
	bar.sync 	0;
	add.s32 	%r517, %r489, 16384;
	add.s32 	%r310, %r517, %r488;
	add.s32 	%r312, %r517, %r491;
	add.s32 	%r314, %r517, %r493;
	add.s32 	%r316, %r517, %r495;
	selp.b32 	%r518, 16, 0, %p39;
	selp.b32 	%r313, %r518, 0, %p38;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r310 + 0 ], [ %rd45 + 0 ], 0x10, %r313;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r312 + 0 ], [ %rd46 + 0 ], 0x10, %r313;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r314 + 0 ], [ %rd47 + 0 ], 0x10, %r313;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r316 + 0 ], [ %rd48 + 0 ], 0x10, %r313;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 51
	setp.lt.s32 	%p40, %r8, %r516;
	setp.lt.s32 	%p41, %r9, %r516;
	setp.lt.s32 	%p42, %r10, %r516;
	setp.lt.s32 	%p43, %r11, %r516;
	.loc	1 331 20
	add.s32 	%r519, %r489, 40960;
	add.s32 	%r318, %r519, %r500;
	add.s32 	%r320, %r519, %r503;
	add.s32 	%r322, %r519, %r505;
	add.s32 	%r324, %r519, %r507;
	selp.b32 	%r520, 16, 0, %p40;
	selp.b32 	%r319, %r520, 0, %p38;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r318 + 0 ], [ %rd49 + 0 ], 0x10, %r319;
	// end inline asm
	selp.b32 	%r521, 16, 0, %p41;
	selp.b32 	%r321, %r521, 0, %p38;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r320 + 0 ], [ %rd50 + 0 ], 0x10, %r321;
	// end inline asm
	selp.b32 	%r522, 16, 0, %p42;
	selp.b32 	%r323, %r522, 0, %p38;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r322 + 0 ], [ %rd51 + 0 ], 0x10, %r323;
	// end inline asm
	selp.b32 	%r523, 16, 0, %p43;
	selp.b32 	%r325, %r523, 0, %p38;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r324 + 0 ], [ %rd52 + 0 ], 0x10, %r325;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 330 20
	// begin inline asm
	cp.async.wait_group 0x4;
	// end inline asm
	bar.sync 	0;
	and.b32  	%r524, %r2, 7;
	bfe.u32 	%r25, %r2, 1, 2;
	and.b32  	%r525, %r4, 16;
	and.b32  	%r526, %r2, 15;
	or.b32  	%r527, %r526, %r525;
	xor.b32  	%r528, %r5, %r25;
	shl.b32 	%r26, %r527, 5;
	shl.b32 	%r529, %r528, 3;
	or.b32  	%r27, %r26, %r529;
	shl.b32 	%r530, %r27, 1;
	add.s32 	%r330, %r489, %r530;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1217, %r1218, %r1219, %r1220 }, [ %r330 + 0 ];
	// end inline asm
	add.s32 	%r335, %r330, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1213, %r1214, %r1215, %r1216 }, [ %r335 + 0 ];
	// end inline asm
	add.s32 	%r340, %r330, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1209, %r1210, %r1211, %r1212 }, [ %r340 + 0 ];
	// end inline asm
	add.s32 	%r345, %r330, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1205, %r1206, %r1207, %r1208 }, [ %r345 + 0 ];
	// end inline asm
	.loc	1 331 20
	bfe.u32 	%r44, %r2, 5, 1;
	shl.b32 	%r531, %r5, 1;
	or.b32  	%r532, %r531, %r44;
	xor.b32  	%r533, %r532, %r524;
	shl.b32 	%r534, %r526, 7;
	shl.b32 	%r535, %r533, 3;
	or.b32  	%r45, %r535, %r534;
	shl.b32 	%r536, %r45, 1;
	add.s32 	%r350, %r501, %r536;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1201, %r1202, %r1203, %r1204 }, [ %r350 + 0 ];
	// end inline asm
	or.b32  	%r537, %r532, 4;
	xor.b32  	%r538, %r537, %r524;
	shl.b32 	%r539, %r538, 3;
	or.b32  	%r50, %r539, %r534;
	shl.b32 	%r540, %r50, 1;
	add.s32 	%r355, %r501, %r540;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1197, %r1198, %r1199, %r1200 }, [ %r355 + 0 ];
	// end inline asm
	or.b32  	%r541, %r532, 8;
	xor.b32  	%r542, %r541, %r524;
	shl.b32 	%r543, %r542, 3;
	or.b32  	%r55, %r543, %r534;
	shl.b32 	%r544, %r55, 1;
	add.s32 	%r360, %r501, %r544;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1193, %r1194, %r1195, %r1196 }, [ %r360 + 0 ];
	// end inline asm
	or.b32  	%r545, %r532, 12;
	xor.b32  	%r546, %r545, %r524;
	shl.b32 	%r547, %r546, 3;
	or.b32  	%r60, %r547, %r534;
	shl.b32 	%r548, %r60, 1;
	add.s32 	%r365, %r501, %r548;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1189, %r1190, %r1191, %r1192 }, [ %r365 + 0 ];
	// end inline asm
	mov.b32 	%r1226, 0;
	mov.u32 	%r1227, %r1226;
	mov.u32 	%r1228, %r1226;
	mov.u32 	%r1229, %r1226;
	mov.u32 	%r1230, %r1226;
	mov.u32 	%r1231, %r1226;
	mov.u32 	%r1232, %r1226;
	mov.u32 	%r1233, %r1226;
	mov.u32 	%r1234, %r1226;
	mov.u32 	%r1235, %r1226;
	mov.u32 	%r1236, %r1226;
	mov.u32 	%r1237, %r1226;
	mov.u32 	%r1238, %r1226;
	mov.u32 	%r1239, %r1226;
	mov.u32 	%r1240, %r1226;
	mov.u32 	%r1241, %r1226;
	mov.u32 	%r1242, %r1226;
	mov.u32 	%r1243, %r1226;
	mov.u32 	%r1244, %r1226;
	mov.u32 	%r1245, %r1226;
	mov.u32 	%r1246, %r1226;
	mov.u32 	%r1247, %r1226;
	mov.u32 	%r1248, %r1226;
	mov.u32 	%r1249, %r1226;
	mov.u32 	%r1250, %r1226;
	mov.u32 	%r1251, %r1226;
	mov.u32 	%r1252, %r1226;
	mov.u32 	%r1253, %r1226;
	mov.u32 	%r1254, %r1226;
	mov.u32 	%r1255, %r1226;
	mov.u32 	%r1256, %r1226;
	mov.u32 	%r1257, %r1226;
	mov.u32 	%r1258, %r1226;
	mov.u32 	%r1259, %r1226;
	mov.u32 	%r1260, %r1226;
	mov.u32 	%r1261, %r1226;
	mov.u32 	%r1262, %r1226;
	mov.u32 	%r1263, %r1226;
	mov.u32 	%r1264, %r1226;
	mov.u32 	%r1265, %r1226;
	mov.u32 	%r1266, %r1226;
	mov.u32 	%r1267, %r1226;
	mov.u32 	%r1268, %r1226;
	mov.u32 	%r1269, %r1226;
	mov.u32 	%r1270, %r1226;
	mov.u32 	%r1271, %r1226;
	mov.u32 	%r1272, %r1226;
	mov.u32 	%r1273, %r1226;
	mov.u32 	%r1274, %r1226;
	mov.u32 	%r1275, %r1226;
	mov.u32 	%r1276, %r1226;
	mov.u32 	%r1277, %r1226;
	mov.u32 	%r1278, %r1226;
	mov.u32 	%r1279, %r1226;
	mov.u32 	%r1280, %r1226;
	mov.u32 	%r1281, %r1226;
	mov.u32 	%r1282, %r1226;
	mov.u32 	%r1283, %r1226;
	mov.u32 	%r1284, %r1226;
	mov.u32 	%r1285, %r1226;
	mov.u32 	%r1286, %r1226;
	mov.u32 	%r1287, %r1226;
	mov.u32 	%r1288, %r1226;
	mov.u32 	%r1289, %r1226;
	.loc	1 327 22
	@%p25 bra 	$L__BB0_4;
	.loc	1 0 22
	cvt.s64.s32 	%rd1, %r471;
	cvt.s64.s32 	%rd2, %r472;
	cvt.s64.s32 	%rd3, %r473;
	cvt.s64.s32 	%rd4, %r474;
	shr.s32 	%r481, %r480, 31;
	shr.u32 	%r482, %r481, 27;
	add.s32 	%r483, %r480, %r482;
	shr.s32 	%r16, %r483, 5;
	cvt.s64.s32 	%rd5, %r484;
	add.s32 	%r65, %r16, -3;
	or.b32  	%r553, %r5, 2;
	xor.b32  	%r554, %r553, %r25;
	shl.b32 	%r555, %r554, 3;
	add.s32 	%r1188, %r274, -96;
	or.b32  	%r67, %r26, %r555;
	.loc	1 327 22
	add.s32 	%r556, %r7, %r6;
	add.s32 	%r557, %r556, 24;
	mad.lo.s32 	%r558, %r275, %r557, %r15;
	mul.wide.s32 	%rd6, %r558, 2;
	mul.lo.s64 	%rd62, %rd5, 6;
	add.s64 	%rd132, %rd27, %rd62;
	shl.b64 	%rd8, %rd5, 1;
	or.b32  	%r559, %r556, 16;
	mad.lo.s32 	%r560, %r275, %r559, %r15;
	mul.wide.s32 	%rd9, %r560, 2;
	add.s32 	%r561, %r556, 8;
	mad.lo.s32 	%r562, %r275, %r561, %r15;
	mul.wide.s32 	%rd10, %r562, 2;
	mad.lo.s32 	%r563, %r275, %r556, %r15;
	mul.wide.s32 	%rd11, %r563, 2;
	shl.b64 	%rd63, %rd4, 1;
	add.s64 	%rd64, %rd63, %rd26;
	add.s64 	%rd131, %rd64, 192;
	shl.b64 	%rd65, %rd3, 1;
	add.s64 	%rd66, %rd65, %rd26;
	add.s64 	%rd130, %rd66, 192;
	shl.b64 	%rd67, %rd2, 1;
	add.s64 	%rd68, %rd67, %rd26;
	add.s64 	%rd129, %rd68, 192;
	shl.b64 	%rd69, %rd1, 1;
	add.s64 	%rd70, %rd69, %rd26;
	add.s64 	%rd128, %rd70, 192;
	mov.f32 	%f770, 0f00000000;
	mov.b32 	%r1224, 2;
	mov.b32 	%r1223, 0;
	shl.b32 	%r1044, %r67, 1;
	mov.u32 	%r1221, %r501;
	mov.u32 	%r1222, %r489;
	mov.f32 	%f771, %f770;
	mov.f32 	%f772, %f770;
	mov.f32 	%f773, %f770;
	mov.f32 	%f774, %f770;
	mov.f32 	%f775, %f770;
	mov.f32 	%f776, %f770;
	mov.f32 	%f777, %f770;
	mov.f32 	%f778, %f770;
	mov.f32 	%f779, %f770;
	mov.f32 	%f780, %f770;
	mov.f32 	%f781, %f770;
	mov.f32 	%f782, %f770;
	mov.f32 	%f783, %f770;
	mov.f32 	%f784, %f770;
	mov.f32 	%f785, %f770;
	mov.f32 	%f786, %f770;
	mov.f32 	%f787, %f770;
	mov.f32 	%f788, %f770;
	mov.f32 	%f789, %f770;
	mov.f32 	%f790, %f770;
	mov.f32 	%f791, %f770;
	mov.f32 	%f792, %f770;
	mov.f32 	%f793, %f770;
	mov.f32 	%f794, %f770;
	mov.f32 	%f795, %f770;
	mov.f32 	%f796, %f770;
	mov.f32 	%f797, %f770;
	mov.f32 	%f798, %f770;
	mov.f32 	%f799, %f770;
	mov.f32 	%f800, %f770;
	mov.f32 	%f801, %f770;
	mov.f32 	%f802, %f770;
	mov.f32 	%f803, %f770;
	mov.f32 	%f804, %f770;
	mov.f32 	%f805, %f770;
	mov.f32 	%f806, %f770;
	mov.f32 	%f807, %f770;
	mov.f32 	%f808, %f770;
	mov.f32 	%f809, %f770;
	mov.f32 	%f810, %f770;
	mov.f32 	%f811, %f770;
	mov.f32 	%f812, %f770;
	mov.f32 	%f813, %f770;
	mov.f32 	%f814, %f770;
	mov.f32 	%f815, %f770;
	mov.f32 	%f816, %f770;
	mov.f32 	%f817, %f770;
	mov.f32 	%f818, %f770;
	mov.f32 	%f819, %f770;
	mov.f32 	%f820, %f770;
	mov.f32 	%f821, %f770;
	mov.f32 	%f822, %f770;
	mov.f32 	%f823, %f770;
	mov.f32 	%f824, %f770;
	mov.f32 	%f825, %f770;
	mov.f32 	%f826, %f770;
	mov.f32 	%f827, %f770;
	mov.f32 	%f828, %f770;
	mov.f32 	%f829, %f770;
	mov.f32 	%f830, %f770;
	mov.f32 	%f831, %f770;
	mov.f32 	%f832, %f770;
	mov.f32 	%f833, %f770;
	mov.f32 	%f834, %f770;
	mov.f32 	%f835, %f770;
	mov.f32 	%f836, %f770;
	mov.f32 	%f837, %f770;
	mov.f32 	%f838, %f770;
	mov.f32 	%f839, %f770;
	mov.f32 	%f840, %f770;
	mov.f32 	%f841, %f770;
	mov.f32 	%f842, %f770;
	mov.f32 	%f843, %f770;
	mov.f32 	%f844, %f770;
	mov.f32 	%f845, %f770;
	mov.f32 	%f846, %f770;
	mov.f32 	%f847, %f770;
	mov.f32 	%f848, %f770;
	mov.f32 	%f849, %f770;
	mov.f32 	%f850, %f770;
	mov.f32 	%f851, %f770;
	mov.f32 	%f852, %f770;
	mov.f32 	%f853, %f770;
	mov.f32 	%f854, %f770;
	mov.f32 	%f855, %f770;
	mov.f32 	%f856, %f770;
	mov.f32 	%f857, %f770;
	mov.f32 	%f858, %f770;
	mov.f32 	%f859, %f770;
	mov.f32 	%f860, %f770;
	mov.f32 	%f861, %f770;
	mov.f32 	%f862, %f770;
	mov.f32 	%f863, %f770;
	mov.f32 	%f864, %f770;
	mov.f32 	%f865, %f770;
	mov.f32 	%f866, %f770;
	mov.f32 	%f867, %f770;
	mov.f32 	%f868, %f770;
	mov.f32 	%f869, %f770;
	mov.f32 	%f870, %f770;
	mov.f32 	%f871, %f770;
	mov.f32 	%f872, %f770;
	mov.f32 	%f873, %f770;
	mov.f32 	%f874, %f770;
	mov.f32 	%f875, %f770;
	mov.f32 	%f876, %f770;
	mov.f32 	%f877, %f770;
	mov.f32 	%f878, %f770;
	mov.f32 	%f879, %f770;
	mov.f32 	%f880, %f770;
	mov.f32 	%f881, %f770;
	mov.f32 	%f882, %f770;
	mov.f32 	%f883, %f770;
	mov.f32 	%f884, %f770;
	mov.f32 	%f885, %f770;
	mov.f32 	%f886, %f770;
	mov.f32 	%f887, %f770;
	mov.f32 	%f888, %f770;
	mov.f32 	%f889, %f770;
	mov.f32 	%f890, %f770;
	mov.f32 	%f891, %f770;
	mov.f32 	%f892, %f770;
	mov.f32 	%f893, %f770;
	mov.f32 	%f894, %f770;
	mov.f32 	%f895, %f770;
	mov.f32 	%f896, %f770;
	mov.f32 	%f897, %f770;
	mov.u32 	%r1225, %r1223;
$L__BB0_2:
	setp.lt.s32 	%p52, %r1225, %r65;
	.loc	1 330 20
	add.s32 	%r568, %r1222, %r1044;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r796, %r797, %r798, %r799 }, [ %r568 + 0 ];
	// end inline asm
	add.s32 	%r573, %r568, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r844, %r845, %r846, %r847 }, [ %r573 + 0 ];
	// end inline asm
	add.s32 	%r578, %r568, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r892, %r893, %r894, %r895 }, [ %r578 + 0 ];
	// end inline asm
	add.s32 	%r583, %r568, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r940, %r941, %r942, %r943 }, [ %r583 + 0 ];
	// end inline asm
	.loc	1 331 20
	add.s32 	%r1046, %r1221, %r536;
	add.s32 	%r588, %r1046, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r800, %r801, %r806, %r807 }, [ %r588 + 0 ];
	// end inline asm
	add.s32 	%r1048, %r1221, %r540;
	add.s32 	%r593, %r1048, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r812, %r813, %r818, %r819 }, [ %r593 + 0 ];
	// end inline asm
	add.s32 	%r1050, %r1221, %r544;
	add.s32 	%r598, %r1050, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r824, %r825, %r830, %r831 }, [ %r598 + 0 ];
	// end inline asm
	add.s32 	%r1052, %r1221, %r548;
	add.s32 	%r603, %r1052, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r836, %r837, %r842, %r843 }, [ %r603 + 0 ];
	// end inline asm
	.loc	1 333 35
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f770, %f771, %f772, %f773 }, { %r1217, %r1218, %r1219, %r1220 }, { %r1201, %r1202 }, { %f770, %f771, %f772, %f773 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f774, %f775, %f776, %f777 }, { %r1217, %r1218, %r1219, %r1220 }, { %r1203, %r1204 }, { %f774, %f775, %f776, %f777 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f778, %f779, %f780, %f781 }, { %r1217, %r1218, %r1219, %r1220 }, { %r1197, %r1198 }, { %f778, %f779, %f780, %f781 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f782, %f783, %f784, %f785 }, { %r1217, %r1218, %r1219, %r1220 }, { %r1199, %r1200 }, { %f782, %f783, %f784, %f785 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f786, %f787, %f788, %f789 }, { %r1217, %r1218, %r1219, %r1220 }, { %r1193, %r1194 }, { %f786, %f787, %f788, %f789 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f790, %f791, %f792, %f793 }, { %r1217, %r1218, %r1219, %r1220 }, { %r1195, %r1196 }, { %f790, %f791, %f792, %f793 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f794, %f795, %f796, %f797 }, { %r1217, %r1218, %r1219, %r1220 }, { %r1189, %r1190 }, { %f794, %f795, %f796, %f797 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f798, %f799, %f800, %f801 }, { %r1217, %r1218, %r1219, %r1220 }, { %r1191, %r1192 }, { %f798, %f799, %f800, %f801 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f802, %f803, %f804, %f805 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1201, %r1202 }, { %f802, %f803, %f804, %f805 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f806, %f807, %f808, %f809 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1203, %r1204 }, { %f806, %f807, %f808, %f809 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f810, %f811, %f812, %f813 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1197, %r1198 }, { %f810, %f811, %f812, %f813 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f814, %f815, %f816, %f817 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1199, %r1200 }, { %f814, %f815, %f816, %f817 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f818, %f819, %f820, %f821 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1193, %r1194 }, { %f818, %f819, %f820, %f821 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f822, %f823, %f824, %f825 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1195, %r1196 }, { %f822, %f823, %f824, %f825 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f826, %f827, %f828, %f829 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1189, %r1190 }, { %f826, %f827, %f828, %f829 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f830, %f831, %f832, %f833 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1191, %r1192 }, { %f830, %f831, %f832, %f833 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f834, %f835, %f836, %f837 }, { %r1209, %r1210, %r1211, %r1212 }, { %r1201, %r1202 }, { %f834, %f835, %f836, %f837 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f838, %f839, %f840, %f841 }, { %r1209, %r1210, %r1211, %r1212 }, { %r1203, %r1204 }, { %f838, %f839, %f840, %f841 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f842, %f843, %f844, %f845 }, { %r1209, %r1210, %r1211, %r1212 }, { %r1197, %r1198 }, { %f842, %f843, %f844, %f845 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f846, %f847, %f848, %f849 }, { %r1209, %r1210, %r1211, %r1212 }, { %r1199, %r1200 }, { %f846, %f847, %f848, %f849 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f850, %f851, %f852, %f853 }, { %r1209, %r1210, %r1211, %r1212 }, { %r1193, %r1194 }, { %f850, %f851, %f852, %f853 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f854, %f855, %f856, %f857 }, { %r1209, %r1210, %r1211, %r1212 }, { %r1195, %r1196 }, { %f854, %f855, %f856, %f857 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f858, %f859, %f860, %f861 }, { %r1209, %r1210, %r1211, %r1212 }, { %r1189, %r1190 }, { %f858, %f859, %f860, %f861 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f862, %f863, %f864, %f865 }, { %r1209, %r1210, %r1211, %r1212 }, { %r1191, %r1192 }, { %f862, %f863, %f864, %f865 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f866, %f867, %f868, %f869 }, { %r1205, %r1206, %r1207, %r1208 }, { %r1201, %r1202 }, { %f866, %f867, %f868, %f869 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f870, %f871, %f872, %f873 }, { %r1205, %r1206, %r1207, %r1208 }, { %r1203, %r1204 }, { %f870, %f871, %f872, %f873 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f874, %f875, %f876, %f877 }, { %r1205, %r1206, %r1207, %r1208 }, { %r1197, %r1198 }, { %f874, %f875, %f876, %f877 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f878, %f879, %f880, %f881 }, { %r1205, %r1206, %r1207, %r1208 }, { %r1199, %r1200 }, { %f878, %f879, %f880, %f881 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f882, %f883, %f884, %f885 }, { %r1205, %r1206, %r1207, %r1208 }, { %r1193, %r1194 }, { %f882, %f883, %f884, %f885 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f886, %f887, %f888, %f889 }, { %r1205, %r1206, %r1207, %r1208 }, { %r1195, %r1196 }, { %f886, %f887, %f888, %f889 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f890, %f891, %f892, %f893 }, { %r1205, %r1206, %r1207, %r1208 }, { %r1189, %r1190 }, { %f890, %f891, %f892, %f893 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f894, %f895, %f896, %f897 }, { %r1205, %r1206, %r1207, %r1208 }, { %r1191, %r1192 }, { %f894, %f895, %f896, %f897 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f770, %f771, %f772, %f773 }, { %r796, %r797, %r798, %r799 }, { %r800, %r801 }, { %f770, %f771, %f772, %f773 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f774, %f775, %f776, %f777 }, { %r796, %r797, %r798, %r799 }, { %r806, %r807 }, { %f774, %f775, %f776, %f777 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f778, %f779, %f780, %f781 }, { %r796, %r797, %r798, %r799 }, { %r812, %r813 }, { %f778, %f779, %f780, %f781 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f782, %f783, %f784, %f785 }, { %r796, %r797, %r798, %r799 }, { %r818, %r819 }, { %f782, %f783, %f784, %f785 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f786, %f787, %f788, %f789 }, { %r796, %r797, %r798, %r799 }, { %r824, %r825 }, { %f786, %f787, %f788, %f789 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f790, %f791, %f792, %f793 }, { %r796, %r797, %r798, %r799 }, { %r830, %r831 }, { %f790, %f791, %f792, %f793 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f794, %f795, %f796, %f797 }, { %r796, %r797, %r798, %r799 }, { %r836, %r837 }, { %f794, %f795, %f796, %f797 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f798, %f799, %f800, %f801 }, { %r796, %r797, %r798, %r799 }, { %r842, %r843 }, { %f798, %f799, %f800, %f801 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f802, %f803, %f804, %f805 }, { %r844, %r845, %r846, %r847 }, { %r800, %r801 }, { %f802, %f803, %f804, %f805 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f806, %f807, %f808, %f809 }, { %r844, %r845, %r846, %r847 }, { %r806, %r807 }, { %f806, %f807, %f808, %f809 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f810, %f811, %f812, %f813 }, { %r844, %r845, %r846, %r847 }, { %r812, %r813 }, { %f810, %f811, %f812, %f813 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f814, %f815, %f816, %f817 }, { %r844, %r845, %r846, %r847 }, { %r818, %r819 }, { %f814, %f815, %f816, %f817 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f818, %f819, %f820, %f821 }, { %r844, %r845, %r846, %r847 }, { %r824, %r825 }, { %f818, %f819, %f820, %f821 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f822, %f823, %f824, %f825 }, { %r844, %r845, %r846, %r847 }, { %r830, %r831 }, { %f822, %f823, %f824, %f825 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f826, %f827, %f828, %f829 }, { %r844, %r845, %r846, %r847 }, { %r836, %r837 }, { %f826, %f827, %f828, %f829 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f830, %f831, %f832, %f833 }, { %r844, %r845, %r846, %r847 }, { %r842, %r843 }, { %f830, %f831, %f832, %f833 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f834, %f835, %f836, %f837 }, { %r892, %r893, %r894, %r895 }, { %r800, %r801 }, { %f834, %f835, %f836, %f837 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f838, %f839, %f840, %f841 }, { %r892, %r893, %r894, %r895 }, { %r806, %r807 }, { %f838, %f839, %f840, %f841 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f842, %f843, %f844, %f845 }, { %r892, %r893, %r894, %r895 }, { %r812, %r813 }, { %f842, %f843, %f844, %f845 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f846, %f847, %f848, %f849 }, { %r892, %r893, %r894, %r895 }, { %r818, %r819 }, { %f846, %f847, %f848, %f849 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f850, %f851, %f852, %f853 }, { %r892, %r893, %r894, %r895 }, { %r824, %r825 }, { %f850, %f851, %f852, %f853 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f854, %f855, %f856, %f857 }, { %r892, %r893, %r894, %r895 }, { %r830, %r831 }, { %f854, %f855, %f856, %f857 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f858, %f859, %f860, %f861 }, { %r892, %r893, %r894, %r895 }, { %r836, %r837 }, { %f858, %f859, %f860, %f861 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f862, %f863, %f864, %f865 }, { %r892, %r893, %r894, %r895 }, { %r842, %r843 }, { %f862, %f863, %f864, %f865 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f866, %f867, %f868, %f869 }, { %r940, %r941, %r942, %r943 }, { %r800, %r801 }, { %f866, %f867, %f868, %f869 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f870, %f871, %f872, %f873 }, { %r940, %r941, %r942, %r943 }, { %r806, %r807 }, { %f870, %f871, %f872, %f873 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f874, %f875, %f876, %f877 }, { %r940, %r941, %r942, %r943 }, { %r812, %r813 }, { %f874, %f875, %f876, %f877 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f878, %f879, %f880, %f881 }, { %r940, %r941, %r942, %r943 }, { %r818, %r819 }, { %f878, %f879, %f880, %f881 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f882, %f883, %f884, %f885 }, { %r940, %r941, %r942, %r943 }, { %r824, %r825 }, { %f882, %f883, %f884, %f885 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f886, %f887, %f888, %f889 }, { %r940, %r941, %r942, %r943 }, { %r830, %r831 }, { %f886, %f887, %f888, %f889 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f890, %f891, %f892, %f893 }, { %r940, %r941, %r942, %r943 }, { %r836, %r837 }, { %f890, %f891, %f892, %f893 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f894, %f895, %f896, %f897 }, { %r940, %r941, %r942, %r943 }, { %r842, %r843 }, { %f894, %f895, %f896, %f897 };
	// end inline asm
	.loc	1 336 18
	add.s64 	%rd75, %rd132, %rd11;
	add.s64 	%rd76, %rd132, %rd10;
	add.s64 	%rd77, %rd132, %rd9;
	.loc	1 327 22
	add.s64 	%rd78, %rd132, %rd6;
	add.s32 	%r1053, %r1224, 1;
	setp.lt.s32 	%p53, %r1053, 3;
	selp.b32 	%r1224, %r1053, 0, %p53;
	.loc	1 330 51
	setp.lt.s32 	%p54, %r12, %r1188;
	.loc	1 330 20
	shl.b32 	%r1054, %r1224, 13;
	add.s32 	%r1056, %r489, %r1054;
	bar.sync 	0;
	add.s32 	%r988, %r1056, %r488;
	add.s32 	%r990, %r1056, %r491;
	add.s32 	%r992, %r1056, %r493;
	add.s32 	%r994, %r1056, %r495;
	selp.b32 	%r1061, 16, 0, %p54;
	selp.b32 	%r991, %r1061, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r988 + 0 ], [ %rd128 + 0 ], 0x10, %r991;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r990 + 0 ], [ %rd129 + 0 ], 0x10, %r991;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r992 + 0 ], [ %rd130 + 0 ], 0x10, %r991;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r994 + 0 ], [ %rd131 + 0 ], 0x10, %r991;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 51
	setp.lt.s32 	%p55, %r8, %r1188;
	setp.lt.s32 	%p56, %r9, %r1188;
	setp.lt.s32 	%p57, %r10, %r1188;
	setp.lt.s32 	%p58, %r11, %r1188;
	.loc	1 331 20
	add.s32 	%r1063, %r501, %r1054;
	add.s32 	%r996, %r1063, %r500;
	add.s32 	%r998, %r1063, %r503;
	add.s32 	%r1000, %r1063, %r505;
	add.s32 	%r1002, %r1063, %r507;
	selp.b32 	%r1068, 16, 0, %p55;
	selp.b32 	%r997, %r1068, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r996 + 0 ], [ %rd75 + 0 ], 0x10, %r997;
	// end inline asm
	selp.b32 	%r1069, 16, 0, %p56;
	selp.b32 	%r999, %r1069, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r998 + 0 ], [ %rd76 + 0 ], 0x10, %r999;
	// end inline asm
	selp.b32 	%r1070, 16, 0, %p57;
	selp.b32 	%r1001, %r1070, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1000 + 0 ], [ %rd77 + 0 ], 0x10, %r1001;
	// end inline asm
	selp.b32 	%r1071, 16, 0, %p58;
	selp.b32 	%r1003, %r1071, 0, %p52;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1002 + 0 ], [ %rd78 + 0 ], 0x10, %r1003;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 327 22
	add.s32 	%r1072, %r1223, 1;
	setp.lt.s32 	%p59, %r1072, 3;
	selp.b32 	%r1223, %r1072, 0, %p59;
	.loc	1 330 20
	shl.b32 	%r1073, %r1223, 13;
	add.s32 	%r1222, %r489, %r1073;
	// begin inline asm
	cp.async.wait_group 0x4;
	// end inline asm
	bar.sync 	0;
	.loc	1 331 20
	add.s32 	%r1221, %r501, %r1073;
	.loc	1 330 20
	add.s32 	%r1008, %r1222, %r530;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1217, %r1218, %r1219, %r1220 }, [ %r1008 + 0 ];
	// end inline asm
	add.s32 	%r1013, %r1008, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1213, %r1214, %r1215, %r1216 }, [ %r1013 + 0 ];
	// end inline asm
	add.s32 	%r1018, %r1008, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1209, %r1210, %r1211, %r1212 }, [ %r1018 + 0 ];
	// end inline asm
	add.s32 	%r1023, %r1008, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1205, %r1206, %r1207, %r1208 }, [ %r1023 + 0 ];
	// end inline asm
	.loc	1 331 20
	add.s32 	%r1028, %r1221, %r536;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1201, %r1202, %r1203, %r1204 }, [ %r1028 + 0 ];
	// end inline asm
	add.s32 	%r1033, %r1221, %r540;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1197, %r1198, %r1199, %r1200 }, [ %r1033 + 0 ];
	// end inline asm
	add.s32 	%r1038, %r1221, %r544;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1193, %r1194, %r1195, %r1196 }, [ %r1038 + 0 ];
	// end inline asm
	add.s32 	%r1043, %r1221, %r548;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1189, %r1190, %r1191, %r1192 }, [ %r1043 + 0 ];
	// end inline asm
	.loc	1 327 22
	add.s32 	%r1225, %r1225, 1;
	add.s64 	%rd132, %rd132, %rd8;
	add.s64 	%rd131, %rd131, 64;
	add.s64 	%rd130, %rd130, 64;
	add.s64 	%rd129, %rd129, 64;
	add.s64 	%rd128, %rd128, 64;
	add.s32 	%r1188, %r1188, -32;
	setp.lt.s32 	%p60, %r1225, %r16;
	@%p60 bra 	$L__BB0_2;
	.loc	1 341 23
	cvt.rn.f16.f32 	%rs1, %f897;
	cvt.rn.f16.f32 	%rs2, %f896;
	mov.b32 	%r1289, {%rs2, %rs1};
	cvt.rn.f16.f32 	%rs3, %f895;
	cvt.rn.f16.f32 	%rs4, %f894;
	mov.b32 	%r1288, {%rs4, %rs3};
	cvt.rn.f16.f32 	%rs5, %f893;
	cvt.rn.f16.f32 	%rs6, %f892;
	mov.b32 	%r1287, {%rs6, %rs5};
	cvt.rn.f16.f32 	%rs7, %f891;
	cvt.rn.f16.f32 	%rs8, %f890;
	mov.b32 	%r1286, {%rs8, %rs7};
	cvt.rn.f16.f32 	%rs9, %f889;
	cvt.rn.f16.f32 	%rs10, %f888;
	mov.b32 	%r1285, {%rs10, %rs9};
	cvt.rn.f16.f32 	%rs11, %f887;
	cvt.rn.f16.f32 	%rs12, %f886;
	mov.b32 	%r1284, {%rs12, %rs11};
	cvt.rn.f16.f32 	%rs13, %f885;
	cvt.rn.f16.f32 	%rs14, %f884;
	mov.b32 	%r1283, {%rs14, %rs13};
	cvt.rn.f16.f32 	%rs15, %f883;
	cvt.rn.f16.f32 	%rs16, %f882;
	mov.b32 	%r1282, {%rs16, %rs15};
	cvt.rn.f16.f32 	%rs17, %f881;
	cvt.rn.f16.f32 	%rs18, %f880;
	mov.b32 	%r1281, {%rs18, %rs17};
	cvt.rn.f16.f32 	%rs19, %f879;
	cvt.rn.f16.f32 	%rs20, %f878;
	mov.b32 	%r1280, {%rs20, %rs19};
	cvt.rn.f16.f32 	%rs21, %f877;
	cvt.rn.f16.f32 	%rs22, %f876;
	mov.b32 	%r1279, {%rs22, %rs21};
	cvt.rn.f16.f32 	%rs23, %f875;
	cvt.rn.f16.f32 	%rs24, %f874;
	mov.b32 	%r1278, {%rs24, %rs23};
	cvt.rn.f16.f32 	%rs25, %f873;
	cvt.rn.f16.f32 	%rs26, %f872;
	mov.b32 	%r1277, {%rs26, %rs25};
	cvt.rn.f16.f32 	%rs27, %f871;
	cvt.rn.f16.f32 	%rs28, %f870;
	mov.b32 	%r1276, {%rs28, %rs27};
	cvt.rn.f16.f32 	%rs29, %f869;
	cvt.rn.f16.f32 	%rs30, %f868;
	mov.b32 	%r1275, {%rs30, %rs29};
	cvt.rn.f16.f32 	%rs31, %f867;
	cvt.rn.f16.f32 	%rs32, %f866;
	mov.b32 	%r1274, {%rs32, %rs31};
	cvt.rn.f16.f32 	%rs33, %f865;
	cvt.rn.f16.f32 	%rs34, %f864;
	mov.b32 	%r1273, {%rs34, %rs33};
	cvt.rn.f16.f32 	%rs35, %f863;
	cvt.rn.f16.f32 	%rs36, %f862;
	mov.b32 	%r1272, {%rs36, %rs35};
	cvt.rn.f16.f32 	%rs37, %f861;
	cvt.rn.f16.f32 	%rs38, %f860;
	mov.b32 	%r1271, {%rs38, %rs37};
	cvt.rn.f16.f32 	%rs39, %f859;
	cvt.rn.f16.f32 	%rs40, %f858;
	mov.b32 	%r1270, {%rs40, %rs39};
	cvt.rn.f16.f32 	%rs41, %f857;
	cvt.rn.f16.f32 	%rs42, %f856;
	mov.b32 	%r1269, {%rs42, %rs41};
	cvt.rn.f16.f32 	%rs43, %f855;
	cvt.rn.f16.f32 	%rs44, %f854;
	mov.b32 	%r1268, {%rs44, %rs43};
	cvt.rn.f16.f32 	%rs45, %f853;
	cvt.rn.f16.f32 	%rs46, %f852;
	mov.b32 	%r1267, {%rs46, %rs45};
	cvt.rn.f16.f32 	%rs47, %f851;
	cvt.rn.f16.f32 	%rs48, %f850;
	mov.b32 	%r1266, {%rs48, %rs47};
	cvt.rn.f16.f32 	%rs49, %f849;
	cvt.rn.f16.f32 	%rs50, %f848;
	mov.b32 	%r1265, {%rs50, %rs49};
	cvt.rn.f16.f32 	%rs51, %f847;
	cvt.rn.f16.f32 	%rs52, %f846;
	mov.b32 	%r1264, {%rs52, %rs51};
	cvt.rn.f16.f32 	%rs53, %f845;
	cvt.rn.f16.f32 	%rs54, %f844;
	mov.b32 	%r1263, {%rs54, %rs53};
	cvt.rn.f16.f32 	%rs55, %f843;
	cvt.rn.f16.f32 	%rs56, %f842;
	mov.b32 	%r1262, {%rs56, %rs55};
	cvt.rn.f16.f32 	%rs57, %f841;
	cvt.rn.f16.f32 	%rs58, %f840;
	mov.b32 	%r1261, {%rs58, %rs57};
	cvt.rn.f16.f32 	%rs59, %f839;
	cvt.rn.f16.f32 	%rs60, %f838;
	mov.b32 	%r1260, {%rs60, %rs59};
	cvt.rn.f16.f32 	%rs61, %f837;
	cvt.rn.f16.f32 	%rs62, %f836;
	mov.b32 	%r1259, {%rs62, %rs61};
	cvt.rn.f16.f32 	%rs63, %f835;
	cvt.rn.f16.f32 	%rs64, %f834;
	mov.b32 	%r1258, {%rs64, %rs63};
	cvt.rn.f16.f32 	%rs65, %f833;
	cvt.rn.f16.f32 	%rs66, %f832;
	mov.b32 	%r1257, {%rs66, %rs65};
	cvt.rn.f16.f32 	%rs67, %f831;
	cvt.rn.f16.f32 	%rs68, %f830;
	mov.b32 	%r1256, {%rs68, %rs67};
	cvt.rn.f16.f32 	%rs69, %f829;
	cvt.rn.f16.f32 	%rs70, %f828;
	mov.b32 	%r1255, {%rs70, %rs69};
	cvt.rn.f16.f32 	%rs71, %f827;
	cvt.rn.f16.f32 	%rs72, %f826;
	mov.b32 	%r1254, {%rs72, %rs71};
	cvt.rn.f16.f32 	%rs73, %f825;
	cvt.rn.f16.f32 	%rs74, %f824;
	mov.b32 	%r1253, {%rs74, %rs73};
	cvt.rn.f16.f32 	%rs75, %f823;
	cvt.rn.f16.f32 	%rs76, %f822;
	mov.b32 	%r1252, {%rs76, %rs75};
	cvt.rn.f16.f32 	%rs77, %f821;
	cvt.rn.f16.f32 	%rs78, %f820;
	mov.b32 	%r1251, {%rs78, %rs77};
	cvt.rn.f16.f32 	%rs79, %f819;
	cvt.rn.f16.f32 	%rs80, %f818;
	mov.b32 	%r1250, {%rs80, %rs79};
	cvt.rn.f16.f32 	%rs81, %f817;
	cvt.rn.f16.f32 	%rs82, %f816;
	mov.b32 	%r1249, {%rs82, %rs81};
	cvt.rn.f16.f32 	%rs83, %f815;
	cvt.rn.f16.f32 	%rs84, %f814;
	mov.b32 	%r1248, {%rs84, %rs83};
	cvt.rn.f16.f32 	%rs85, %f813;
	cvt.rn.f16.f32 	%rs86, %f812;
	mov.b32 	%r1247, {%rs86, %rs85};
	cvt.rn.f16.f32 	%rs87, %f811;
	cvt.rn.f16.f32 	%rs88, %f810;
	mov.b32 	%r1246, {%rs88, %rs87};
	cvt.rn.f16.f32 	%rs89, %f809;
	cvt.rn.f16.f32 	%rs90, %f808;
	mov.b32 	%r1245, {%rs90, %rs89};
	cvt.rn.f16.f32 	%rs91, %f807;
	cvt.rn.f16.f32 	%rs92, %f806;
	mov.b32 	%r1244, {%rs92, %rs91};
	cvt.rn.f16.f32 	%rs93, %f805;
	cvt.rn.f16.f32 	%rs94, %f804;
	mov.b32 	%r1243, {%rs94, %rs93};
	cvt.rn.f16.f32 	%rs95, %f803;
	cvt.rn.f16.f32 	%rs96, %f802;
	mov.b32 	%r1242, {%rs96, %rs95};
	cvt.rn.f16.f32 	%rs97, %f801;
	cvt.rn.f16.f32 	%rs98, %f800;
	mov.b32 	%r1241, {%rs98, %rs97};
	cvt.rn.f16.f32 	%rs99, %f799;
	cvt.rn.f16.f32 	%rs100, %f798;
	mov.b32 	%r1240, {%rs100, %rs99};
	cvt.rn.f16.f32 	%rs101, %f797;
	cvt.rn.f16.f32 	%rs102, %f796;
	mov.b32 	%r1239, {%rs102, %rs101};
	cvt.rn.f16.f32 	%rs103, %f795;
	cvt.rn.f16.f32 	%rs104, %f794;
	mov.b32 	%r1238, {%rs104, %rs103};
	cvt.rn.f16.f32 	%rs105, %f793;
	cvt.rn.f16.f32 	%rs106, %f792;
	mov.b32 	%r1237, {%rs106, %rs105};
	cvt.rn.f16.f32 	%rs107, %f791;
	cvt.rn.f16.f32 	%rs108, %f790;
	mov.b32 	%r1236, {%rs108, %rs107};
	cvt.rn.f16.f32 	%rs109, %f789;
	cvt.rn.f16.f32 	%rs110, %f788;
	mov.b32 	%r1235, {%rs110, %rs109};
	cvt.rn.f16.f32 	%rs111, %f787;
	cvt.rn.f16.f32 	%rs112, %f786;
	mov.b32 	%r1234, {%rs112, %rs111};
	cvt.rn.f16.f32 	%rs113, %f785;
	cvt.rn.f16.f32 	%rs114, %f784;
	mov.b32 	%r1233, {%rs114, %rs113};
	cvt.rn.f16.f32 	%rs115, %f783;
	cvt.rn.f16.f32 	%rs116, %f782;
	mov.b32 	%r1232, {%rs116, %rs115};
	cvt.rn.f16.f32 	%rs117, %f781;
	cvt.rn.f16.f32 	%rs118, %f780;
	mov.b32 	%r1231, {%rs118, %rs117};
	cvt.rn.f16.f32 	%rs119, %f779;
	cvt.rn.f16.f32 	%rs120, %f778;
	mov.b32 	%r1230, {%rs120, %rs119};
	cvt.rn.f16.f32 	%rs121, %f777;
	cvt.rn.f16.f32 	%rs122, %f776;
	mov.b32 	%r1229, {%rs122, %rs121};
	cvt.rn.f16.f32 	%rs123, %f775;
	cvt.rn.f16.f32 	%rs124, %f774;
	mov.b32 	%r1228, {%rs124, %rs123};
	cvt.rn.f16.f32 	%rs125, %f773;
	cvt.rn.f16.f32 	%rs126, %f772;
	mov.b32 	%r1227, {%rs126, %rs125};
	cvt.rn.f16.f32 	%rs127, %f771;
	cvt.rn.f16.f32 	%rs128, %f770;
	mov.b32 	%r1226, {%rs128, %rs127};
$L__BB0_4:
	.loc	1 313 51
	or.b32  	%r1139, %r1, %r8;
	.loc	1 313 38
	or.b32  	%r1140, %r1139, 120;
	or.b32  	%r1141, %r1139, 112;
	or.b32  	%r1142, %r1139, 104;
	or.b32  	%r1143, %r1139, 96;
	or.b32  	%r1144, %r1139, 88;
	or.b32  	%r1145, %r1139, 80;
	or.b32  	%r1146, %r1139, 72;
	or.b32  	%r1147, %r1139, 64;
	or.b32  	%r1148, %r1139, 56;
	or.b32  	%r1149, %r1139, 48;
	or.b32  	%r1150, %r1139, 40;
	or.b32  	%r1151, %r1139, 32;
	or.b32  	%r1152, %r1, %r11;
	or.b32  	%r1153, %r1, %r10;
	or.b32  	%r1154, %r1, %r9;
	.loc	1 327 22
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 347 33
	mul.lo.s32 	%r1155, %r1139, %r276;
	mul.lo.s32 	%r1156, %r1154, %r276;
	mul.lo.s32 	%r1157, %r1153, %r276;
	mul.lo.s32 	%r1158, %r1152, %r276;
	shl.b32 	%r1159, %r276, 5;
	add.s32 	%r1160, %r1155, %r1159;
	shl.b32 	%r1161, %r276, 3;
	add.s32 	%r1162, %r1160, %r1161;
	add.s32 	%r1163, %r1162, %r1161;
	add.s32 	%r1164, %r1163, %r1161;
	add.s32 	%r1165, %r1164, %r1161;
	add.s32 	%r1166, %r1165, %r1161;
	add.s32 	%r1167, %r1166, %r1161;
	add.s32 	%r1168, %r1167, %r1161;
	add.s32 	%r1169, %r1168, %r1161;
	add.s32 	%r1170, %r1169, %r1161;
	add.s32 	%r1171, %r1170, %r1161;
	add.s32 	%r1172, %r1171, %r1161;
	.loc	1 347 21
	mul.wide.s32 	%rd95, %r1155, 2;
	add.s64 	%rd96, %rd28, %rd95;
	mul.wide.s32 	%rd97, %r1156, 2;
	add.s64 	%rd98, %rd28, %rd97;
	mul.wide.s32 	%rd99, %r1157, 2;
	add.s64 	%rd100, %rd28, %rd99;
	mul.wide.s32 	%rd101, %r1158, 2;
	add.s64 	%rd102, %rd28, %rd101;
	mul.wide.s32 	%rd103, %r1160, 2;
	add.s64 	%rd104, %rd28, %rd103;
	mul.wide.s32 	%rd105, %r1162, 2;
	add.s64 	%rd106, %rd28, %rd105;
	mul.wide.s32 	%rd107, %r1163, 2;
	add.s64 	%rd108, %rd28, %rd107;
	mul.wide.s32 	%rd109, %r1164, 2;
	add.s64 	%rd110, %rd28, %rd109;
	mul.wide.s32 	%rd111, %r1165, 2;
	add.s64 	%rd112, %rd28, %rd111;
	mul.wide.s32 	%rd113, %r1166, 2;
	add.s64 	%rd114, %rd28, %rd113;
	mul.wide.s32 	%rd115, %r1167, 2;
	add.s64 	%rd116, %rd28, %rd115;
	mul.wide.s32 	%rd117, %r1168, 2;
	add.s64 	%rd118, %rd28, %rd117;
	mul.wide.s32 	%rd119, %r1169, 2;
	add.s64 	%rd120, %rd28, %rd119;
	mul.wide.s32 	%rd121, %r1170, 2;
	add.s64 	%rd122, %rd28, %rd121;
	mul.wide.s32 	%rd123, %r1171, 2;
	add.s64 	%rd124, %rd28, %rd123;
	mul.wide.s32 	%rd125, %r1172, 2;
	add.s64 	%rd126, %rd28, %rd125;
	.loc	1 347 52
	mul.wide.s32 	%rd127, %r14, 2;
	add.s64 	%rd79, %rd96, %rd127;
	add.s64 	%rd80, %rd98, %rd127;
	add.s64 	%rd81, %rd100, %rd127;
	add.s64 	%rd82, %rd102, %rd127;
	add.s64 	%rd83, %rd104, %rd127;
	add.s64 	%rd84, %rd106, %rd127;
	add.s64 	%rd85, %rd108, %rd127;
	add.s64 	%rd86, %rd110, %rd127;
	add.s64 	%rd87, %rd112, %rd127;
	add.s64 	%rd88, %rd114, %rd127;
	add.s64 	%rd89, %rd116, %rd127;
	add.s64 	%rd90, %rd118, %rd127;
	add.s64 	%rd91, %rd120, %rd127;
	add.s64 	%rd92, %rd122, %rd127;
	add.s64 	%rd93, %rd124, %rd127;
	add.s64 	%rd94, %rd126, %rd127;
	.loc	1 348 33
	setp.lt.s32 	%p77, %r1139, %r272;
	setp.lt.s32 	%p78, %r1154, %r272;
	setp.lt.s32 	%p79, %r1153, %r272;
	setp.lt.s32 	%p80, %r1152, %r272;
	setp.lt.s32 	%p81, %r1151, %r272;
	setp.lt.s32 	%p82, %r1150, %r272;
	setp.lt.s32 	%p83, %r1149, %r272;
	setp.lt.s32 	%p84, %r1148, %r272;
	setp.lt.s32 	%p85, %r1147, %r272;
	setp.lt.s32 	%p86, %r1146, %r272;
	setp.lt.s32 	%p87, %r1145, %r272;
	setp.lt.s32 	%p88, %r1144, %r272;
	setp.lt.s32 	%p89, %r1143, %r272;
	setp.lt.s32 	%p90, %r1142, %r272;
	setp.lt.s32 	%p91, %r1141, %r272;
	setp.lt.s32 	%p92, %r1140, %r272;
	.loc	1 348 58
	setp.lt.s32 	%p93, %r14, %r273;
	.loc	1 348 39
	and.pred  	%p61, %p77, %p93;
	and.pred  	%p62, %p78, %p93;
	and.pred  	%p63, %p79, %p93;
	and.pred  	%p64, %p80, %p93;
	and.pred  	%p65, %p81, %p93;
	and.pred  	%p66, %p82, %p93;
	and.pred  	%p67, %p83, %p93;
	and.pred  	%p68, %p84, %p93;
	and.pred  	%p69, %p85, %p93;
	and.pred  	%p70, %p86, %p93;
	and.pred  	%p71, %p87, %p93;
	and.pred  	%p72, %p88, %p93;
	and.pred  	%p73, %p89, %p93;
	and.pred  	%p74, %p90, %p93;
	and.pred  	%p75, %p91, %p93;
	and.pred  	%p76, %p92, %p93;
	.loc	1 349 21
	shl.b32 	%r1173, %r2, 1;
	and.b32  	%r1174, %r1173, 6;
	and.b32  	%r1175, %r4, 23;
	shl.b32 	%r1176, %r44, 3;
	or.b32  	%r1177, %r1176, %r1174;
	mad.lo.s32 	%r1178, %r1175, 136, %r1177;
	shl.b32 	%r1179, %r1178, 1;
	add.s32 	%r1181, %r489, %r1179;
	st.shared.b32 	[%r1181], %r1226;
	st.shared.b32 	[%r1181+2176], %r1227;
	st.shared.b32 	[%r1181+32], %r1228;
	st.shared.b32 	[%r1181+2208], %r1229;
	st.shared.b32 	[%r1181+64], %r1230;
	st.shared.b32 	[%r1181+2240], %r1231;
	st.shared.b32 	[%r1181+96], %r1232;
	st.shared.b32 	[%r1181+2272], %r1233;
	st.shared.b32 	[%r1181+128], %r1234;
	st.shared.b32 	[%r1181+2304], %r1235;
	st.shared.b32 	[%r1181+160], %r1236;
	st.shared.b32 	[%r1181+2336], %r1237;
	st.shared.b32 	[%r1181+192], %r1238;
	st.shared.b32 	[%r1181+2368], %r1239;
	st.shared.b32 	[%r1181+224], %r1240;
	st.shared.b32 	[%r1181+2400], %r1241;
	bar.sync 	0;
	shl.b32 	%r1182, %r3, 1;
	and.b32  	%r1183, %r1182, 6;
	or.b32  	%r1184, %r1183, %r5;
	mad.lo.s32 	%r1185, %r1184, 136, %r13;
	shl.b32 	%r1186, %r1185, 1;
	add.s32 	%r1187, %r489, %r1186;
	ld.shared.v4.u32 	{%r1075, %r1076, %r1077, %r1078}, [%r1187];
	ld.shared.v4.u32 	{%r1079, %r1080, %r1081, %r1082}, [%r1187+2176];
	ld.shared.v4.u32 	{%r1083, %r1084, %r1085, %r1086}, [%r1187+4352];
	ld.shared.v4.u32 	{%r1087, %r1088, %r1089, %r1090}, [%r1187+6528];
	bar.sync 	0;
	st.shared.b32 	[%r1181], %r1242;
	st.shared.b32 	[%r1181+2176], %r1243;
	st.shared.b32 	[%r1181+32], %r1244;
	st.shared.b32 	[%r1181+2208], %r1245;
	st.shared.b32 	[%r1181+64], %r1246;
	st.shared.b32 	[%r1181+2240], %r1247;
	st.shared.b32 	[%r1181+96], %r1248;
	st.shared.b32 	[%r1181+2272], %r1249;
	st.shared.b32 	[%r1181+128], %r1250;
	st.shared.b32 	[%r1181+2304], %r1251;
	st.shared.b32 	[%r1181+160], %r1252;
	st.shared.b32 	[%r1181+2336], %r1253;
	st.shared.b32 	[%r1181+192], %r1254;
	st.shared.b32 	[%r1181+2368], %r1255;
	st.shared.b32 	[%r1181+224], %r1256;
	st.shared.b32 	[%r1181+2400], %r1257;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r1091, %r1092, %r1093, %r1094}, [%r1187];
	ld.shared.v4.u32 	{%r1095, %r1096, %r1097, %r1098}, [%r1187+2176];
	ld.shared.v4.u32 	{%r1099, %r1100, %r1101, %r1102}, [%r1187+4352];
	ld.shared.v4.u32 	{%r1103, %r1104, %r1105, %r1106}, [%r1187+6528];
	bar.sync 	0;
	st.shared.b32 	[%r1181], %r1258;
	st.shared.b32 	[%r1181+2176], %r1259;
	st.shared.b32 	[%r1181+32], %r1260;
	st.shared.b32 	[%r1181+2208], %r1261;
	st.shared.b32 	[%r1181+64], %r1262;
	st.shared.b32 	[%r1181+2240], %r1263;
	st.shared.b32 	[%r1181+96], %r1264;
	st.shared.b32 	[%r1181+2272], %r1265;
	st.shared.b32 	[%r1181+128], %r1266;
	st.shared.b32 	[%r1181+2304], %r1267;
	st.shared.b32 	[%r1181+160], %r1268;
	st.shared.b32 	[%r1181+2336], %r1269;
	st.shared.b32 	[%r1181+192], %r1270;
	st.shared.b32 	[%r1181+2368], %r1271;
	st.shared.b32 	[%r1181+224], %r1272;
	st.shared.b32 	[%r1181+2400], %r1273;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r1107, %r1108, %r1109, %r1110}, [%r1187];
	ld.shared.v4.u32 	{%r1111, %r1112, %r1113, %r1114}, [%r1187+2176];
	ld.shared.v4.u32 	{%r1115, %r1116, %r1117, %r1118}, [%r1187+4352];
	ld.shared.v4.u32 	{%r1119, %r1120, %r1121, %r1122}, [%r1187+6528];
	bar.sync 	0;
	st.shared.b32 	[%r1181], %r1274;
	st.shared.b32 	[%r1181+2176], %r1275;
	st.shared.b32 	[%r1181+32], %r1276;
	st.shared.b32 	[%r1181+2208], %r1277;
	st.shared.b32 	[%r1181+64], %r1278;
	st.shared.b32 	[%r1181+2240], %r1279;
	st.shared.b32 	[%r1181+96], %r1280;
	st.shared.b32 	[%r1181+2272], %r1281;
	st.shared.b32 	[%r1181+128], %r1282;
	st.shared.b32 	[%r1181+2304], %r1283;
	st.shared.b32 	[%r1181+160], %r1284;
	st.shared.b32 	[%r1181+2336], %r1285;
	st.shared.b32 	[%r1181+192], %r1286;
	st.shared.b32 	[%r1181+2368], %r1287;
	st.shared.b32 	[%r1181+224], %r1288;
	st.shared.b32 	[%r1181+2400], %r1289;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r1123, %r1124, %r1125, %r1126}, [%r1187];
	ld.shared.v4.u32 	{%r1127, %r1128, %r1129, %r1130}, [%r1187+2176];
	ld.shared.v4.u32 	{%r1131, %r1132, %r1133, %r1134}, [%r1187+4352];
	ld.shared.v4.u32 	{%r1135, %r1136, %r1137, %r1138}, [%r1187+6528];
	// begin inline asm
	@%p61 st.global.v4.b32 [ %rd79 + 0 ], { %r1075, %r1076, %r1077, %r1078 };
	// end inline asm
	// begin inline asm
	@%p62 st.global.v4.b32 [ %rd80 + 0 ], { %r1079, %r1080, %r1081, %r1082 };
	// end inline asm
	// begin inline asm
	@%p63 st.global.v4.b32 [ %rd81 + 0 ], { %r1083, %r1084, %r1085, %r1086 };
	// end inline asm
	// begin inline asm
	@%p64 st.global.v4.b32 [ %rd82 + 0 ], { %r1087, %r1088, %r1089, %r1090 };
	// end inline asm
	// begin inline asm
	@%p65 st.global.v4.b32 [ %rd83 + 0 ], { %r1091, %r1092, %r1093, %r1094 };
	// end inline asm
	// begin inline asm
	@%p66 st.global.v4.b32 [ %rd84 + 0 ], { %r1095, %r1096, %r1097, %r1098 };
	// end inline asm
	// begin inline asm
	@%p67 st.global.v4.b32 [ %rd85 + 0 ], { %r1099, %r1100, %r1101, %r1102 };
	// end inline asm
	// begin inline asm
	@%p68 st.global.v4.b32 [ %rd86 + 0 ], { %r1103, %r1104, %r1105, %r1106 };
	// end inline asm
	// begin inline asm
	@%p69 st.global.v4.b32 [ %rd87 + 0 ], { %r1107, %r1108, %r1109, %r1110 };
	// end inline asm
	// begin inline asm
	@%p70 st.global.v4.b32 [ %rd88 + 0 ], { %r1111, %r1112, %r1113, %r1114 };
	// end inline asm
	// begin inline asm
	@%p71 st.global.v4.b32 [ %rd89 + 0 ], { %r1115, %r1116, %r1117, %r1118 };
	// end inline asm
	// begin inline asm
	@%p72 st.global.v4.b32 [ %rd90 + 0 ], { %r1119, %r1120, %r1121, %r1122 };
	// end inline asm
	// begin inline asm
	@%p73 st.global.v4.b32 [ %rd91 + 0 ], { %r1123, %r1124, %r1125, %r1126 };
	// end inline asm
	// begin inline asm
	@%p74 st.global.v4.b32 [ %rd92 + 0 ], { %r1127, %r1128, %r1129, %r1130 };
	// end inline asm
	// begin inline asm
	@%p75 st.global.v4.b32 [ %rd93 + 0 ], { %r1131, %r1132, %r1133, %r1134 };
	// end inline asm
	// begin inline asm
	@%p76 st.global.v4.b32 [ %rd94 + 0 ], { %r1135, %r1136, %r1137, %r1138 };
	// end inline asm
	.loc	1 349 4
	ret;
$L__tmp6:
$L__func_end0:

}
	.file	1 "/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/02.matrix-multiplication/matrix_multiplication.py"
	.file	2 "/data_ssd1/zjy_home/frameworks/cuda/triton/python/triton/language/standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 5
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 265
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 109
.b8 97
.b8 116
.b8 114
.b8 105
.b8 120
.b8 95
.b8 109
.b8 117
.b8 108
.b8 116
.b8 105
.b8 112
.b8 108
.b8 105
.b8 99
.b8 97
.b8 116
.b8 105
.b8 111
.b8 110
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 100
.b8 97
.b8 116
.b8 97
.b8 95
.b8 115
.b8 115
.b8 100
.b8 49
.b8 47
.b8 122
.b8 106
.b8 121
.b8 95
.b8 104
.b8 111
.b8 109
.b8 101
.b8 47
.b8 109
.b8 121
.b8 95
.b8 99
.b8 111
.b8 100
.b8 101
.b8 47
.b8 77
.b8 76
.b8 67
.b8 45
.b8 76
.b8 101
.b8 97
.b8 114
.b8 110
.b8 105
.b8 110
.b8 103
.b8 47
.b8 84
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 47
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 45
.b8 101
.b8 120
.b8 97
.b8 109
.b8 112
.b8 108
.b8 101
.b8 115
.b8 47
.b8 48
.b8 50
.b8 46
.b8 109
.b8 97
.b8 116
.b8 114
.b8 105
.b8 120
.b8 45
.b8 109
.b8 117
.b8 108
.b8 116
.b8 105
.b8 112
.b8 108
.b8 105
.b8 99
.b8 97
.b8 116
.b8 105
.b8 111
.b8 110
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 109
.b8 97
.b8 116
.b8 109
.b8 117
.b8 108
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 155
.b8 4
.b32 155
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 37
.b8 1
.b8 27
.b8 4
.b32 155
.b64 $L__tmp2
.b64 $L__tmp3
.b8 1
.b8 38
.b8 1
.b8 27
.b8 4
.b32 155
.b64 $L__tmp4
.b64 $L__tmp5
.b8 1
.b8 71
.b8 1
.b8 33
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}

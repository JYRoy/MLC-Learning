//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_80
.address_size 64

	// .globl	matmul_kernel
.extern .shared .align 16 .b8 global_smem[];

.visible .entry matmul_kernel(
	.param .u64 matmul_kernel_param_0,
	.param .u64 matmul_kernel_param_1,
	.param .u64 matmul_kernel_param_2,
	.param .u32 matmul_kernel_param_3,
	.param .u32 matmul_kernel_param_4,
	.param .u32 matmul_kernel_param_5,
	.param .u32 matmul_kernel_param_6,
	.param .u32 matmul_kernel_param_7,
	.param .u32 matmul_kernel_param_8
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<48>;
	.reg .b16 	%rs<211>;
	.reg .b32 	%r<918>;
	.reg .f32 	%f<354>;
	.reg .b64 	%rd<67>;
	.loc	1 260 0
$L__func_begin0:
	.loc	1 260 0

	ld.param.u32 	%r82, [matmul_kernel_param_8];
	ld.param.u32 	%r81, [matmul_kernel_param_5];
	ld.param.u32 	%r80, [matmul_kernel_param_4];
	ld.param.u32 	%r79, [matmul_kernel_param_3];
	ld.param.u64 	%rd23, [matmul_kernel_param_2];
	ld.param.u64 	%rd22, [matmul_kernel_param_1];
	ld.param.u64 	%rd21, [matmul_kernel_param_0];
$L__tmp0:
	.loc	1 292 24
	// begin inline asm
	mov.u32 %r83, %ctaid.x;
	// end inline asm
$L__tmp1:
	.loc	2 44 22
	add.s32 	%r134, %r79, 127;
	.loc	2 44 28
	shr.s32 	%r135, %r134, 31;
	shr.u32 	%r136, %r135, 25;
	add.s32 	%r137, %r134, %r136;
	shr.s32 	%r138, %r137, 7;
$L__tmp2:
	.loc	2 44 22
	add.s32 	%r139, %r80, 31;
	.loc	2 44 28
	shr.s32 	%r140, %r139, 31;
	shr.u32 	%r141, %r140, 27;
	add.s32 	%r142, %r139, %r141;
	shr.s32 	%r143, %r142, 5;
$L__tmp3:
	.loc	1 295 38
	shl.b32 	%r145, %r143, 3;
	ld.param.u32 	%r146, [matmul_kernel_param_6];
	ld.param.u32 	%r147, [matmul_kernel_param_7];
	.loc	1 296 22
	div.s32 	%r148, %r83, %r145;
	.loc	1 297 29
	shl.b32 	%r149, %r148, 3;
	.loc	1 299 20
	sub.s32 	%r150, %r138, %r149;
	.loc	1 299 33
	min.s32 	%r152, %r150, 8;
	mul.lo.s32 	%r153, %r148, %r145;
	sub.s32 	%r154, %r83, %r153;
	.loc	1 304 40
	div.s32 	%r155, %r154, %r152;
	mul.lo.s32 	%r156, %r155, %r152;
	sub.s32 	%r157, %r154, %r156;
	.loc	1 302 8
	add.s32 	%r158, %r157, %r149;
	.loc	1 313 23
	shl.b32 	%r159, %r158, 7;
	.loc	1 313 51
	mov.u32 	%r1, %tid.x;
	and.b32  	%r2, %r1, 31;
	shr.u32 	%r3, %r1, 5;
	bfe.u32 	%r160, %r1, 2, 5;
	or.b32  	%r161, %r160, 32;
	or.b32  	%r162, %r160, 64;
	or.b32  	%r163, %r160, 96;
	.loc	1 313 38
	or.b32  	%r4, %r159, %r160;
	or.b32  	%r5, %r159, %r161;
	or.b32  	%r6, %r159, %r162;
	or.b32  	%r7, %r159, %r163;
	.loc	1 313 68
	rem.s32 	%r164, %r4, %r79;
	rem.s32 	%r165, %r5, %r79;
	rem.s32 	%r166, %r6, %r79;
	rem.s32 	%r167, %r7, %r79;
	.loc	1 314 23
	shl.b32 	%r8, %r155, 5;
	.loc	1 314 38
	or.b32  	%r168, %r8, %r160;
	.loc	1 314 68
	rem.s32 	%r169, %r168, %r80;
	.loc	1 316 60
	shl.b32 	%r170, %r1, 4;
	and.b32  	%r9, %r170, 48;
	.loc	1 316 53
	mad.lo.s32 	%r171, %r164, %r146, %r9;
	mad.lo.s32 	%r172, %r165, %r146, %r9;
	mad.lo.s32 	%r173, %r166, %r146, %r9;
	mad.lo.s32 	%r174, %r167, %r146, %r9;
	.loc	1 316 22
	cvt.s64.s32 	%rd1, %r171;
	add.s64 	%rd24, %rd21, %rd1;
	cvt.s64.s32 	%rd2, %r172;
	add.s64 	%rd25, %rd21, %rd2;
	cvt.s64.s32 	%rd3, %r173;
	add.s64 	%rd26, %rd21, %rd3;
	cvt.s64.s32 	%rd4, %r174;
	add.s64 	%rd27, %rd21, %rd4;
	.loc	1 318 52
	mad.lo.s32 	%r175, %r169, %r147, %r9;
	.loc	1 318 22
	cvt.s64.s32 	%rd5, %r175;
	add.s64 	%rd28, %rd22, %rd5;
$L__tmp4:
	.loc	2 44 22
	add.s32 	%r176, %r81, 63;
	mov.b32 	%r902, 0;
$L__tmp5:
	.loc	1 330 20
	// begin inline asm
	cvt.rz.f16.f32 %rs1, %r902;
	// end inline asm
	mov.b32 	%r86, {%rs1, %rs1};
	// begin inline asm
	{                            
.reg .b32 a<2>;              
and.b32 a0, %r86, 0xfffefffe;  
and.b32 a1, %r86, 0xfffefffe;  
add.u32 a0, a0, 0x00800080;  
add.u32 a1, a1, 0x00800080;  
prmt.b32 %r85, a0, a1, 0x7531; 
	}
	// end inline asm
	.loc	1 327 22
	setp.lt.s32 	%p23, %r176, 64;
	setp.gt.s32 	%p24, %r176, 63;
	.loc	1 330 51
	setp.lt.s32 	%p47, %r9, %r81;
	.loc	1 330 20
	shl.b32 	%r181, %r160, 6;
	shl.b32 	%r182, %r1, 1;
	xor.b32  	%r183, %r170, %r182;
	and.b32  	%r184, %r183, 48;
	or.b32  	%r15, %r181, %r184;
	mov.u32 	%r185, global_smem;
	add.s32 	%r88, %r185, %r15;
	shl.b32 	%r186, %r161, 6;
	or.b32  	%r16, %r186, %r184;
	add.s32 	%r90, %r185, %r16;
	shl.b32 	%r187, %r162, 6;
	or.b32  	%r17, %r187, %r184;
	add.s32 	%r92, %r185, %r17;
	shl.b32 	%r188, %r163, 6;
	or.b32  	%r18, %r188, %r184;
	add.s32 	%r94, %r185, %r18;
	selp.b32 	%r189, 16, 0, %p24;
	selp.b32 	%r91, %r189, 0, %p47;
	mov.pred 	%p8, -1;
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r88 + 0 ], [ %rd24 + 0 ], 0x10, %r91;
	// end inline asm
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r90 + 0 ], [ %rd25 + 0 ], 0x10, %r91;
	// end inline asm
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r92 + 0 ], [ %rd26 + 0 ], 0x10, %r91;
	// end inline asm
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r94 + 0 ], [ %rd27 + 0 ], 0x10, %r91;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 20
	add.s32 	%r96, %r88, 24576;
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r96 + 0 ], [ %rd28 + 0 ], 0x10, %r91;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 327 22
	setp.gt.s32 	%p25, %r176, 127;
	.loc	1 335 18
	add.s64 	%rd29, %rd24, 64;
	add.s64 	%rd30, %rd25, 64;
	add.s64 	%rd31, %rd26, 64;
	add.s64 	%rd32, %rd27, 64;
	.loc	1 336 18
	add.s64 	%rd33, %rd28, 64;
	.loc	1 330 55
	add.s32 	%r190, %r81, -64;
	.loc	1 330 51
	setp.lt.s32 	%p46, %r9, %r190;
	.loc	1 330 20
	bar.sync 	0;
	add.s32 	%r191, %r185, 8192;
	add.s32 	%r98, %r191, %r15;
	add.s32 	%r100, %r191, %r16;
	add.s32 	%r102, %r191, %r17;
	add.s32 	%r104, %r191, %r18;
	selp.b32 	%r192, 16, 0, %p46;
	selp.b32 	%r101, %r192, 0, %p25;
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r98 + 0 ], [ %rd29 + 0 ], 0x10, %r101;
	// end inline asm
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r100 + 0 ], [ %rd30 + 0 ], 0x10, %r101;
	// end inline asm
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r102 + 0 ], [ %rd31 + 0 ], 0x10, %r101;
	// end inline asm
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r104 + 0 ], [ %rd32 + 0 ], 0x10, %r101;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 20
	add.s32 	%r106, %r88, 26624;
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r106 + 0 ], [ %rd33 + 0 ], 0x10, %r101;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 327 22
	setp.gt.s32 	%p26, %r176, 191;
	.loc	1 335 18
	add.s64 	%rd34, %rd24, 128;
	add.s64 	%rd35, %rd25, 128;
	add.s64 	%rd36, %rd26, 128;
	add.s64 	%rd37, %rd27, 128;
	.loc	1 336 18
	add.s64 	%rd38, %rd28, 128;
	.loc	1 330 55
	add.s32 	%r193, %r81, -128;
	.loc	1 330 51
	setp.lt.s32 	%p45, %r9, %r193;
	.loc	1 330 20
	bar.sync 	0;
	add.s32 	%r194, %r185, 16384;
	add.s32 	%r108, %r194, %r15;
	add.s32 	%r110, %r194, %r16;
	add.s32 	%r112, %r194, %r17;
	add.s32 	%r114, %r194, %r18;
	selp.b32 	%r195, 16, 0, %p45;
	selp.b32 	%r111, %r195, 0, %p26;
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r108 + 0 ], [ %rd34 + 0 ], 0x10, %r111;
	// end inline asm
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r110 + 0 ], [ %rd35 + 0 ], 0x10, %r111;
	// end inline asm
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r112 + 0 ], [ %rd36 + 0 ], 0x10, %r111;
	// end inline asm
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r114 + 0 ], [ %rd37 + 0 ], 0x10, %r111;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 20
	add.s32 	%r116, %r88, 28672;
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r116 + 0 ], [ %rd38 + 0 ], 0x10, %r111;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 330 20
	// begin inline asm
	cp.async.wait_group 0x4;
	// end inline asm
	bar.sync 	0;
	mov.u32 	%r903, %r902;
	mov.u32 	%r904, %r902;
	mov.u32 	%r905, %r902;
	mov.u32 	%r906, %r902;
	mov.u32 	%r907, %r902;
	mov.u32 	%r908, %r902;
	mov.u32 	%r909, %r902;
	mov.u32 	%r910, %r902;
	mov.u32 	%r911, %r902;
	mov.u32 	%r912, %r902;
	mov.u32 	%r913, %r902;
	mov.u32 	%r914, %r902;
	mov.u32 	%r915, %r902;
	mov.u32 	%r916, %r902;
	mov.u32 	%r917, %r902;
	.loc	1 327 22
	@%p23 bra 	$L__BB0_4;
	.loc	1 0 22
	shr.s32 	%r177, %r176, 31;
	shr.u32 	%r178, %r177, 26;
	add.s32 	%r179, %r176, %r178;
	shr.s32 	%r10, %r179, 6;
	add.s32 	%r20, %r10, -3;
	add.s32 	%r200, %r185, 30720;
	add.s32 	%r21, %r200, %r15;
	add.s32 	%r22, %r200, %r16;
	add.s32 	%r23, %r200, %r17;
	add.s32 	%r24, %r200, %r18;
	add.s32 	%r201, %r185, 38912;
	add.s32 	%r25, %r201, %r15;
	and.b32  	%r202, %r1, 7;
	bfe.u32 	%r203, %r1, 3, 1;
	shr.u32 	%r204, %r2, 4;
	shl.b32 	%r205, %r3, 1;
	and.b32  	%r206, %r205, 6;
	or.b32  	%r207, %r206, %r203;
	bfe.u32 	%r208, %r1, 1, 2;
	xor.b32  	%r209, %r204, %r208;
	shl.b32 	%r210, %r207, 9;
	shl.b32 	%r211, %r202, 6;
	or.b32  	%r212, %r210, %r211;
	shl.b32 	%r213, %r209, 4;
	or.b32  	%r214, %r212, %r213;
	add.s32 	%r232, %r200, %r214;
	or.b32  	%r215, %r204, 2;
	xor.b32  	%r216, %r215, %r208;
	shl.b32 	%r217, %r216, 4;
	or.b32  	%r218, %r212, %r217;
	add.s32 	%r237, %r200, %r218;
	add.s32 	%r242, %r232, 4096;
	add.s32 	%r247, %r237, 4096;
	xor.b32  	%r219, %r203, %r208;
	shl.b32 	%r220, %r204, 9;
	or.b32  	%r221, %r220, %r211;
	shl.b32 	%r222, %r219, 4;
	or.b32  	%r223, %r222, %r221;
	add.s32 	%r300, %r201, %r223;
	or.b32  	%r224, %r203, 2;
	xor.b32  	%r225, %r224, %r208;
	shl.b32 	%r226, %r225, 4;
	or.b32  	%r227, %r226, %r221;
	add.s32 	%r305, %r201, %r227;
	add.s32 	%r310, %r300, 1024;
	add.s32 	%r315, %r305, 1024;
	add.s32 	%r896, %r81, -192;
	.loc	1 327 22
	add.s64 	%rd39, %rd5, %rd22;
	add.s64 	%rd66, %rd39, 192;
	add.s64 	%rd40, %rd4, %rd21;
	add.s64 	%rd65, %rd40, 192;
	add.s64 	%rd41, %rd3, %rd21;
	add.s64 	%rd64, %rd41, 192;
	add.s64 	%rd42, %rd2, %rd21;
	add.s64 	%rd63, %rd42, 192;
	add.s64 	%rd43, %rd1, %rd21;
	add.s64 	%rd62, %rd43, 192;
	add.s32 	%r897, %r185, 24576;
	mov.f32 	%f322, 0f00000000;
	mov.b32 	%r900, 2;
	mov.b32 	%r899, 0;
	mov.u32 	%r898, %r185;
	mov.f32 	%f323, %f322;
	mov.f32 	%f324, %f322;
	mov.f32 	%f325, %f322;
	mov.f32 	%f326, %f322;
	mov.f32 	%f327, %f322;
	mov.f32 	%f328, %f322;
	mov.f32 	%f329, %f322;
	mov.f32 	%f330, %f322;
	mov.f32 	%f331, %f322;
	mov.f32 	%f332, %f322;
	mov.f32 	%f333, %f322;
	mov.f32 	%f334, %f322;
	mov.f32 	%f335, %f322;
	mov.f32 	%f336, %f322;
	mov.f32 	%f337, %f322;
	mov.f32 	%f338, %f322;
	mov.f32 	%f339, %f322;
	mov.f32 	%f340, %f322;
	mov.f32 	%f341, %f322;
	mov.f32 	%f342, %f322;
	mov.f32 	%f343, %f322;
	mov.f32 	%f344, %f322;
	mov.f32 	%f345, %f322;
	mov.f32 	%f346, %f322;
	mov.f32 	%f347, %f322;
	mov.f32 	%f348, %f322;
	mov.f32 	%f349, %f322;
	mov.f32 	%f350, %f322;
	mov.f32 	%f351, %f322;
	mov.f32 	%f352, %f322;
	mov.f32 	%f353, %f322;
	mov.u32 	%r901, %r899;
$L__BB0_2:
	.loc	1 0 22
	mov.pred 	%p5, %p46;
	mov.pred 	%p46, %p45;
	.loc	1 327 22
	setp.lt.s32 	%p32, %r901, %r20;
	.loc	1 330 20
	add.s32 	%r566, %r898, %r15;
	add.s32 	%r567, %r898, %r16;
	add.s32 	%r568, %r898, %r17;
	add.s32 	%r569, %r898, %r18;
	ld.shared.v4.u32 	{%r570, %r571, %r572, %r573}, [%r566];
	bfe.u32 	%r578, %r573, 0, 8;
	cvt.u16.u32 	%rs3, %r578;
	bfe.u32 	%r579, %r85, 0, 8;
	cvt.u16.u32 	%rs4, %r579;
	selp.b16 	%rs5, %rs3, %rs4, %p47;
	cvt.u32.u16 	%r580, %rs5;
	bfe.u32 	%r581, %r573, 8, 8;
	cvt.u16.u32 	%rs6, %r581;
	bfe.u32 	%r582, %r85, 8, 8;
	cvt.u16.u32 	%rs7, %r582;
	selp.b16 	%rs8, %rs6, %rs7, %p47;
	cvt.u32.u16 	%r583, %rs8;
	bfi.b32 	%r584, %r583, %r580, 8, 8;
	bfe.u32 	%r585, %r573, 16, 8;
	cvt.u16.u32 	%rs9, %r585;
	bfe.u32 	%r586, %r85, 16, 8;
	cvt.u16.u32 	%rs10, %r586;
	selp.b16 	%rs11, %rs9, %rs10, %p47;
	cvt.u32.u16 	%r587, %rs11;
	bfi.b32 	%r588, %r587, %r584, 16, 8;
	bfe.u32 	%r589, %r573, 24, 8;
	cvt.u16.u32 	%rs12, %r589;
	bfe.u32 	%r590, %r85, 24, 8;
	cvt.u16.u32 	%rs13, %r590;
	selp.b16 	%rs14, %rs12, %rs13, %p47;
	cvt.u32.u16 	%r591, %rs14;
	bfi.b32 	%r592, %r591, %r588, 24, 8;
	bfe.u32 	%r593, %r572, 0, 8;
	cvt.u16.u32 	%rs15, %r593;
	selp.b16 	%rs17, %rs15, %rs4, %p47;
	cvt.u32.u16 	%r595, %rs17;
	bfe.u32 	%r596, %r572, 8, 8;
	cvt.u16.u32 	%rs18, %r596;
	selp.b16 	%rs20, %rs18, %rs7, %p47;
	cvt.u32.u16 	%r598, %rs20;
	bfi.b32 	%r599, %r598, %r595, 8, 8;
	bfe.u32 	%r600, %r572, 16, 8;
	cvt.u16.u32 	%rs21, %r600;
	selp.b16 	%rs23, %rs21, %rs10, %p47;
	cvt.u32.u16 	%r602, %rs23;
	bfi.b32 	%r603, %r602, %r599, 16, 8;
	bfe.u32 	%r604, %r572, 24, 8;
	cvt.u16.u32 	%rs24, %r604;
	selp.b16 	%rs26, %rs24, %rs13, %p47;
	cvt.u32.u16 	%r606, %rs26;
	bfi.b32 	%r607, %r606, %r603, 24, 8;
	bfe.u32 	%r608, %r570, 0, 8;
	cvt.u16.u32 	%rs27, %r608;
	selp.b16 	%rs29, %rs27, %rs4, %p47;
	cvt.u32.u16 	%r610, %rs29;
	bfe.u32 	%r611, %r570, 8, 8;
	cvt.u16.u32 	%rs30, %r611;
	selp.b16 	%rs32, %rs30, %rs7, %p47;
	cvt.u32.u16 	%r613, %rs32;
	bfi.b32 	%r614, %r613, %r610, 8, 8;
	bfe.u32 	%r615, %r570, 16, 8;
	cvt.u16.u32 	%rs33, %r615;
	selp.b16 	%rs35, %rs33, %rs10, %p47;
	cvt.u32.u16 	%r617, %rs35;
	bfi.b32 	%r618, %r617, %r614, 16, 8;
	bfe.u32 	%r619, %r570, 24, 8;
	cvt.u16.u32 	%rs36, %r619;
	selp.b16 	%rs38, %rs36, %rs13, %p47;
	cvt.u32.u16 	%r621, %rs38;
	bfi.b32 	%r622, %r621, %r618, 24, 8;
	bfe.u32 	%r623, %r571, 0, 8;
	cvt.u16.u32 	%rs39, %r623;
	selp.b16 	%rs41, %rs39, %rs4, %p47;
	cvt.u32.u16 	%r625, %rs41;
	bfe.u32 	%r626, %r571, 8, 8;
	cvt.u16.u32 	%rs42, %r626;
	selp.b16 	%rs44, %rs42, %rs7, %p47;
	cvt.u32.u16 	%r628, %rs44;
	bfi.b32 	%r629, %r628, %r625, 8, 8;
	bfe.u32 	%r630, %r571, 16, 8;
	cvt.u16.u32 	%rs45, %r630;
	selp.b16 	%rs47, %rs45, %rs10, %p47;
	cvt.u32.u16 	%r632, %rs47;
	bfi.b32 	%r633, %r632, %r629, 16, 8;
	bfe.u32 	%r634, %r571, 24, 8;
	cvt.u16.u32 	%rs48, %r634;
	selp.b16 	%rs50, %rs48, %rs13, %p47;
	cvt.u32.u16 	%r636, %rs50;
	bfi.b32 	%r637, %r636, %r633, 24, 8;
	ld.shared.v4.u32 	{%r638, %r639, %r640, %r641}, [%r567];
	bfe.u32 	%r646, %r641, 0, 8;
	cvt.u16.u32 	%rs51, %r646;
	selp.b16 	%rs52, %rs51, %rs4, %p47;
	cvt.u32.u16 	%r647, %rs52;
	bfe.u32 	%r648, %r641, 8, 8;
	cvt.u16.u32 	%rs53, %r648;
	selp.b16 	%rs54, %rs53, %rs7, %p47;
	cvt.u32.u16 	%r649, %rs54;
	bfi.b32 	%r650, %r649, %r647, 8, 8;
	bfe.u32 	%r651, %r641, 16, 8;
	cvt.u16.u32 	%rs55, %r651;
	selp.b16 	%rs56, %rs55, %rs10, %p47;
	cvt.u32.u16 	%r652, %rs56;
	bfi.b32 	%r653, %r652, %r650, 16, 8;
	bfe.u32 	%r654, %r641, 24, 8;
	cvt.u16.u32 	%rs57, %r654;
	selp.b16 	%rs58, %rs57, %rs13, %p47;
	cvt.u32.u16 	%r655, %rs58;
	bfi.b32 	%r656, %r655, %r653, 24, 8;
	bfe.u32 	%r657, %r640, 0, 8;
	cvt.u16.u32 	%rs59, %r657;
	selp.b16 	%rs60, %rs59, %rs4, %p47;
	cvt.u32.u16 	%r658, %rs60;
	bfe.u32 	%r659, %r640, 8, 8;
	cvt.u16.u32 	%rs61, %r659;
	selp.b16 	%rs62, %rs61, %rs7, %p47;
	cvt.u32.u16 	%r660, %rs62;
	bfi.b32 	%r661, %r660, %r658, 8, 8;
	bfe.u32 	%r662, %r640, 16, 8;
	cvt.u16.u32 	%rs63, %r662;
	selp.b16 	%rs64, %rs63, %rs10, %p47;
	cvt.u32.u16 	%r663, %rs64;
	bfi.b32 	%r664, %r663, %r661, 16, 8;
	bfe.u32 	%r665, %r640, 24, 8;
	cvt.u16.u32 	%rs65, %r665;
	selp.b16 	%rs66, %rs65, %rs13, %p47;
	cvt.u32.u16 	%r666, %rs66;
	bfi.b32 	%r667, %r666, %r664, 24, 8;
	bfe.u32 	%r668, %r638, 0, 8;
	cvt.u16.u32 	%rs67, %r668;
	selp.b16 	%rs68, %rs67, %rs4, %p47;
	cvt.u32.u16 	%r669, %rs68;
	bfe.u32 	%r670, %r638, 8, 8;
	cvt.u16.u32 	%rs69, %r670;
	selp.b16 	%rs70, %rs69, %rs7, %p47;
	cvt.u32.u16 	%r671, %rs70;
	bfi.b32 	%r672, %r671, %r669, 8, 8;
	bfe.u32 	%r673, %r638, 16, 8;
	cvt.u16.u32 	%rs71, %r673;
	selp.b16 	%rs72, %rs71, %rs10, %p47;
	cvt.u32.u16 	%r674, %rs72;
	bfi.b32 	%r675, %r674, %r672, 16, 8;
	bfe.u32 	%r676, %r638, 24, 8;
	cvt.u16.u32 	%rs73, %r676;
	selp.b16 	%rs74, %rs73, %rs13, %p47;
	cvt.u32.u16 	%r677, %rs74;
	bfi.b32 	%r678, %r677, %r675, 24, 8;
	bfe.u32 	%r679, %r639, 0, 8;
	cvt.u16.u32 	%rs75, %r679;
	selp.b16 	%rs76, %rs75, %rs4, %p47;
	cvt.u32.u16 	%r680, %rs76;
	bfe.u32 	%r681, %r639, 8, 8;
	cvt.u16.u32 	%rs77, %r681;
	selp.b16 	%rs78, %rs77, %rs7, %p47;
	cvt.u32.u16 	%r682, %rs78;
	bfi.b32 	%r683, %r682, %r680, 8, 8;
	bfe.u32 	%r684, %r639, 16, 8;
	cvt.u16.u32 	%rs79, %r684;
	selp.b16 	%rs80, %rs79, %rs10, %p47;
	cvt.u32.u16 	%r685, %rs80;
	bfi.b32 	%r686, %r685, %r683, 16, 8;
	bfe.u32 	%r687, %r639, 24, 8;
	cvt.u16.u32 	%rs81, %r687;
	selp.b16 	%rs82, %rs81, %rs13, %p47;
	cvt.u32.u16 	%r688, %rs82;
	bfi.b32 	%r689, %r688, %r686, 24, 8;
	ld.shared.v4.u32 	{%r690, %r691, %r692, %r693}, [%r568];
	bfe.u32 	%r698, %r693, 0, 8;
	cvt.u16.u32 	%rs83, %r698;
	selp.b16 	%rs84, %rs83, %rs4, %p47;
	cvt.u32.u16 	%r699, %rs84;
	bfe.u32 	%r700, %r693, 8, 8;
	cvt.u16.u32 	%rs85, %r700;
	selp.b16 	%rs86, %rs85, %rs7, %p47;
	cvt.u32.u16 	%r701, %rs86;
	bfi.b32 	%r702, %r701, %r699, 8, 8;
	bfe.u32 	%r703, %r693, 16, 8;
	cvt.u16.u32 	%rs87, %r703;
	selp.b16 	%rs88, %rs87, %rs10, %p47;
	cvt.u32.u16 	%r704, %rs88;
	bfi.b32 	%r705, %r704, %r702, 16, 8;
	bfe.u32 	%r706, %r693, 24, 8;
	cvt.u16.u32 	%rs89, %r706;
	selp.b16 	%rs90, %rs89, %rs13, %p47;
	cvt.u32.u16 	%r707, %rs90;
	bfi.b32 	%r708, %r707, %r705, 24, 8;
	bfe.u32 	%r709, %r692, 0, 8;
	cvt.u16.u32 	%rs91, %r709;
	selp.b16 	%rs92, %rs91, %rs4, %p47;
	cvt.u32.u16 	%r710, %rs92;
	bfe.u32 	%r711, %r692, 8, 8;
	cvt.u16.u32 	%rs93, %r711;
	selp.b16 	%rs94, %rs93, %rs7, %p47;
	cvt.u32.u16 	%r712, %rs94;
	bfi.b32 	%r713, %r712, %r710, 8, 8;
	bfe.u32 	%r714, %r692, 16, 8;
	cvt.u16.u32 	%rs95, %r714;
	selp.b16 	%rs96, %rs95, %rs10, %p47;
	cvt.u32.u16 	%r715, %rs96;
	bfi.b32 	%r716, %r715, %r713, 16, 8;
	bfe.u32 	%r717, %r692, 24, 8;
	cvt.u16.u32 	%rs97, %r717;
	selp.b16 	%rs98, %rs97, %rs13, %p47;
	cvt.u32.u16 	%r718, %rs98;
	bfi.b32 	%r719, %r718, %r716, 24, 8;
	bfe.u32 	%r720, %r690, 0, 8;
	cvt.u16.u32 	%rs99, %r720;
	selp.b16 	%rs100, %rs99, %rs4, %p47;
	cvt.u32.u16 	%r721, %rs100;
	bfe.u32 	%r722, %r690, 8, 8;
	cvt.u16.u32 	%rs101, %r722;
	selp.b16 	%rs102, %rs101, %rs7, %p47;
	cvt.u32.u16 	%r723, %rs102;
	bfi.b32 	%r724, %r723, %r721, 8, 8;
	bfe.u32 	%r725, %r690, 16, 8;
	cvt.u16.u32 	%rs103, %r725;
	selp.b16 	%rs104, %rs103, %rs10, %p47;
	cvt.u32.u16 	%r726, %rs104;
	bfi.b32 	%r727, %r726, %r724, 16, 8;
	bfe.u32 	%r728, %r690, 24, 8;
	cvt.u16.u32 	%rs105, %r728;
	selp.b16 	%rs106, %rs105, %rs13, %p47;
	cvt.u32.u16 	%r729, %rs106;
	bfi.b32 	%r730, %r729, %r727, 24, 8;
	bfe.u32 	%r731, %r691, 0, 8;
	cvt.u16.u32 	%rs107, %r731;
	selp.b16 	%rs108, %rs107, %rs4, %p47;
	cvt.u32.u16 	%r732, %rs108;
	bfe.u32 	%r733, %r691, 8, 8;
	cvt.u16.u32 	%rs109, %r733;
	selp.b16 	%rs110, %rs109, %rs7, %p47;
	cvt.u32.u16 	%r734, %rs110;
	bfi.b32 	%r735, %r734, %r732, 8, 8;
	bfe.u32 	%r736, %r691, 16, 8;
	cvt.u16.u32 	%rs111, %r736;
	selp.b16 	%rs112, %rs111, %rs10, %p47;
	cvt.u32.u16 	%r737, %rs112;
	bfi.b32 	%r738, %r737, %r735, 16, 8;
	bfe.u32 	%r739, %r691, 24, 8;
	cvt.u16.u32 	%rs113, %r739;
	selp.b16 	%rs114, %rs113, %rs13, %p47;
	cvt.u32.u16 	%r740, %rs114;
	bfi.b32 	%r741, %r740, %r738, 24, 8;
	ld.shared.v4.u32 	{%r742, %r743, %r744, %r745}, [%r569];
	bfe.u32 	%r750, %r745, 0, 8;
	cvt.u16.u32 	%rs115, %r750;
	selp.b16 	%rs116, %rs115, %rs4, %p47;
	cvt.u32.u16 	%r751, %rs116;
	bfe.u32 	%r752, %r745, 8, 8;
	cvt.u16.u32 	%rs117, %r752;
	selp.b16 	%rs118, %rs117, %rs7, %p47;
	cvt.u32.u16 	%r753, %rs118;
	bfi.b32 	%r754, %r753, %r751, 8, 8;
	bfe.u32 	%r755, %r745, 16, 8;
	cvt.u16.u32 	%rs119, %r755;
	selp.b16 	%rs120, %rs119, %rs10, %p47;
	cvt.u32.u16 	%r756, %rs120;
	bfi.b32 	%r757, %r756, %r754, 16, 8;
	bfe.u32 	%r758, %r745, 24, 8;
	cvt.u16.u32 	%rs121, %r758;
	selp.b16 	%rs122, %rs121, %rs13, %p47;
	cvt.u32.u16 	%r759, %rs122;
	bfi.b32 	%r760, %r759, %r757, 24, 8;
	bfe.u32 	%r761, %r744, 0, 8;
	cvt.u16.u32 	%rs123, %r761;
	selp.b16 	%rs124, %rs123, %rs4, %p47;
	cvt.u32.u16 	%r762, %rs124;
	bfe.u32 	%r763, %r744, 8, 8;
	cvt.u16.u32 	%rs125, %r763;
	selp.b16 	%rs126, %rs125, %rs7, %p47;
	cvt.u32.u16 	%r764, %rs126;
	bfi.b32 	%r765, %r764, %r762, 8, 8;
	bfe.u32 	%r766, %r744, 16, 8;
	cvt.u16.u32 	%rs127, %r766;
	selp.b16 	%rs128, %rs127, %rs10, %p47;
	cvt.u32.u16 	%r767, %rs128;
	bfi.b32 	%r768, %r767, %r765, 16, 8;
	bfe.u32 	%r769, %r744, 24, 8;
	cvt.u16.u32 	%rs129, %r769;
	selp.b16 	%rs130, %rs129, %rs13, %p47;
	cvt.u32.u16 	%r770, %rs130;
	bfi.b32 	%r771, %r770, %r768, 24, 8;
	bfe.u32 	%r772, %r742, 0, 8;
	cvt.u16.u32 	%rs131, %r772;
	selp.b16 	%rs132, %rs131, %rs4, %p47;
	cvt.u32.u16 	%r773, %rs132;
	bfe.u32 	%r774, %r742, 8, 8;
	cvt.u16.u32 	%rs133, %r774;
	selp.b16 	%rs134, %rs133, %rs7, %p47;
	cvt.u32.u16 	%r775, %rs134;
	bfi.b32 	%r776, %r775, %r773, 8, 8;
	bfe.u32 	%r777, %r742, 16, 8;
	cvt.u16.u32 	%rs135, %r777;
	selp.b16 	%rs136, %rs135, %rs10, %p47;
	cvt.u32.u16 	%r778, %rs136;
	bfi.b32 	%r779, %r778, %r776, 16, 8;
	bfe.u32 	%r780, %r742, 24, 8;
	cvt.u16.u32 	%rs137, %r780;
	selp.b16 	%rs138, %rs137, %rs13, %p47;
	cvt.u32.u16 	%r781, %rs138;
	bfi.b32 	%r782, %r781, %r779, 24, 8;
	bfe.u32 	%r783, %r743, 0, 8;
	cvt.u16.u32 	%rs139, %r783;
	selp.b16 	%rs140, %rs139, %rs4, %p47;
	cvt.u32.u16 	%r784, %rs140;
	bfe.u32 	%r785, %r743, 8, 8;
	cvt.u16.u32 	%rs141, %r785;
	selp.b16 	%rs142, %rs141, %rs7, %p47;
	cvt.u32.u16 	%r786, %rs142;
	bfi.b32 	%r787, %r786, %r784, 8, 8;
	bfe.u32 	%r788, %r743, 16, 8;
	cvt.u16.u32 	%rs143, %r788;
	selp.b16 	%rs144, %rs143, %rs10, %p47;
	cvt.u32.u16 	%r789, %rs144;
	bfi.b32 	%r790, %r789, %r787, 16, 8;
	bfe.u32 	%r791, %r743, 24, 8;
	cvt.u16.u32 	%rs145, %r791;
	selp.b16 	%rs146, %rs145, %rs13, %p47;
	cvt.u32.u16 	%r792, %rs146;
	bfi.b32 	%r793, %r792, %r790, 24, 8;
	.loc	1 333 35
	st.shared.u32 	[%r21+4], %r637;
	st.shared.u32 	[%r21], %r622;
	st.shared.u32 	[%r21+8], %r607;
	st.shared.u32 	[%r21+12], %r592;
	st.shared.u32 	[%r22+4], %r689;
	st.shared.u32 	[%r22], %r678;
	st.shared.u32 	[%r22+8], %r667;
	st.shared.u32 	[%r22+12], %r656;
	st.shared.u32 	[%r23+4], %r741;
	st.shared.u32 	[%r23], %r730;
	st.shared.u32 	[%r23+8], %r719;
	st.shared.u32 	[%r23+12], %r708;
	st.shared.u32 	[%r24+4], %r793;
	st.shared.u32 	[%r24], %r782;
	st.shared.u32 	[%r24+8], %r771;
	st.shared.u32 	[%r24+12], %r760;
	.loc	1 331 20
	add.s32 	%r794, %r897, %r15;
	ld.shared.v4.u32 	{%r795, %r796, %r797, %r798}, [%r794];
	bfe.u32 	%r803, %r795, 0, 8;
	cvt.u16.u32 	%rs147, %r803;
	selp.b16 	%rs148, %rs147, %rs4, %p47;
	cvt.u32.u16 	%r804, %rs148;
	bfe.u32 	%r805, %r795, 8, 8;
	cvt.u16.u32 	%rs149, %r805;
	selp.b16 	%rs150, %rs149, %rs7, %p47;
	cvt.u32.u16 	%r806, %rs150;
	bfi.b32 	%r807, %r806, %r804, 8, 8;
	bfe.u32 	%r808, %r795, 16, 8;
	cvt.u16.u32 	%rs151, %r808;
	selp.b16 	%rs152, %rs151, %rs10, %p47;
	cvt.u32.u16 	%r809, %rs152;
	bfi.b32 	%r810, %r809, %r807, 16, 8;
	bfe.u32 	%r811, %r795, 24, 8;
	cvt.u16.u32 	%rs153, %r811;
	selp.b16 	%rs154, %rs153, %rs13, %p47;
	cvt.u32.u16 	%r812, %rs154;
	bfi.b32 	%r813, %r812, %r810, 24, 8;
	bfe.u32 	%r814, %r796, 0, 8;
	cvt.u16.u32 	%rs155, %r814;
	selp.b16 	%rs156, %rs155, %rs4, %p47;
	cvt.u32.u16 	%r815, %rs156;
	bfe.u32 	%r816, %r796, 8, 8;
	cvt.u16.u32 	%rs157, %r816;
	selp.b16 	%rs158, %rs157, %rs7, %p47;
	cvt.u32.u16 	%r817, %rs158;
	bfi.b32 	%r818, %r817, %r815, 8, 8;
	bfe.u32 	%r819, %r796, 16, 8;
	cvt.u16.u32 	%rs159, %r819;
	selp.b16 	%rs160, %rs159, %rs10, %p47;
	cvt.u32.u16 	%r820, %rs160;
	bfi.b32 	%r821, %r820, %r818, 16, 8;
	bfe.u32 	%r822, %r796, 24, 8;
	cvt.u16.u32 	%rs161, %r822;
	selp.b16 	%rs162, %rs161, %rs13, %p47;
	cvt.u32.u16 	%r823, %rs162;
	bfi.b32 	%r824, %r823, %r821, 24, 8;
	bfe.u32 	%r825, %r797, 0, 8;
	cvt.u16.u32 	%rs163, %r825;
	selp.b16 	%rs164, %rs163, %rs4, %p47;
	cvt.u32.u16 	%r826, %rs164;
	bfe.u32 	%r827, %r797, 8, 8;
	cvt.u16.u32 	%rs165, %r827;
	selp.b16 	%rs166, %rs165, %rs7, %p47;
	cvt.u32.u16 	%r828, %rs166;
	bfi.b32 	%r829, %r828, %r826, 8, 8;
	bfe.u32 	%r830, %r797, 16, 8;
	cvt.u16.u32 	%rs167, %r830;
	selp.b16 	%rs168, %rs167, %rs10, %p47;
	cvt.u32.u16 	%r831, %rs168;
	bfi.b32 	%r832, %r831, %r829, 16, 8;
	bfe.u32 	%r833, %r797, 24, 8;
	cvt.u16.u32 	%rs169, %r833;
	selp.b16 	%rs170, %rs169, %rs13, %p47;
	cvt.u32.u16 	%r834, %rs170;
	bfi.b32 	%r835, %r834, %r832, 24, 8;
	bfe.u32 	%r836, %r798, 0, 8;
	cvt.u16.u32 	%rs171, %r836;
	selp.b16 	%rs172, %rs171, %rs4, %p47;
	cvt.u32.u16 	%r837, %rs172;
	bfe.u32 	%r838, %r798, 8, 8;
	cvt.u16.u32 	%rs173, %r838;
	selp.b16 	%rs174, %rs173, %rs7, %p47;
	cvt.u32.u16 	%r839, %rs174;
	bfi.b32 	%r840, %r839, %r837, 8, 8;
	bfe.u32 	%r841, %r798, 16, 8;
	cvt.u16.u32 	%rs175, %r841;
	selp.b16 	%rs176, %rs175, %rs10, %p47;
	cvt.u32.u16 	%r842, %rs176;
	bfi.b32 	%r843, %r842, %r840, 16, 8;
	bfe.u32 	%r844, %r798, 24, 8;
	cvt.u16.u32 	%rs177, %r844;
	selp.b16 	%rs178, %rs177, %rs13, %p47;
	cvt.u32.u16 	%r845, %rs178;
	bfi.b32 	%r846, %r845, %r843, 24, 8;
	.loc	1 333 35
	st.shared.u32 	[%r25+12], %r846;
	st.shared.u32 	[%r25+8], %r835;
	st.shared.u32 	[%r25+4], %r824;
	st.shared.u32 	[%r25], %r813;
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r228, %r229, %r230, %r231 }, [ %r232 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r233, %r234, %r235, %r236 }, [ %r237 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r238, %r239, %r240, %r241 }, [ %r242 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r243, %r244, %r245, %r246 }, [ %r247 + 0 ];
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r364, 0, %r228, 0x5140; 
	prmt.b32 %r366, 0, %r228, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r412, 0, %r230, 0x5140; 
	prmt.b32 %r414, 0, %r230, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r365, 0, %r229, 0x5140; 
	prmt.b32 %r367, 0, %r229, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r413, 0, %r231, 0x5140; 
	prmt.b32 %r415, 0, %r231, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r460, 0, %r233, 0x5140; 
	prmt.b32 %r462, 0, %r233, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r508, 0, %r235, 0x5140; 
	prmt.b32 %r510, 0, %r235, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r461, 0, %r234, 0x5140; 
	prmt.b32 %r463, 0, %r234, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r509, 0, %r236, 0x5140; 
	prmt.b32 %r511, 0, %r236, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r388, 0, %r238, 0x5140; 
	prmt.b32 %r390, 0, %r238, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r436, 0, %r240, 0x5140; 
	prmt.b32 %r438, 0, %r240, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r389, 0, %r239, 0x5140; 
	prmt.b32 %r391, 0, %r239, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r437, 0, %r241, 0x5140; 
	prmt.b32 %r439, 0, %r241, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r484, 0, %r243, 0x5140; 
	prmt.b32 %r486, 0, %r243, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r532, 0, %r245, 0x5140; 
	prmt.b32 %r534, 0, %r245, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r485, 0, %r244, 0x5140; 
	prmt.b32 %r487, 0, %r244, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r533, 0, %r246, 0x5140; 
	prmt.b32 %r535, 0, %r246, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r296, %r297, %r298, %r299 }, [ %r300 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r301, %r302, %r303, %r304 }, [ %r305 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r306, %r307, %r308, %r309 }, [ %r310 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r311, %r312, %r313, %r314 }, [ %r315 + 0 ];
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r368, 0, %r296, 0x5140; 
	prmt.b32 %r369, 0, %r296, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r416, 0, %r297, 0x5140; 
	prmt.b32 %r417, 0, %r297, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r374, 0, %r298, 0x5140; 
	prmt.b32 %r375, 0, %r298, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r422, 0, %r299, 0x5140; 
	prmt.b32 %r423, 0, %r299, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r464, 0, %r301, 0x5140; 
	prmt.b32 %r465, 0, %r301, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r512, 0, %r302, 0x5140; 
	prmt.b32 %r513, 0, %r302, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r470, 0, %r303, 0x5140; 
	prmt.b32 %r471, 0, %r303, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r518, 0, %r304, 0x5140; 
	prmt.b32 %r519, 0, %r304, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r380, 0, %r306, 0x5140; 
	prmt.b32 %r381, 0, %r306, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r428, 0, %r307, 0x5140; 
	prmt.b32 %r429, 0, %r307, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r386, 0, %r308, 0x5140; 
	prmt.b32 %r387, 0, %r308, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r434, 0, %r309, 0x5140; 
	prmt.b32 %r435, 0, %r309, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r476, 0, %r311, 0x5140; 
	prmt.b32 %r477, 0, %r311, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r524, 0, %r312, 0x5140; 
	prmt.b32 %r525, 0, %r312, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r482, 0, %r313, 0x5140; 
	prmt.b32 %r483, 0, %r313, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r530, 0, %r314, 0x5140; 
	prmt.b32 %r531, 0, %r314, 0x7362; 
	}
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f322, %f323, %f324, %f325 }, { %r364, %r365, %r366, %r367 }, { %r368, %r369 }, { %f322, %f323, %f324, %f325 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f326, %f327, %f328, %f329 }, { %r364, %r365, %r366, %r367 }, { %r374, %r375 }, { %f326, %f327, %f328, %f329 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f330, %f331, %f332, %f333 }, { %r364, %r365, %r366, %r367 }, { %r380, %r381 }, { %f330, %f331, %f332, %f333 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f334, %f335, %f336, %f337 }, { %r364, %r365, %r366, %r367 }, { %r386, %r387 }, { %f334, %f335, %f336, %f337 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f338, %f339, %f340, %f341 }, { %r388, %r389, %r390, %r391 }, { %r368, %r369 }, { %f338, %f339, %f340, %f341 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f342, %f343, %f344, %f345 }, { %r388, %r389, %r390, %r391 }, { %r374, %r375 }, { %f342, %f343, %f344, %f345 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f346, %f347, %f348, %f349 }, { %r388, %r389, %r390, %r391 }, { %r380, %r381 }, { %f346, %f347, %f348, %f349 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f350, %f351, %f352, %f353 }, { %r388, %r389, %r390, %r391 }, { %r386, %r387 }, { %f350, %f351, %f352, %f353 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f322, %f323, %f324, %f325 }, { %r412, %r413, %r414, %r415 }, { %r416, %r417 }, { %f322, %f323, %f324, %f325 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f326, %f327, %f328, %f329 }, { %r412, %r413, %r414, %r415 }, { %r422, %r423 }, { %f326, %f327, %f328, %f329 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f330, %f331, %f332, %f333 }, { %r412, %r413, %r414, %r415 }, { %r428, %r429 }, { %f330, %f331, %f332, %f333 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f334, %f335, %f336, %f337 }, { %r412, %r413, %r414, %r415 }, { %r434, %r435 }, { %f334, %f335, %f336, %f337 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f338, %f339, %f340, %f341 }, { %r436, %r437, %r438, %r439 }, { %r416, %r417 }, { %f338, %f339, %f340, %f341 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f342, %f343, %f344, %f345 }, { %r436, %r437, %r438, %r439 }, { %r422, %r423 }, { %f342, %f343, %f344, %f345 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f346, %f347, %f348, %f349 }, { %r436, %r437, %r438, %r439 }, { %r428, %r429 }, { %f346, %f347, %f348, %f349 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f350, %f351, %f352, %f353 }, { %r436, %r437, %r438, %r439 }, { %r434, %r435 }, { %f350, %f351, %f352, %f353 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f322, %f323, %f324, %f325 }, { %r460, %r461, %r462, %r463 }, { %r464, %r465 }, { %f322, %f323, %f324, %f325 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f326, %f327, %f328, %f329 }, { %r460, %r461, %r462, %r463 }, { %r470, %r471 }, { %f326, %f327, %f328, %f329 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f330, %f331, %f332, %f333 }, { %r460, %r461, %r462, %r463 }, { %r476, %r477 }, { %f330, %f331, %f332, %f333 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f334, %f335, %f336, %f337 }, { %r460, %r461, %r462, %r463 }, { %r482, %r483 }, { %f334, %f335, %f336, %f337 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f338, %f339, %f340, %f341 }, { %r484, %r485, %r486, %r487 }, { %r464, %r465 }, { %f338, %f339, %f340, %f341 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f342, %f343, %f344, %f345 }, { %r484, %r485, %r486, %r487 }, { %r470, %r471 }, { %f342, %f343, %f344, %f345 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f346, %f347, %f348, %f349 }, { %r484, %r485, %r486, %r487 }, { %r476, %r477 }, { %f346, %f347, %f348, %f349 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f350, %f351, %f352, %f353 }, { %r484, %r485, %r486, %r487 }, { %r482, %r483 }, { %f350, %f351, %f352, %f353 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f322, %f323, %f324, %f325 }, { %r508, %r509, %r510, %r511 }, { %r512, %r513 }, { %f322, %f323, %f324, %f325 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f326, %f327, %f328, %f329 }, { %r508, %r509, %r510, %r511 }, { %r518, %r519 }, { %f326, %f327, %f328, %f329 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f330, %f331, %f332, %f333 }, { %r508, %r509, %r510, %r511 }, { %r524, %r525 }, { %f330, %f331, %f332, %f333 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f334, %f335, %f336, %f337 }, { %r508, %r509, %r510, %r511 }, { %r530, %r531 }, { %f334, %f335, %f336, %f337 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f338, %f339, %f340, %f341 }, { %r532, %r533, %r534, %r535 }, { %r512, %r513 }, { %f338, %f339, %f340, %f341 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f342, %f343, %f344, %f345 }, { %r532, %r533, %r534, %r535 }, { %r518, %r519 }, { %f342, %f343, %f344, %f345 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f346, %f347, %f348, %f349 }, { %r532, %r533, %r534, %r535 }, { %r524, %r525 }, { %f346, %f347, %f348, %f349 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f350, %f351, %f352, %f353 }, { %r532, %r533, %r534, %r535 }, { %r530, %r531 }, { %f350, %f351, %f352, %f353 };
	// end inline asm
	.loc	1 327 22
	add.s32 	%r847, %r900, 1;
	setp.lt.s32 	%p33, %r847, 3;
	selp.b32 	%r900, %r847, 0, %p33;
	.loc	1 330 51
	setp.lt.s32 	%p45, %r9, %r896;
	.loc	1 330 20
	shl.b32 	%r848, %r900, 13;
	add.s32 	%r850, %r185, %r848;
	add.s32 	%r556, %r850, %r15;
	add.s32 	%r558, %r850, %r16;
	add.s32 	%r560, %r850, %r17;
	add.s32 	%r562, %r850, %r18;
	selp.b32 	%r851, 16, 0, %p45;
	selp.b32 	%r559, %r851, 0, %p32;
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r556 + 0 ], [ %rd62 + 0 ], 0x10, %r559;
	// end inline asm
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r558 + 0 ], [ %rd63 + 0 ], 0x10, %r559;
	// end inline asm
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r560 + 0 ], [ %rd64 + 0 ], 0x10, %r559;
	// end inline asm
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r562 + 0 ], [ %rd65 + 0 ], 0x10, %r559;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 331 20
	shl.b32 	%r852, %r900, 11;
	add.s32 	%r564, %r96, %r852;
	// begin inline asm
	@%p8 cp.async.cg.shared.global [ %r564 + 0 ], [ %rd66 + 0 ], 0x10, %r559;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 327 22
	add.s32 	%r853, %r899, 1;
	setp.lt.s32 	%p34, %r853, 3;
	selp.b32 	%r899, %r853, 0, %p34;
	.loc	1 330 20
	shl.b32 	%r854, %r899, 13;
	add.s32 	%r898, %r185, %r854;
	// begin inline asm
	cp.async.wait_group 0x4;
	// end inline asm
	bar.sync 	0;
	.loc	1 331 20
	shl.b32 	%r855, %r899, 11;
	add.s32 	%r856, %r185, %r855;
	add.s32 	%r897, %r856, 24576;
	.loc	1 327 22
	add.s32 	%r901, %r901, 1;
	add.s64 	%rd66, %rd66, 64;
	add.s64 	%rd65, %rd65, 64;
	add.s64 	%rd64, %rd64, 64;
	add.s64 	%rd63, %rd63, 64;
	add.s64 	%rd62, %rd62, 64;
	add.s32 	%r896, %r896, -64;
	setp.lt.s32 	%p35, %r901, %r10;
	mov.pred 	%p47, %p5;
	@%p35 bra 	$L__BB0_2;
	.loc	1 341 23
	cvt.rn.f16.f32 	%rs179, %f353;
	cvt.rn.f16.f32 	%rs180, %f352;
	mov.b32 	%r917, {%rs180, %rs179};
	cvt.rn.f16.f32 	%rs181, %f351;
	cvt.rn.f16.f32 	%rs182, %f350;
	mov.b32 	%r916, {%rs182, %rs181};
	cvt.rn.f16.f32 	%rs183, %f349;
	cvt.rn.f16.f32 	%rs184, %f348;
	mov.b32 	%r915, {%rs184, %rs183};
	cvt.rn.f16.f32 	%rs185, %f347;
	cvt.rn.f16.f32 	%rs186, %f346;
	mov.b32 	%r914, {%rs186, %rs185};
	cvt.rn.f16.f32 	%rs187, %f345;
	cvt.rn.f16.f32 	%rs188, %f344;
	mov.b32 	%r913, {%rs188, %rs187};
	cvt.rn.f16.f32 	%rs189, %f343;
	cvt.rn.f16.f32 	%rs190, %f342;
	mov.b32 	%r912, {%rs190, %rs189};
	cvt.rn.f16.f32 	%rs191, %f341;
	cvt.rn.f16.f32 	%rs192, %f340;
	mov.b32 	%r911, {%rs192, %rs191};
	cvt.rn.f16.f32 	%rs193, %f339;
	cvt.rn.f16.f32 	%rs194, %f338;
	mov.b32 	%r910, {%rs194, %rs193};
	cvt.rn.f16.f32 	%rs195, %f337;
	cvt.rn.f16.f32 	%rs196, %f336;
	mov.b32 	%r909, {%rs196, %rs195};
	cvt.rn.f16.f32 	%rs197, %f335;
	cvt.rn.f16.f32 	%rs198, %f334;
	mov.b32 	%r908, {%rs198, %rs197};
	cvt.rn.f16.f32 	%rs199, %f333;
	cvt.rn.f16.f32 	%rs200, %f332;
	mov.b32 	%r907, {%rs200, %rs199};
	cvt.rn.f16.f32 	%rs201, %f331;
	cvt.rn.f16.f32 	%rs202, %f330;
	mov.b32 	%r906, {%rs202, %rs201};
	cvt.rn.f16.f32 	%rs203, %f329;
	cvt.rn.f16.f32 	%rs204, %f328;
	mov.b32 	%r905, {%rs204, %rs203};
	cvt.rn.f16.f32 	%rs205, %f327;
	cvt.rn.f16.f32 	%rs206, %f326;
	mov.b32 	%r904, {%rs206, %rs205};
	cvt.rn.f16.f32 	%rs207, %f325;
	cvt.rn.f16.f32 	%rs208, %f324;
	mov.b32 	%r903, {%rs208, %rs207};
	cvt.rn.f16.f32 	%rs209, %f323;
	cvt.rn.f16.f32 	%rs210, %f322;
	mov.b32 	%r902, {%rs210, %rs209};
$L__BB0_4:
	.loc	1 314 51
	shl.b32 	%r873, %r1, 3;
	and.b32  	%r874, %r873, 24;
	.loc	1 314 38
	or.b32  	%r875, %r8, %r874;
	.loc	1 327 22
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 347 33
	mul.lo.s32 	%r876, %r4, %r82;
	mul.lo.s32 	%r877, %r5, %r82;
	mul.lo.s32 	%r878, %r6, %r82;
	mul.lo.s32 	%r879, %r7, %r82;
	.loc	1 347 21
	mul.wide.s32 	%rd53, %r876, 2;
	add.s64 	%rd54, %rd23, %rd53;
	mul.wide.s32 	%rd55, %r877, 2;
	add.s64 	%rd56, %rd23, %rd55;
	mul.wide.s32 	%rd57, %r878, 2;
	add.s64 	%rd58, %rd23, %rd57;
	mul.wide.s32 	%rd59, %r879, 2;
	add.s64 	%rd60, %rd23, %rd59;
	.loc	1 347 52
	mul.wide.s32 	%rd61, %r875, 2;
	add.s64 	%rd49, %rd54, %rd61;
	add.s64 	%rd50, %rd56, %rd61;
	add.s64 	%rd51, %rd58, %rd61;
	add.s64 	%rd52, %rd60, %rd61;
	.loc	1 348 33
	setp.lt.s32 	%p40, %r4, %r79;
	setp.lt.s32 	%p41, %r5, %r79;
	setp.lt.s32 	%p42, %r6, %r79;
	setp.lt.s32 	%p43, %r7, %r79;
	.loc	1 348 58
	setp.lt.s32 	%p44, %r875, %r80;
	.loc	1 348 39
	and.pred  	%p36, %p40, %p44;
	and.pred  	%p37, %p41, %p44;
	and.pred  	%p38, %p42, %p44;
	and.pred  	%p39, %p43, %p44;
	.loc	1 349 21
	and.b32  	%r880, %r3, 3;
	shr.u32 	%r881, %r2, 2;
	and.b32  	%r882, %r1, 3;
	shl.b32 	%r883, %r880, 4;
	or.b32  	%r884, %r883, %r881;
	mul.lo.s32 	%r885, %r884, 80;
	shl.b32 	%r886, %r882, 2;
	or.b32  	%r887, %r886, %r885;
	add.s32 	%r889, %r185, %r887;
	st.shared.b32 	[%r889], %r902;
	st.shared.b32 	[%r889+640], %r903;
	st.shared.b32 	[%r889+16], %r904;
	st.shared.b32 	[%r889+656], %r905;
	st.shared.b32 	[%r889+32], %r906;
	st.shared.b32 	[%r889+672], %r907;
	st.shared.b32 	[%r889+48], %r908;
	st.shared.b32 	[%r889+688], %r909;
	bar.sync 	0;
	shl.b32 	%r890, %r880, 3;
	or.b32  	%r891, %r890, %r881;
	shl.b32 	%r892, %r882, 3;
	mad.lo.s32 	%r893, %r891, 40, %r892;
	shl.b32 	%r894, %r893, 1;
	add.s32 	%r895, %r185, %r894;
	ld.shared.v4.u32 	{%r857, %r858, %r859, %r860}, [%r895];
	ld.shared.v4.u32 	{%r861, %r862, %r863, %r864}, [%r895+2560];
	bar.sync 	0;
	st.shared.b32 	[%r889], %r910;
	st.shared.b32 	[%r889+640], %r911;
	st.shared.b32 	[%r889+16], %r912;
	st.shared.b32 	[%r889+656], %r913;
	st.shared.b32 	[%r889+32], %r914;
	st.shared.b32 	[%r889+672], %r915;
	st.shared.b32 	[%r889+48], %r916;
	st.shared.b32 	[%r889+688], %r917;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r865, %r866, %r867, %r868}, [%r895];
	ld.shared.v4.u32 	{%r869, %r870, %r871, %r872}, [%r895+2560];
	// begin inline asm
	@%p36 st.global.v4.b32 [ %rd49 + 0 ], { %r857, %r858, %r859, %r860 };
	// end inline asm
	// begin inline asm
	@%p37 st.global.v4.b32 [ %rd50 + 0 ], { %r861, %r862, %r863, %r864 };
	// end inline asm
	// begin inline asm
	@%p38 st.global.v4.b32 [ %rd51 + 0 ], { %r865, %r866, %r867, %r868 };
	// end inline asm
	// begin inline asm
	@%p39 st.global.v4.b32 [ %rd52 + 0 ], { %r869, %r870, %r871, %r872 };
	// end inline asm
	.loc	1 349 4
	ret;
$L__tmp6:
$L__func_end0:

}
	.file	1 "/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/02.matrix-multiplication/matrix_multiplication.py"
	.file	2 "/data_ssd1/zjy_home/frameworks/cuda/triton/python/triton/language/standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 5
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 265
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 109
.b8 97
.b8 116
.b8 114
.b8 105
.b8 120
.b8 95
.b8 109
.b8 117
.b8 108
.b8 116
.b8 105
.b8 112
.b8 108
.b8 105
.b8 99
.b8 97
.b8 116
.b8 105
.b8 111
.b8 110
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 100
.b8 97
.b8 116
.b8 97
.b8 95
.b8 115
.b8 115
.b8 100
.b8 49
.b8 47
.b8 122
.b8 106
.b8 121
.b8 95
.b8 104
.b8 111
.b8 109
.b8 101
.b8 47
.b8 109
.b8 121
.b8 95
.b8 99
.b8 111
.b8 100
.b8 101
.b8 47
.b8 77
.b8 76
.b8 67
.b8 45
.b8 76
.b8 101
.b8 97
.b8 114
.b8 110
.b8 105
.b8 110
.b8 103
.b8 47
.b8 84
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 47
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 45
.b8 101
.b8 120
.b8 97
.b8 109
.b8 112
.b8 108
.b8 101
.b8 115
.b8 47
.b8 48
.b8 50
.b8 46
.b8 109
.b8 97
.b8 116
.b8 114
.b8 105
.b8 120
.b8 45
.b8 109
.b8 117
.b8 108
.b8 116
.b8 105
.b8 112
.b8 108
.b8 105
.b8 99
.b8 97
.b8 116
.b8 105
.b8 111
.b8 110
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 109
.b8 97
.b8 116
.b8 109
.b8 117
.b8 108
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 155
.b8 4
.b32 155
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 37
.b8 1
.b8 27
.b8 4
.b32 155
.b64 $L__tmp2
.b64 $L__tmp3
.b8 1
.b8 38
.b8 1
.b8 27
.b8 4
.b32 155
.b64 $L__tmp4
.b64 $L__tmp5
.b8 1
.b8 71
.b8 1
.b8 33
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}

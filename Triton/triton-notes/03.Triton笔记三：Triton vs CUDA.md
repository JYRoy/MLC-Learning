# Triton vs CUDA

- [Triton vs CUDA](#triton-vs-cuda)
  - [Overview](#overview)
  - [Triton vs CUDA Programming Model](#triton-vs-cuda-programming-model)
  - [CUDA Elmentwise Add](#cuda-elmentwise-add)
    - [Cache hit rate](#cache-hit-rate)
  - [References](#references)

## Overview

前文中已经通过反汇编ptx简单看过Triton和CUDA C的对应关系，继续做些深入的对比分析。

## Triton vs CUDA Programming Model

从Triton官方文档中也可以看的出来Triton的分块设计思路

![Triton vs CUDA Programming Model.png](../.images/Triton%20vs%20CUDA%20Programming%20Model.png)

Cuda中是按元素计算输出，规划到线程级别。

Triton中是分块计算输出，在固定的M、N方向上，按照K方向滑动，利用输入的tiles计算输出的一个block。

## CUDA Elmentwise Add

```c++
#include  "cuda_runtime.h"   // import cuda runtime to use cuda functions

// 定义 kernel function
__global__ void matrixAdd(int* A, int* B, int* C, int width, int height) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < height && col < width) {
        C[row * width + col] = A[row * width + col] + B[row * width + col];
    }
}

// 调用 kernel function
int main() {
    // 定义 A B 的大小
    int width = 4;
    int height = 5;

    // 在CPU上初始化内存,结果 A B C 和 Triton 一样里面一样也是 pointers 
    int* A, * B, * C;
    A = (int*)malloc(width * height * sizeof(int));
    B = (int*)malloc(width * height * sizeof(int));
    C = (int*)malloc(width * height * sizeof(int));

    // 给 A B 填上值
    for (int i = 0; i < height; i++) {
        for (int j = 0; j < width; j++) {
            A[i * width + j] = i;
            B[i * width + j] = j;
        }
    }

    // 在 GPU 上面对 A B C 初始化内存, 所以用到了 cudaMalloc 函数 (来自 cuda_runtime.h)
    int* d_A, * d_B, * d_C;
    cudaMalloc((void**)&d_A, width * width * sizeof(int));
    cudaMalloc((void**)&d_B, width * width * sizeof(int));
    cudaMalloc((void**)&d_C, width * width * sizeof(int));

    // 对于 A B, 把他们的值从 CPU 挪到 GPU. 这其实对应于 pytorch 里面的 .to("cuda")
    cudaMemcpy(d_A, A, width * width * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, B, width * width * sizeof(int), cudaMemcpyHostToDevice);

    // 调用 kernel function, 并确定 "循环" 多少次, 和前面 Triton 里面的 grid 相似但是不同
    dim3 block(height, width, 1);
    dim3 grid(1, 1, 1);
    matrixAdd <<<grid, block>>> (d_A, d_B, d_C, width, height);

    // 因为涉及到 CPU 和 GPU 的同时使用 (异构计算), 所以要 sync 一下
    cudaDeviceSynchronize();

    // 把计算结构从 GPU 拷贝回 CPU
    cudaMemcpy(C, d_C, width * width * sizeof(int), cudaMemcpyDeviceToHost);

    // 释放CPU和GPU内存
    free(A); free(B); free(C);
    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);
}
```

Triton和CUDA首先最大的区别就是在block和grid上面，它们之间的概念不是完全对应的

- triton block：kernel function要处理的一个数据块，triton中循环执行次数就是block的个数
- cuda block：循环级别，由thread构成，cuda中kernel处理的不是数据块，而是一个一个的scalar，因此cuda的循环次数是scalar个数

众所周知，cuda中一共有三个循环级别：

- thread：最小的单位
- block：多个thread构成一个block，一个block的thread会调度到一个sm上，它们共享L1 Cache/Shared Memory
- grid：多个block构成一个grid

triton中可以认为只有两个级别：

- program：最小单位，一个program对应一个输出中的block
- grid：多个program构成一个grid

所以，可以认为grid是更加的，一个program对应一个cuda block，至于cuda block中的thread怎么调度，归编译器管。

CUDA的一些基础知识：

`dim3 block(height, width, 1);`：意味着定义每个block由height * width * 1个thread构成
`dim3 grid(1, 1, 1);`：grid由1 * 1 * 1个block构成

进入kernel内部，计算得到threadIdx.x和threadIdx.y在整个grid中的位置：

```c
int row = blockIdx.y * blockDim.y + threadIdx.y;
int col = blockIdx.x * blockDim.x + threadIdx.x;
```

- threadIdx.x, threadIdx.y, threadIdx.z：分别代表当前 thread 在其所在的 thread block 的 xyz 坐标
- blockIdx.x, blockIdx.y, blockIdx.z：分别代表当前 thread block 在其所在的 thread grid 的 xyz 坐标
- blockDim.x, blockDim.y, blockDim.z：分别代表当前 thread block 在 xyz 维度分别有多少个 thread
- gridDim.x, gridDim.y, gridDim.z：分别代表当前 thread grid 在 xyz 维度分别有多少个 thread block

### Cache hit rate

这个cuda程序中其实还隐含了如何去提升cache hit rate的方法

```c
dim3 block(height, width, 1);
dim3 grid(1, 1, 1);
matrixAdd <<<grid, block>>> (d_A, d_B, d_C, width, height);
```

之所以要用一个block来实现整个matrix，是为了让matrix尽可能的加载到shared memory中，充分利用一个block只会调度到一个SM上，并且使用SM内的shared memory的机制。

如果要显示地使用shared memory可以按照下面的方式，

```c
__shared__ int my_shared_mem[mem_size]
```

而Triton中，可以看到，我们没有显示地使用shared memory，也没有强制程序使用一个thread block，但是按照block program的方式，它依然从cache hit rate中受益了，因为它把shared memory的使用时机放在了compiler中去自动处理。

因此，对于Triton比较准确的一句话描述是
”Block-wise编程，Block上面的归用户，Block内部的归Triton compiler自动化处理！“


## References

- [Triton 和 CUDA - 董鑫 Nvidia Researcher](https://www.zhihu.com/question/622685131/answer/3217107882)
- [谈谈对OpenAI Triton的一些理解 by 杨军 Nvidia compute arch](https://zhuanlan.zhihu.com/p/613244988) 高质量
# Triton 源码

- [Triton 源码](#triton-源码)
  - [Overview](#overview)
  - [Triton 源码结构](#triton-源码结构)
  - [Triton 前端](#triton-前端)
    - [JIT decortor](#jit-decortor)
    - [生成AST](#生成ast)
    - [生成Triton IR](#生成triton-ir)
    - [Triton Dialect](#triton-dialect)
  - [Triton 中端](#triton-中端)
    - [TritonIR转TritonGPUIR](#tritonir转tritongpuir)
    - [TritonGPUDialect](#tritongpudialect)
  - [Triton 后端](#triton-后端)
    - [TritonGPUIR生成LLVM IR(MLIR)](#tritongpuir生成llvm-irmlir)
    - [LLVM IR(MLIR)生成LLVM IR](#llvm-irmlir生成llvm-ir)
    - [LLVM IR生成PTX](#llvm-ir生成ptx)
    - [PTX生成cubin](#ptx生成cubin)
  - [Kernel Run](#kernel-run)
  - [Cache过程](#cache过程)
    - [Kernel Cache](#kernel-cache)
    - [File Cache](#file-cache)
  - [Triton Autotune](#triton-autotune)
  - [Triton Heuristic](#triton-heuristic)
  - [References](#references)

## Overview

Triton 3.0 代码为例：release/3.0.x:72b422d4906de2ae0209543faa6b8cc17ea4b11f

Triton整体依然可以认为是三段式的：前端 + 中端 + 后端

![Triton compiler arch.png](../.images/Triton%20compiler%20arch.png)

- 前端：Python源码生成AST，再由AST生成Triton IR
- 中端：执行各种优化pass，Triton IR生成TritonGPU IR
- 后端：TritonGPU IR生成LLVM IR，LLVM IR由LLVM后端生成PTX，使用ptxas最终编译称为cubin

目前Triton是基于MLIR开发的，所以从这个视角看，Triton中有两个Dialect：

- Triton Dialect：硬件无关
- TritonGPU Dialect：硬件相关

## Triton 源码结构

先熟悉下triton中所有的module

```python
# triton/__init__.py
"""isort:skip_file"""
__version__ = '3.0.0'

# ---------------------------------------
# Note: import order is significant here.

# submodules
from .runtime import (
    autotune,
    Config,
    heuristics,
    JITFunction,
    KernelInterface,
    reinterpret,
    TensorWrapper,
    OutOfResources,
    InterpreterError,
    MockTensor,
)
from .runtime.jit import jit
from .compiler import compile, CompilationError
from .errors import TritonError

from . import language
from . import testing
from . import tools

__all__ = [
    "autotune",
    "cdiv",
    "CompilationError",
    "compile",
    "Config",
    "heuristics",
    "impl",
    "InterpreterError",
    "jit",
    "JITFunction",
    "KernelInterface",
    "language",
    "MockTensor",
    "next_power_of_2",
    "ops",
    "OutOfResources",
    "reinterpret",
    "runtime",
    "TensorWrapper",
    "TritonError",
    "testing",
    "tools",
]

# -------------------------------------
# misc. utilities that  don't fit well
# into any specific module
# -------------------------------------


def cdiv(x: int, y: int):
    return (x + y - 1) // y


def next_power_of_2(n: int):
    """Return the smallest power of 2 greater than or equal to n"""
    n -= 1
    n |= n >> 1
    n |= n >> 2
    n |= n >> 4
    n |= n >> 8
    n |= n >> 16
    n |= n >> 32
    n += 1
    return n

```

其中比较重要的是：

- triton.language：triton kernel相关的，包括算子以及一些类型定义；
- triton.jit：装饰器，用于触发triton compiler；
- triton.autotune：装饰器，用于出发auto tune自动调参工具；
- triton.heuristics：指定某个超参数的计算过程；
- triton.Config：超参数的搜索空间；
- triton.testing：benchmark test相关的接口；

`language/__init__.py`中能够看到提供的所有操作。

```python
# triton/language/__init__.py
__all__ = [
    "PropagateNan",
    "TRITON_MAX_TENSOR_NUMEL",
    "_experimental_descriptor_load",
    "_experimental_descriptor_store",
    "abs",
    "advance",
    "arange",
    "argmax",
    "argmin",
    "associative_scan",
    "atomic_add",
    "atomic_and",
    "atomic_cas",
    "atomic_max",
    "atomic_min",
    "atomic_or",
    "atomic_xchg",
    "atomic_xor",
    "bfloat16",
    "block_type",
    "broadcast",
    "broadcast_to",
    "builtin",
    "cat",
    "cast",
    "cdiv",
    "ceil",
    "clamp",
    "const",
    "const_pointer_type",
    "constexpr",
    "cos",
    "cumprod",
    "cumsum",
    "debug_barrier",
    "device_assert",
    "device_print",
    "div_rn",
    "dot",
    "dtype",
    "erf",
    "exp",
    "exp2",
    "expand_dims",
    "extra",
    "fdiv",
    "flip",
    "float16",
    "float32",
    "float64",
    "float8e4b15",
    "float8e4nv",
    "float8e4b8",
    "float8e5",
    "float8e5b16",
    "floor",
    "fma",
    "full",
    "function_type",
    "histogram",
    "inline_asm_elementwise",
    "interleave",
    "int1",
    "int16",
    "int32",
    "int64",
    "int8",
    "ir",
    "join",
    "load",
    "log",
    "log2",
    "make_block_ptr",
    "math",
    "max",
    "max_constancy",
    "max_contiguous",
    "maximum",
    "min",
    "minimum",
    "multiple_of",
    "num_programs",
    "pair_uniform_to_normal",
    "permute",
    "philox",
    "philox_impl",
    "pi32_t",
    "pointer_type",
    "program_id",
    "rand",
    "rand4x",
    "randint",
    "randint4x",
    "randn",
    "randn4x",
    "range",
    "ravel",
    "reduce",
    "reshape",
    "rsqrt",
    "sigmoid",
    "sin",
    "softmax",
    "sort",
    "split",
    "sqrt",
    "sqrt_rn",
    "static_assert",
    "static_print",
    "static_range",
    "store",
    "sum",
    "swizzle2d",
    "tensor",
    "trans",
    "triton",
    "uint16",
    "uint32",
    "uint64",
    "uint8",
    "uint_to_uniform_float",
    "umulhi",
    "view",
    "void",
    "where",
    "xor_sum",
    "zeros",
    "zeros_like",
]
```

Triton的核心代码都实现在python和include中

![Triton Top Level Source Code.png](../.images/Triton%20Top%20Level%20Source%20Code.png)

python目录内分为两个部分，一部分是triton，一部分是src

- triton中是上面看到的所有module的python代码
- src部分是外层include的源代码，主要就是编译器部分了

![Triton python source code.png](../.images/Triton%20python%20source%20code.png)

其实核心代码就在include、python/triton、python/src中

## Triton 前端

Triton前端代码将用户用Python编写的kernel转换为对应的Triton IR (Triton Dialect)。

### JIT decortor

这一步的入口函数，就是jit decorator。

```python
# tirton/runtime/jit.py
def jit(
    fn: Optional[T] = None,
    *,
    version=None,
    repr: Optional[Callable] = None,
    launch_metadata: Optional[Callable] = None,
    do_not_specialize: Optional[Iterable[int]] = None,
    debug: Optional[bool] = None,
    noinline: Optional[bool] = None,
) -> Union[JITFunction[T], Callable[[T], JITFunction[T]]]:
    """
    Decorator for JIT-compiling a function using the Triton compiler.

    :note: When a jit'd function is called, arguments are
        implicitly converted to pointers if they have a :code:`.data_ptr()` method
        and a `.dtype` attribute.

    :note: This function will be compiled and run on the GPU. It will only have access to:

           * python primitives,
           * builtins within the triton package,
           * arguments to this function,
           * other jit'd functions

    :param fn: the function to be jit-compiled
    :type fn: Callable
    """

    def decorator(fn: T) -> JITFunction[T]:
        assert callable(fn)
        if os.getenv("TRITON_INTERPRET", "0") == "1":
            from .interpreter import InterpretedFunction
            return InterpretedFunction(fn)
        else:
            return JITFunction(
                fn,
                version=version,
                do_not_specialize=do_not_specialize,
                debug=debug,
                noinline=noinline,
                repr=repr,
                launch_metadata=launch_metadata,
            )

    if fn is not None:
        return decorator(fn)

    else:
        return decorator
```

也就是说，我们在host代码中launch的kernel函数，实际上就是调用了JITFunction类的方法。

以vectorAdd为例子，triton中使用了和cuda类似的launch kernel方式

```python
add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
```

即，`add_kernel[grid]`调用了JITFunction的`__getitem__`方法。

因为JITFunction继承自KernelInterface类，并且JITFunction类内部没有重写`__getitem__`方法

```python
class JITFunction(KernelInterface[T]):
    ...
```

所以，`add_kernel[grid]`实际上调用了KernelInterface类的`__getitem__`方法

```python
class KernelInterface(Generic[T]):
    run: T

    def __getitem__(self, grid) -> T:
        """
        A JIT function is launched with: fn[grid](*args, **kwargs).
        Hence JITFunction.__getitem__ returns a callable proxy that
        memorizes the grid.
        """
        return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
        # return cast(T, functools.partial(cast(Callable, self.run), grid=grid))
```

返回了一个lambda函数，所以`(x, y, output, n_elements, BLOCK_SIZE=1024)`是lambda函数的调用

这里的使用方式，是子类使用父类方法，所以self是子类对象，即是子类JITFunction的run方法的调用，参数是`(x, y, output, n_elements, BLOCK_SIZE=1024)`

run方法里面最核心的三步操作：

- 获取AST：`src = self.ASTSource(self, signature, constants, configs[0])`
- 编译：`kernel = self.compile(src, target=target,options=options.__dict__,)`
- 执行：`kernel.run(grid_0, grid_1, grid_2, stream, ...)`

下面逐行分析下run方法

```python
class JITFunction(KernelInterface[T]):

    def run(self, *args, grid, warmup, **kwargs):
        # parse options
        # 获取当前设备
        device = driver.active.get_current_device()
        # 获取当前设备的stream
        stream = driver.active.get_current_stream(device)
        kwargs["debug"] = self.debug

        # Execute pre run hooks with args and kwargs
        # 执行hook，用户可以通过add_kernel.add_pre_run_hook(my_hook)方式添加pre_run_hook
        for hook in self.pre_run_hooks:
            hook(*args, **kwargs)
        
        # todo：不清楚binder的作用
        if self.binder is None:
            self.create_binder()

        bound_args, sig_and_spec, constexpr_vals, non_constexpr_vals, excess_kwargs = self.binder(*args, **kwargs)

        # compute cache key
        # 类似："*fp16*fp16*fp16*fp16i32i32DDDDDD((32, 128), {'num_ctas': 1, 'debug': None})"的string作为key
        key = ''.join(sig_and_spec) + str((constexpr_vals, excess_kwargs))
        # self.cache用于存储编译后的kernel，如果是第一次执行，cache就是一个空的defaultdict
        kernel = self.cache[device].get(key, None)

        # 第一次执行，没有kernel cache，因此要进入编译流程
        if kernel is None:
            # Kernel is not cached; we have to compile.
            # 和llvm中的target类似，就是平台相关的信息，Triton中是GPUTarget类对应了hip和cuda，同时还存储了capability和warp_size
            # GPUTarget(backend='cuda', arch=80, warp_size=32)
            target = driver.active.get_current_target()
            # <nvi.CUDABackend object at 0x7fc52dcc66e0>
            backend = self.make_backend(target)
            # {'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128, 'num_ctas': 1, 'debug': None}
            options = backend.parse_options(kwargs)

            # deprecated arguments
            assert "device_type" not in kwargs, "device_type option is deprecated; current target will be used"
            assert "device" not in kwargs, "device option is deprecated; current device will be used"
            assert "stream" not in kwargs, "stream option is deprecated; current stream will be used"
            # {'num_ctas': 1, 'debug': None}
            for k in excess_kwargs:
                if k not in options.__dict__:
                    raise KeyError("Keyword argument %s was specified but unrecognised" % k)

            # bound_args是kernel的所有输入参数，包括常量参数
            """
            {'DW': tensor([[-0.4097, -0.1726, -0.0188,  ..., -0.0649,  0.0228,  0.1083],
                [ 0.3774,  0.0352, -0.2380,  ..., -1.1426, -0.4902,  0.0113],
                [ 0.3823, -0.3728,  0.5493,  ..., -0.2502,  0.0594, -0.2191],
                ...,
                [-0.5967, -0.4812, -0.4919,  ...,  0.1768, -0.1425,  0.1530],
                [ 0.3452, -0.4663,  0.8062,  ...,  0.2399, -0.5469, -0.6846],
                [ 0.3804,  0.1188,  0.1857,  ..., -0.7290,  0.3096, -0.3596]],
                device='cuda:0', dtype=torch.float16), 'DB': tensor([[ 0.4019,  0.1949,  0.3682,  ...,  0.6538,  0.4087,  0.7461],
                [-0.0181,  0.4734, -0.1302,  ..., -0.0966,  0.4004, -0.0760],
                [ 0.4890, -0.2385, -0.3589,  ..., -0.5532,  0.2712,  0.1644],
                ...,
                [ 0.3857,  0.0612, -0.2368,  ..., -0.0600, -0.3054,  0.1996],
                [ 0.6079,  0.2634, -0.2778,  ...,  0.3577,  0.2805,  0.1104],
                [-0.0302, -0.3418, -0.0416,  ...,  0.6602, -0.4807,  0.1779]],
                device='cuda:0', dtype=torch.float16), 'FINAL_DW': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', dtype=torch.float16), 'FINAL_DB': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', dtype=torch.float16), 'M': 96, 'N': 8192, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}
            """
            bound_vals = tuple(bound_args.values())

            # `None` is nullptr. Implicitly convert to *i8. This needs to be
            # done here rather than when we build the signature as otherwise
            # the kernel cache key could not distinguish between byte pointers
            # and None arguments, resulting in a downstream mismatch:
            # 非常量的参数名
            sigkeys = [self.params[i].name for i in self.non_constexpr_indices]
            # 非常量的参数的类型
            sigvals = sig_and_spec[:len(sigkeys)]
            # 拼接成 sigkeys:sigvals 的map
            signature = {k: ('*i8' if (v == 'none') else v) for (k, v) in zip(sigkeys, sigvals)}

            configs = (self._get_config(*bound_vals), )
            constants = {
                p.name: v
                for (v, p) in zip(bound_vals, self.params)
                if p.is_constexpr or p.num in configs[0].equal_to_1 or v is None
            }
            for i, arg in constants.items():
                if callable(arg):
                    raise TypeError(f"Callable constexpr at index {i} is not supported")

            if self._call_hook(key, signature, device, constants, options, configs):
                return None
            # compile the kernel
            # 转换为AST <triton.compiler.compiler.ASTSource object at 0x7fc52dcc50f0>
            src = self.ASTSource(self, signature, constants, configs[0])
            # 转换为CompiledKernel对象 <triton.compiler.compiler.CompiledKernel object at 0x7fc52dcea650>
            kernel = self.compile(
                src,
                target=target,
                options=options.__dict__,
            )
            # 将编译完的kernel存入cache中，方便之后再次执行
            self.cache[device][key] = kernel

        # Check that used global values have not changed.
        not_present = object()
        for (name, globals_dict_id), (val, globals_dict) in self.used_global_vals.items():
            if (newVal := globals_dict.get(name, not_present)) != val:
                raise RuntimeError(
                    f"Global variable {name} has changed since we compiled this kernel, from {val} to {newVal}")

        if not warmup:
            # canonicalize grid
            assert grid is not None
            if callable(grid):
                # Arguments are passed as a dict to `grid`, by contract.
                # TODO(jlebar): In the new launch API, pass the compiler flags as a
                # second parameter to `grid`.
                grid = grid(bound_args)
            grid_size = len(grid)
            grid_0 = grid[0]
            grid_1 = grid[1] if grid_size > 1 else 1
            grid_2 = grid[2] if grid_size > 2 else 1

            # launch kernel
            # todo：先不管metadata是什么
            launch_metadata = kernel.launch_metadata(grid, stream, *non_constexpr_vals)
            kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,
                       self.CompiledKernel.launch_enter_hook, self.CompiledKernel.launch_exit_hook, *non_constexpr_vals)
        return kernel
```

涉及到前端的部分，有两处：

- 生成AST：`src = self.ASTSource(self, signature, constants, configs[0])`
- 编译Kernel中的一部分：`kernel = self.compile(src,target=target,options=options.__dict__,)`

### 生成AST

```python
src = self.ASTSource(self, signature, constants, configs[0])
```

代码的返回值是 `<triton.compiler.compiler.ASTSource object at 0x7fc52dcc50f0>`，就是下面的ASTSource的对象，这里并没有做真正的AST生成，支持提供了一个用于AST生成的接口，真正的AST生成在ast_to_ttir中执行

```python
class ASTSource:

    def __init__(self, fn, signature, constants=None, attrs=None) -> None:
        self.fn = fn
        self.ext = "ttir"
        self.name = fn.__name__
        self.signature = signature
        self.constants = constants
        self.attrs = attrs
        if isinstance(self.signature, str):
            self.signature = {k: v.strip() for k, v in enumerate(self.signature.split(","))}
        if self.constants is None:
            self.constants = dict()
        if self.attrs is None:
            self.attrs = AttrsDescriptor()

    def hash(self):
        sorted_sig = [v for k, v in sorted(self.signature.items())]
        # Note - we stringify the keys here to allow sorting to work for cases
        # where constants have mixed int/str keys.
        sorted_constants = sorted((str(k), v) for k, v in self.constants.items())
        key = f"{self.fn.cache_key}-{self.attrs.hash()}-{sorted_sig}-{sorted_constants}"
        return hashlib.sha256(key.encode("utf-8")).hexdigest()

    def make_ir(self, options, codegen_fns, context):
        return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)

    def parse_options(self):
        return dict()
```

### 生成Triton IR

`self.compile(...)`实际上调用的是Triton中comiler module下的compile函数

- stages：`{'ttir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6710>, 'ttgir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6d40>, 'llir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6dd0>, 'ptx': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6e60>, 'cubin': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6ef0>}`

先看一下简化的compiler函数流程

```python
def compile(src, target=None, options=None):
    ...
    # 根据指定的target获取一个backend，这里会返回CUDABackend
    backend = make_backend(target) 
    ...
    # 添加编译stage
    backend.add_stages(stages, options) 
    ...
    # 加载所需dialects
    ir.load_dialects(context)  
    backend.load_dialects(context)
    # 创建Triton IR
    module = src.make_ir(options, context) 
    ...
    for ext, compile_ir in list(stages.items())[first_stage:]:
        # 编译、优化各个阶段IR
        next_module = compile_ir(module, metadata) 
        ...
        module = next_module
    ...
    return CompiledKernel(src, metadata_group, hash)
```

下面看下完整的代码

```python
# triton/compiler/compiler.py:226
def compile(src, target=None, options=None):
    if target is None:
        target = driver.active.get_current_target()
    assert isinstance(target, GPUTarget), "target must be of GPUTarget type"
    backend = make_backend(target)
    # 第一次编译的话，src是ASTSource，ir_source是false
    ir_source = not isinstance(src, ASTSource)
    # create backend
    if ir_source:
        assert isinstance(src, str), "source must be either AST or a filepath"
        src = IRSource(src)
    extra_options = src.parse_options()
    options = backend.parse_options(dict(options or dict(), **extra_options))
    # create cache manager
    env_vars = get_cache_invalidating_env_vars()
    key = f"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}"
    hash = hashlib.sha256(key.encode("utf-8")).hexdigest()
    fn_cache_manager = get_cache_manager(hash)
    # For dumping/overriding only hash the source as we want it to be independent of triton
    # core changes to make it easier to track kernels by hash.
    enable_override = os.environ.get("TRITON_KERNEL_OVERRIDE", "0") == "1"
    enable_ir_dump = os.environ.get("TRITON_KERNEL_DUMP", "0") == "1"
    fn_override_manager = get_override_manager(src.hash()) if enable_override else None
    fn_dump_manager = get_dump_manager(src.hash()) if enable_ir_dump else None
    metadata_filename = f"{src.name}.json"
    metadata_group = fn_cache_manager.get_group(metadata_filename) or {}
    metadata_path = metadata_group.get(metadata_filename)
    always_compile = os.environ.get("TRITON_ALWAYS_COMPILE", "0") == "1"
    if not always_compile and metadata_path is not None:
        # cache hit!
        metadata = json.loads(Path(metadata_path).read_text())
        return CompiledKernel(src, metadata_group, hash)
    # initialize metadata
    metadata = {
        "hash": hash,
        "target": target,
        **options.__dict__,
        **env_vars,
    }
    # run compilation pipeline  and populate metadata
    stages = dict()
    backend.add_stages(stages, options)
    # src.ext = "ttir"
    first_stage = list(stages.keys()).index(src.ext)
    # when the source is an IR file, don't apply the passes related to this stage. This makes it easier to write IR level tests.
    if ir_source:
        first_stage += 1
    # ir来自于triton._C模块，已经不是python代码了，<module 'triton._C.libtriton.ir'>
    # context: <triton._C.libtriton.ir.context object at 0x7f28fd677b70>
    context = ir.context()
    # 加载dialect
    ir.load_dialects(context)
    backend.load_dialects(context)
    codegen_fns = backend.get_codegen_implementation()
    try:
        # 调用ASTSource的make_ir方法获取到 <triton._C.libtriton.ir.module object at 0x7f28b5fe0ef0>
        module = src.make_ir(options, codegen_fns, context)
    except Exception as e:
        filter_traceback(e)
        raise
    use_ttgir_loc = os.environ.get("USE_TTGIR_LOC", "0") == "1"
    # 按照stage从AST开始一步步生成，同时进行优化，每一步生成的结果会单独存储到metadata_group[ir_filename]中
    for ext, compile_ir in list(stages.items())[first_stage:]:
        next_module = compile_ir(module, metadata)
        ir_filename = f"{src.name}.{ext}"
        metadata_group[ir_filename] = fn_cache_manager.put(next_module, ir_filename)
        if fn_dump_manager is not None:
            fn_dump_manager.put(next_module, ir_filename)
        if (fn_override_manager is not None and fn_override_manager.has_file(ir_filename)):
            print(f"\nOverriding kernel with file {ir_filename}")
            full_name = fn_override_manager.get_file(ir_filename)
            next_module = parse(full_name, ext, context)
        # use an env variable to parse ttgir from file
        if use_ttgir_loc and ext == "ttgir":
            ttgir_full_name = fn_cache_manager.get_file(ir_filename)
            next_module.create_location_snapshot(ttgir_full_name)
            print(f"Create new locations for {ttgir_full_name}")
        module = next_module
    # write-back metadata
    metadata_group[metadata_filename] = fn_cache_manager.put(json.dumps(metadata, default=vars), metadata_filename,binary=False)
    fn_cache_manager.put_group(metadata_filename, metadata_group)
    # return handle to compiled kernel
    return CompiledKernel(src, metadata_group, hash)
```

从上到下疏离关键逻辑，首先是dialect加载，是由ir的方法来负责的

在本文已开始，我们看过了Triton的项目结构，c++相关的定义都在src中，来到src/main.cc可以知道ir这个module的定义

```python
# python/src/main.cc
PYBIND11_MODULE(libtriton, m) {
  m.doc() = "Python bindings to the C++ Triton API";
  init_triton_env_vars(m);
  init_triton_ir(m.def_submodule("ir"));
  init_triton_passes(m.def_submodule("passes"));
  init_triton_interpreter(m.def_submodule("interpreter"));
  init_triton_llvm(m.def_submodule("llvm"));
  FOR_EACH_P(INIT_BACKEND, TRITON_BACKENDS_TUPLE)
}
```

```c++
  m.def("load_dialects", [](MLIRContext &context) {
    DialectRegistry registry;
    registry.insert<TritonDialect, ::mlir::triton::gpu::TritonGPUDialect,
                    math::MathDialect, arith::ArithDialect, index::IndexDialect,
                    scf::SCFDialect, ::mlir::gpu::GPUDialect,
                    cf::ControlFlowDialect, LLVM::LLVMDialect>();
    registerBuiltinDialectTranslation(registry);
    registerLLVMDialectTranslation(registry);
    context.appendDialectRegistry(registry);
    context.loadAllAvailableDialects();
  });
```

对于TritonIR生成，需要的是TritonDialect，它是由TableGen根据td文件生成的类型。这里先不看TritonDialect具体定义，下面的Triton Dialect章节中专门分析。

继续向后走，真正的代码生成存在于`module = src.make_ir(options, codegen_fns, context)`和之后的for loop内

先看第一步的make_ir究竟做了什么

回到ASTSource

```python
class ASTSource:
    def make_ir(self, options, codegen_fns, context):
        return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)
```

调用了code_generator中的ast_to_ttir函数

```python
# python/triton/compiler/code_generator.py
def ast_to_ttir(fn, specialization, context, options, codegen_fns):
    # fn还是JITFunction
    attrs = specialization.attrs
    # create kernel prototype
    cst_key = lambda i: fn.arg_names.index(i) if isinstance(i, str) else i
    constants = {cst_key(key): value for key, value in specialization.constants.items()}
    # visit kernel AST
    gscope = fn.__globals__.copy()
    function_name = fn.repr(specialization)
    tys = list(specialization.signature.values())
    new_constants = {k: True if k in tys and tys[k] == "i1" else 1 for k in attrs.equal_to_1}
    new_attrs = {k: [("tt.divisibility", 16)] for k in attrs.divisible_by_16}

    all_constants = constants.copy()
    all_constants.update(new_constants)
    arg_types = [str_to_ty(v) for k, v in specialization.signature.items() if k not in specialization.constants]
    file_name, begin_line = _get_fn_file_line(fn)

    prototype = language.function_type([], arg_types)
    generator = CodeGenerator(context, prototype, gscope=gscope, constants=all_constants, function_name=function_name,
                              jit_fn=fn, attributes=new_attrs, is_kernel=True, file_name=file_name,
                              begin_line=begin_line, options=options, codegen_fns=codegen_fns)
    # fn.parse(): <ast.Module object at 0x7f78ef4ad2a0>
    # 输入是AST
    generator.visit(fn.parse())
    # 输出已经是ir module了：<triton._C.libtriton.ir.module object at 0x7f78ef477e20>
    ret = generator.module
    # module takes ownership of the context
    ret.context = context
    return ret
```

也就是说，这里做了两步操作，一个是`fn.parse`获取Python AST，然后是`generator.visit`生成TritonIR

回到JITFunction，可以看到，实际上就是调用的python官方的ast方法获取的AST，self.src则是源码的字符串

```python
import ast
class JITFunction(KernelInterface[T]):
    def parse(self):
        tree = ast.parse(self.src)
        assert isinstance(tree, ast.Module)
        assert len(tree.body) == 1
        assert isinstance(tree.body[0], ast.FunctionDef)
        return tree
```

self.src示例：

```shell
(Pdb) p self.src
"def add_kernel(\n    x_ptr,  # *Pointer* to first input vector.\n    y_ptr,  # *Pointer* to second input vector.\n    output_ptr,  # *Pointer* to output vector.\n    n_elements,  # Size of the vector.\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n    # NOTE: `constexpr` so it can be used as a shape value.\n):\n    # There are multiple 'programs' processing different data. We identify which program\n    # we are here:\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n    # This program will process inputs that are offset from the initial data.\n    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n    # Note that offsets is a list of pointers:\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    # Create a mask to guard memory operations against out-of-bounds accesses.\n    mask = offsets < n_elements\n    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n    # multiple of the block size.\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    # Write x + y back to DRAM.\n    tl.store(output_ptr + offsets, output, mask=mask)\n"
```

返回的tree则是`<ast.Module object at 0x7f061493f9a0>`，这是一个树。

这里没有直接遍历它，而是查看了下它的root node是不是ast.FunctionDef（之后生成tirton ir的时候才回去遍历它），那就在自己写一个方法来遍历下这个树，把它print出来看看它长什么样子

```python
def print_ast(node, level=0):
    # 打印当前节点的类型和一些信息
    print('  ' * level + type(node).__name__)
    
    # 遍历所有属性，如果属性值是AST节点，递归打印
    for field, value in ast.iter_fields(node):
        if isinstance(value, list):
            for item in value:
                if isinstance(item, ast.AST):
                    print_ast(item, level + 1)
        elif isinstance(value, ast.AST):
            print_ast(value, level + 1)
```

获得输出如下，拿这个AST和add_kernel函数对比是能完全对应的，毕竟只是翻译。具体怎么翻译过来的先不深究。

```python
Module
  FunctionDef
    arguments
      arg
      arg
      arg
      arg
      arg
        Attribute
          Name
            Load
          Load
    Assign
      Name
        Store
      Call
        Attribute
          Name
            Load
          Load
        keyword
          Constant
    Assign
      Name
        Store
      BinOp
        Name
          Load
        Mult
        Name
          Load
    Assign
      Name
        Store
      BinOp
        Name
          Load
        Add
        Call
          Attribute
            Name
              Load
            Load
          Constant
          Name
            Load
    Assign
      Name
        Store
      Compare
        Name
          Load
        Lt
        Name
          Load
    Assign
      Name
        Store
      Call
        Attribute
          Name
            Load
          Load
        BinOp
          Name
            Load
          Add
          Name
            Load
        keyword
          Name
            Load
    Assign
      Name
        Store
      Call
        Attribute
          Name
            Load
          Load
        BinOp
          Name
            Load
          Add
          Name
            Load
        keyword
          Name
            Load
    Assign
      Name
        Store
      BinOp
        Name
          Load
        Add
        Name
          Load
    Expr
      Call
        Attribute
          Name
            Load
          Load
        BinOp
          Name
            Load
          Add
          Name
            Load
        Name
          Load
        keyword
          Name
            Load
```

完成`fn.parse()`获取到AST之后，进入visit方法`generator.visit(fn.parse())`

```python
class CodeGenerator(ast.NodeVisitor):
    def visit(self, node):
        if node is None:
            return
        with warnings.catch_warnings():
            # The ast library added visit_Constant and deprecated some other
            # methods but we can't move to that without breaking Python 3.6 and 3.7.
            warnings.simplefilter("ignore", DeprecationWarning)  # python 3.9
            warnings.simplefilter("ignore", PendingDeprecationWarning)  # python 3.8

            # 下面两行为了保留现场
            # cur_node初始化为None
            last_node = self.cur_node
            # self.builder也来自于triton._C
            last_loc = self.builder.get_loc()
            # node为AST的root node，也就是FunctionDef
            self.cur_node = node
            if hasattr(node, 'lineno') and hasattr(node, 'col_offset'):
                self.builder.set_loc(self.file_name, self.begin_line + node.lineno, node.col_offset)
                last_loc = self.builder.get_loc()
            try:
                # 子类调用父类的visit方法
                ret = super().visit(node)
            except CompilationError:
                raise
            except Exception as e:
                # Wrap the error in a CompilationError which contains the source
                # of the @jit function.
                raise CompilationError(self.jit_fn.src, self.cur_node, repr(e)) from None

            # Reset the location to the last one before the visit
            if last_loc:
                self.cur_node = last_node
                self.builder.set_loc(last_loc)
            return ret
```

进一步深入到父类`ast.NodeVisitor`的visit方法，它还是python内置的ast module。

```python
import ast
class CodeGenerator(ast.NodeVisitor):
    ...
```

如果深入进去看的话，实际上就是上面实现的深度优先遍历 + 子类的visit方法，只是没有print，不知道它长什么样子。

```python
class NodeVisitor(object):
    def visit(self, node):
        """Visit a node."""
        # 这里的self是子类
        method = 'visit_' + node.__class__.__name__
        # 获取子类中的当前节点visit的方法，否则就是generic_visit
        visitor = getattr(self, method, self.generic_visit)
        return visitor(node)

    def generic_visit(self, node):
        """Called if no explicit visitor function exists for a node."""
        for field, value in iter_fields(node):
            if isinstance(value, list):
                for item in value:
                    if isinstance(item, AST):
                        self.visit(item)
            elif isinstance(value, AST):
                self.visit(value)
```

为了能够完成转换，这里最关键的一步是`visitor = getattr(self, method, self.generic_visit)`，vistor实际上是子类CodeGenerator中的`'visit_' + node.__class__.__name__`组合起来的方法。

例如：

```python
class CodeGenerator(ast.NodeVisitor):
    def visit_Module:
        ...
    def visit_FunctionDef:
        ...
    def visit_Load:
        ...
    ...
```

以visit_FunctionDef为例子，看下它里面做了什么，重点是怎么从AST称为Triton IR的

```python
class CodeGenerator(ast.NodeVisitor):
    def visit_FunctionDef(self, node):
        # visit我们实现的kernel函数的入参
        # arg_names: ['x_ptr', 'y_ptr', 'output_ptr', 'n_elements', 'BLOCK_SIZE']
        # kwarg_names: None
        arg_names, kwarg_names = self.visit(node.args)
        if self.fn:
            raise self._unsupported(node, "nested function definition is not supported.")
        # initialize defaults
        # vector add这个例子里没有default args
        for i, default_value in enumerate(node.args.defaults):
            arg_node = node.args.args[-i - 1]
            annotation = arg_node.annotation
            name = arg_node.arg
            st_target = ast.Name(id=name, ctx=ast.Store())
            if annotation is None:
                init_node = ast.Assign(targets=[st_target], value=default_value)
            else:
                init_node = ast.AnnAssign(target=st_target, value=default_value, annotation=annotation)

            try:
                assert not self.visiting_arg_default_value
                self.visiting_arg_default_value = True
                self.visit(init_node)
            finally:
                self.visiting_arg_default_value = False

        # initialize function
        visibility = "public" if self.is_kernel else "private"
        # 通过builder获取到self.fn：<triton._C.libtriton.ir.function object at 0x7f6835d001f0>
        # self.module: <triton._C.libtriton.ir.module object at 0x7f682cc0aca0>
        self.fn = self.builder.get_or_insert_function(self.module, self.function_name,
                                                      self.prototype.to_ir(self.builder), visibility, self.noinline)
        self.module.push_back(self.fn)
        entry = self.fn.add_entry_block()
        arg_values = []
        idx = 0
        for i, arg_name in enumerate(arg_names):
            if i in self.constants:
                cst = self.constants[i]
                if not _is_constexpr(cst):
                    cst = constexpr(self.constants[i])
                arg_values.append(cst)
                continue
            else:
                if i in self.attributes:
                    for name, value in self.attributes[i]:
                        self.fn.set_arg_attr(idx, name, value)
                arg_values.append(tensor(self.fn.args(idx), self.prototype.param_types[idx]))
                idx += 1

        insert_pt = self.builder.get_insertion_block()
        for arg_name, arg_value in zip(arg_names, arg_values):
            self.set_value(arg_name, arg_value)
        self.builder.set_insertion_point_to_start(entry)
        # visit function body
        self.visit_compound_statement(node.body)
        # finalize function
        if self.ret_type is None or self.ret_type == language.void:
            self.ret_type = language.void
            self.builder.ret([])
        else:
            # update return type
            if isinstance(self.ret_type, tuple):
                self.prototype.ret_types = list(self.ret_type)
                self.fn.reset_type(self.prototype.to_ir(self.builder))
            else:
                self.prototype.ret_types = [self.ret_type]
                self.fn.reset_type(self.prototype.to_ir(self.builder))
        if insert_pt:
            self.builder.set_insertion_point_to_end(insert_pt)
        # Remove dead code
        self.fn.finalize()
```

实际的转换动作由builder对象负责，它在code_generator的`__init__`中被初始化。它是来自_C.libtriton中的module。

```python
from .._C.libtriton import ir

class CodeGenerator(ast.NodeVisitor):

    def __init__(self, context, prototype, gscope, attributes, constants, function_name, jit_fn: JITFunction, options,
                 codegen_fns, debug=None, module=None, is_kernel=False, function_types: Optional[Dict] = None,
                 noinline=False, file_name: Optional[str] = None, begin_line=0):
        self.context = context
        self.builder = ir.builder(context)
        ...
```

继续深入到`init_triton_ir`中找builder的binding，这个class binding足足有500+行，TritonOpBuilder就是它实际的c++类了。

```python
# python/src/ir.cc
py::class_<TritonOpBuilder>(m, "builder", py::module_local(),
                              py::dynamic_attr())
      .def(py::init<MLIRContext *>())
      // getters
      .def("create_module",
           [](TritonOpBuilder &self) -> ModuleOp {
             return self.create<ModuleOp>();
           })
      // insertion block/point
      .def("set_insertion_point_to_start",
           [](TritonOpBuilder &self, Block &block) -> void {
             self.setInsertionPointToStart(block);
           })
      .def("set_insertion_point_to_end",
           [](TritonOpBuilder &self, Block &block) {
             self.setInsertionPointToEnd(block);
           }
        ...
      );
```

以前面遇到的`get_or_insert_function`为例：

```python
self.fn = self.builder.get_or_insert_function(self.module, self.function_name,
                                              self.prototype.to_ir(self.builder), visibility, self.noinline)
```

```c++
// Ops
.def("get_or_insert_function",
      [](TritonOpBuilder &self, ModuleOp &module, std::string &funcName,
        Type &funcType, std::string &visibility,
        bool noinline) -> FuncOp {
        if (Operation *funcOperation = module.lookupSymbol(funcName))
          # 如果当前module中已经创建过这个function，则直接返回它
          return llvm::dyn_cast<FuncOp>(funcOperation);
        if (auto funcTy = dyn_cast<FunctionType>(funcType)) {
          # 如果没有，则按照它的函数名、函数定义、函数属性创建一个FuncOp出来
          llvm::SmallVector<NamedAttribute> attrs = {
              NamedAttribute(
                  self.getBuilder().getStringAttr("sym_visibility"),
                  self.getBuilder().getStringAttr(visibility)),
              NamedAttribute(self.getBuilder().getStringAttr("noinline"),
                            self.getBuilder().getBoolAttr(noinline))};
          return self.create<FuncOp>(funcName, funcTy, attrs);
        }
        throw std::invalid_argument("invalid function type");
      })
```

可以看到，实际执行TritonIR的类型创建的，是又TritonOpBuilder中的builder负责的

```c++
class TritonOpBuilder {
// ...
private:
  std::unique_ptr<OpBuilder> builder;
  std::unique_ptr<Location> lastLoc;
// ...
}
```

OpBuilder，[mlir::OpBuilder](https://mlir.llvm.org/doxygen/classmlir_1_1OpBuilder.html)，MLIR提供的标准接口用于创建OP。

create出来的FuncOp类型，它是Triton Dialect，它的实现是由TableGen来生成`build/XXXX/include/triton/Dialect/Triton/IR/Ops.h.inc`

```c++
class FuncOp : public ::mlir::Op<FuncOp, ::mlir::OpTrait::OneRegion, ::mlir::OpTrait::ZeroResults, ::mlir::OpTrait::ZeroSuccessors, ::mlir::OpTrait::ZeroOperands, ::mlir::OpTrait::OpInvariants, ::mlir::BytecodeOpInterface::Trait, ::mlir::OpTrait::AffineScope, ::mlir::OpTrait::AutomaticAllocationScope, ::mlir::CallableOpInterface::Trait, ::mlir::SymbolOpInterface::Trait, ::mlir::FunctionOpInterface::Trait, ::mlir::OpTrait::IsIsolatedFromAbove, ::mlir::OpAsmOpInterface::Trait> {
// ...
  static constexpr ::llvm::StringLiteral getOperationName() {
    return ::llvm::StringLiteral("tt.func");  // op名被定义为tt.func，我们在打印出来的tritonir中看到的就是这个名字
  }
}
```

它也进行了python binding

```c++
  py::class_<FuncOp, OpState>(m, "function", py::module_local())
      // .def_property_readonly("attrs", &ir::function::attrs)
      // .def("add_attr", &ir::function::add_attr);
      .def("args",
           [](FuncOp &self, unsigned idx) -> BlockArgument {
             if (idx >= self.getNumArguments())
               throw pybind11::index_error(
                   "Function argument index out of range");
             return self.getArgument(idx);
           })
      .def(
          "add_entry_block",
          [](FuncOp &self) -> Block * { return self.addEntryBlock(); },
          ret::reference)
      .def(
          "set_arg_attr",
          [](FuncOp &self, int arg_no, const std::string &name, int val) {
            // set arg attributes "name" to value "val"
            auto attrTy = IntegerType::get(self.getContext(), 32);
            self.setArgAttr(arg_no, name, IntegerAttr::get(attrTy, val));
          },
          ret::reference)
      //  .def("has_attr", &::FuncOp::hasAttr)
      .def("finalize",
           [](FuncOp &self) -> void {
             // Remove dead code
             // 1. Unreachable code after return
             self.walk([&](Block *block) {
               Operation *retOp = nullptr;
               // It's better to not use walk here because we only want to
               // check operations in the current block
               for (auto &op : block->getOperations()) {
                 if (isa<ReturnOp>(op))
                   if (retOp == nullptr) {
                     retOp = &op;
                     break;
                   }
               }
               if (retOp && retOp != &block->back()) {
                 auto pos = retOp->getIterator();
                 pos++;
                 auto *newBlock = block->splitBlock(pos);
                 newBlock->erase();
               }
             });
             // 2. Check if the result of tl.advance is used
             self.walk([&](Operation *op) {
               if (isa<AdvanceOp>(op) && op->getResult(0).use_empty())
                 outputWarning(op->getLoc(), "The result of tl.advance is not "
                                             "being used. Note that tl.advance "
                                             "does not have any side effects. "
                                             "To move the block pointer, you "
                                             "need to assign the result of "
                                             "tl.advance to a variable.");
             });
           })
      .def_property_readonly("type", &FuncOp::getFunctionType)
      .def("reset_type", &FuncOp::setType);
```

之后，fn再加入到module中

```python
self.module.push_back(self.fn)
```

对应到c++世界中如下：

```c++
py::class_<ModuleOp, OpState>(m, "module", py::module_local(),
                              py::dynamic_attr())
    // ...
    .def("push_back",
          [](ModuleOp &self, FuncOp &funcOp) -> void {
            self.push_back(funcOp);
          })
    // ...
```

在来看一下对于具体的计算Op，是怎么生成TritonIR的，这里以BinOp二元运算为例子：

```python
def visit_BinOp(self, node):
    # 获取左右操作数
    lhs = self.visit(node.left)
    rhs = self.visit(node.right)
    # 对于vector add，获取到的method_name是add
    method_name = self._method_name_for_bin_op.get(type(node.op))
    if method_name is None:
        raise self._unsupported(node,
                                "AST binary operator '{}' is not (currently) implemented.".format(node.op.__name__))
    return self._apply_binary_method(method_name, lhs, rhs)
```

调用`_apply_binary_method`

```python
def _apply_binary_method(self, method_name, lhs, rhs):
    # TODO: raise something meaningful if getattr fails below, esp for reverse method
    # 对于做操作数是triton_tensor类型的，要调用triton_tensor的add方法
    if _is_triton_tensor(lhs):
        return getattr(lhs, method_name)(rhs, _builder=self.builder)
    if _is_triton_tensor(rhs):
        reverse_method_name = re.sub(r"__(.*)__", r"__r\1__", method_name)
        return getattr(rhs, reverse_method_name)(lhs, _builder=self.builder)
    return getattr(lhs, method_name)(rhs)
```

triton tensor类型被定义在了`python/triton/language/core.py`文件中

```python
from . import semantic
class tensor:
  """Represents an N-dimensional array of values or pointers.

    :code:`tensor` is the fundamental data structure in Triton programs.  Most
    functions in :py:mod:`triton.language` operate on and return tensors.
  """
  ...
  @builtin
  def __add__(self, other, _builder=None):
      other = _to_tensor(other, _builder)
      return semantic.add(self, other, _builder)
  ...
```

进入到semantic.p中的add函数

```python
def add(input: tl.tensor, other: tl.tensor, builder: ir.builder) -> tl.tensor:
    input, other = binary_op_type_checking_impl(input, other, builder, True, True)
    input_scalar_ty = input.type.scalar
    other_scalar_ty = other.type.scalar
    if input_scalar_ty.is_ptr() and other_scalar_ty.is_ptr():
        raise TypeError("cannot add pointers together")

    # offset + ptr
    # ptr + offset
    if other_scalar_ty.is_ptr() and not input_scalar_ty.is_ptr():
        input, other = other, input
        input_scalar_ty = input.type.scalar
        other_scalar_ty = other.type.scalar
    if input_scalar_ty.is_ptr():
        return tl.tensor(builder.create_addptr(input.handle, other.handle), input.type)
    # float + float
    elif input_scalar_ty.is_floating():
        return tl.tensor(builder.create_fadd(input.handle, other.handle), input.type)
    # int + int
    elif input_scalar_ty.is_int():
        return tl.tensor(builder.create_add(input.handle, other.handle), input.type)
    raise TypeError(f"unexpected type {input_scalar_ty}")
```

可以看到无论哪一个分支，都是进入到ir.builder当中，create_add是使用的预置的arith dialect中的AddIOp

```c++
.def("create_add",
      [](TritonOpBuilder &self, Value &lhs, Value &rhs) -> Value {
        return self.create<arith::AddIOp>(lhs, rhs);
      })
```

对于load和store函数，则binding到了triton定义的LoadOp和StoreOp

```c++
// Input/Output
.def("create_load",
      [](TritonOpBuilder &self, Value &ptrs, CacheModifier cacheModifier,
        EvictionPolicy evictionPolicy, bool isVolatile) -> Value {
        return self.create<LoadOp>(ptrs, cacheModifier, evictionPolicy,
                                  isVolatile);
      })
.def("create_store",
      [](TritonOpBuilder &self, Value &ptrs, Value &value,
        CacheModifier cacheModifier,
        EvictionPolicy evictionPolicy) -> void {
        self.create<StoreOp>(ptrs, value, cacheModifier, evictionPolicy);
      })
```

它们的实现也是由TableGen来生成的`build/XXXX/include/triton/Dialect/Triton/IR/Ops.h.inc`

```c++
namespace mlir {
namespace triton {
class LoadOp : public ::mlir::Op<...> {
  //...
}
}
}
```

经过code generator完成AST Tree的遍历后，即可获得最终的ModuleOp，它就是ast_to_ttir函数的返回值，也就是最终的TritonIR

```python
def ast_to_ttir(fn, specialization, context, options, codegen_fns):
    ...
    generator.visit(fn.parse())

    ret = generator.module
    # module takes ownership of the context
    ret.context = context
    return ret
```

通过设置环境变量TRITON_CACHE_DIR=./和TRITON_KERNEL_DUMP=1，我们可以将所有的IR保存到当前路径

vector add用例的TritonIR如下，和我们在python中写的add_kernel亦是能对应上的，比如前面看过的FuncOp的OpName "tt.func"

```c++
// add_kernel.ttir
#loc = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0)
module {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0)) attributes {noinline = false} {
    %c1024_i32 = arith.constant 1024 : i32 loc(#loc1)
    %0 = tt.get_program_id x : i32 loc(#loc2)
    %1 = arith.muli %0, %c1024_i32 : i32 loc(#loc3)
    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32> loc(#loc4)
    %3 = tt.splat %1 : i32 -> tensor<1024xi32> loc(#loc5)
    %4 = arith.addi %3, %2 : tensor<1024xi32> loc(#loc5)
    %5 = tt.splat %arg3 : i32 -> tensor<1024xi32> loc(#loc6)
    %6 = arith.cmpi slt, %4, %5 : tensor<1024xi32> loc(#loc6)
    %7 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc7)
    %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc7)
    %9 = tt.load %8, %6 : tensor<1024x!tt.ptr<f32>> loc(#loc8)
    %10 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc9)
    %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc9)
    %12 = tt.load %11, %6 : tensor<1024x!tt.ptr<f32>> loc(#loc10)
    %13 = arith.addf %9, %12 : tensor<1024xf32> loc(#loc11)
    %14 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc12)
    %15 = tt.addptr %14, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc12)
    tt.store %15, %13, %6 : tensor<1024x!tt.ptr<f32>> loc(#loc13)
    tt.return loc(#loc14)
  } loc(#loc)
} loc(#loc)
#loc1 = loc(unknown)
#loc2 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":27:24)
#loc3 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":32:24)
#loc4 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":33:41)
#loc5 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":33:28)
#loc6 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":35:21)
#loc7 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":38:24)
#loc8 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":38:16)
#loc9 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":39:24)
#loc10 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":39:16)
#loc11 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":40:17)
#loc12 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:26)
#loc13 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:35)
#loc14 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:4)
```

至此，Triton前端的使命已经完成！我们也顺利从Python世界进入到C++世界，再进入到了MLIR世界。

### Triton Dialect

此处还有一个重要问题，我们一开始说的Trition定义了两个MLIR Dialect，一个是Triton Dialect，另一个TritonGPU Dialect。

而Triton IR已经生成了，似乎除了OpBuilder，ModuleOp，FuncOp，LoadOp之外，还没有完整的看一下Triton Dialect的定义。

在Triton Doc中可以找到Triton Dialect的说明：[‘tt’ Dialect](https://triton-lang.org/main/dialects/TritonDialect.html)

代码中，Triton Dialect被实现在`python/triton/_C/include/triton/Dialect/Triton/`路径下。


| 文件                                                 | 说明                            |
| ---------------------------------------------------- | ------------------------------- |
| include/triton/Dialect/Triton/IR/TritonDialect.td    | ttir的定义 Dialect TableGen文件 |
| include/triton/Dialect/Triton/IR/TritonOps.td        | ttir内部的op相关的定义          |
| include/triton/Dialect/Triton/IR/TritonAttrDefs.td   | ops的属性信息定义               |
| include/triton/Dialect/Triton/IR/TritonTypes.td      | 内部类型定义                    |
| include/triton/Dialect/Triton/IR/TritonInterfaces.td | interfaces定义                  |


首先来看下`include/triton/Dialect/Triton/IR/TritonDialect.td`中Triton_Dialect的定义

```
def Triton_Dialect : Dialect {
  // 定义Triton Dialect的名字
  let name = "tt";
  // 定义Triton Dialect的c++命名空间
  let cppNamespace = "::mlir::triton";

  // 描述说明
  let summary = "The Triton IR in MLIR";

  let description = [{
    Triton Dialect.

    Dependent Dialects:
      * Arith:
        * addf, addi, andi, cmpf, cmpi, divf, fptosi, ...
      * Math:
        * exp, sin, cos, log, ...
      * StructuredControlFlow:
        * for, if, while, yield, condition
      * ControlFlow:
        * br, cond_br
  }];

  // 依赖的MLIR的预置Dialects
  let dependentDialects = [
    "arith::ArithDialect",     // 处理加减乘除等运算
    "math::MathDialect",       // 处理log、exp等复杂数学运算
    "scf::SCFDialect",         // 结构化控制流，保留for、if等语句
    "cf::ControlFlowDialect"   // 无结构控制流，只有条件跳转命令
  ];

  // 声明要添加的自定义函数
  let extraClassDeclaration = [{
    void registerTypes();
  }];
  
  // MLIR Dialect提供的一些属性配置
  let hasConstantMaterializer = 1;      // Triton Dialect是否支持常量的解析和生成
  let useDefaultTypePrinterParser = 1;  // Triton Dialect是否使用默认的类型打印和解析器
  let usePropertiesForAttributes = 1;   // Triton Dialect是否使用属性的属性来表示属性
}
```

根据这个td文件，TableGen可以生成出如下的类型

```c++
/*===- TableGen'erated file -------------------------------------*- C++ -*-===*\
|*                                                                            *|
|* Dialect Declarations                                                       *|
|*                                                                            *|
|* Automatically generated file, do not edit!                                 *|
|* From: TritonDialect.td                                                     *|
|*                                                                            *|
\*===----------------------------------------------------------------------===*/

namespace mlir {
namespace triton {

class TritonDialect : public ::mlir::Dialect {
  explicit TritonDialect(::mlir::MLIRContext *context);

  void initialize();
  friend class ::mlir::MLIRContext;
public:
  ~TritonDialect() override;
  static constexpr ::llvm::StringLiteral getDialectNamespace() {
    return ::llvm::StringLiteral("tt");
  }

  /// Parse a type registered to this dialect.
  ::mlir::Type parseType(::mlir::DialectAsmParser &parser) const override;

  /// Print a type registered to this dialect.
  void printType(::mlir::Type type,
                 ::mlir::DialectAsmPrinter &os) const override;

  /// Materialize a single constant operation from a given attribute value with
  /// the desired resultant type.
  ::mlir::Operation *materializeConstant(::mlir::OpBuilder &builder,
                                         ::mlir::Attribute value,
                                         ::mlir::Type type,
                                         ::mlir::Location loc) override;

    void registerTypes();
  };
} // namespace triton
} // namespace mlir
MLIR_DECLARE_EXPLICIT_TYPE_ID(::mlir::triton::TritonDialect)
```


接着是`include/triton/Dialect/Triton/IR/TritonOps.td`中定义的各种算子，算子比较多，总共有38个，还是选择一个典型的来看一下

```
//
// Op Base
//
// 所有Op的父类，继承自Op
class TT_Op<string mnemonic, list<Trait> traits = []> :
    Op<Triton_Dialect, mnemonic,
       !listconcat(traits, [TensorSizeTrait, VerifyTensorLayoutsTrait])> {
}

//
// Dot Op
//
// 继承自TT_Op
def TT_DotOp : TT_Op<"dot", [Pure,
                             DeclareOpInterfaceMethods<InferTypeOpInterface>,
                             TypesMatchWith<"result's type matches accumulator's type",
                                            "d", "c", "$_self">]> {
    // 描述说明
    let summary = "dot";

    let description = [{
        $d = matrix_multiply($a, $b) + $c. $inputPrecision describes how to exercise the TC
        when the inputs are f32. It can be one of: tf32, tf32x3, ieee.
        tf32: use TC with tf32 ops.
        tf32x3: implement the 3xTF32 trick. For more info see the pass in F32DotTC.cpp
        ieee: don't use TC, implement dot in software.
        If the GPU does not have Tensor cores or the inputs are not f32, this flag is ignored.
    }];
    
    // 参数 
    let arguments = (
      ins
      TT_TensorOrMemDesc:$a,  // 输入a
      TT_TensorOrMemDesc:$b,  // 输入b
      TT_FpIntTensor:$c,      // 输入c
      DefaultValuedAttr<TT_InputPrecisionAttr, "::mlir::triton::InputPrecision::IEEE">:$inputPrecision,
      DefaultValuedAttr<I32Attr, "0">:$maxNumImpreciseAcc
    );

    let results = (outs TT_FpIntTensor:$d);  // 输出d

    // attr-dict prints enums as integers.  To get inputPrecision printed as a
    // string, we need to specify it explicitly.
    let assemblyFormat = [{
      $a`,` $b`,` $c (`,` `inputPrecision` `=` $inputPrecision^)? attr-dict `:`
      type($a) `*` type($b) `->` type($d)
    }];
    let hasVerifier = 1;
}
```

再是属性定义`include/triton/Dialect/Triton/IR/TritonAttrDefs.td`

```c++
// Attributes for LoadOp and StoreOp
// 用于Load和Store的缓存修改器的属性
def TT_CacheModifierAttr : I32EnumAttr<
    "CacheModifier", "",
    [
        I32EnumAttrCase<"NONE", 1, "none">,  // 没有特定的缓存策略
        I32EnumAttrCase<"CA", 2, "ca">,      // 内存访问合并
        I32EnumAttrCase<"CG", 3, "cg">,      // 粗力度并行激素啊
        I32EnumAttrCase<"WB", 4, "wb">,      // 写回
        I32EnumAttrCase<"CS", 5, "cs">,      // 计算着色器
        I32EnumAttrCase<"WT", 6, "wt">,      // 写透缓存
    ]> {
    let cppNamespace = "::mlir::triton";
}
```

Types中则是定义的用于Triton IR中的类型`include/triton/Dialect/Triton/IR/TritonTypes.td`，例如浮点、布尔、整数、张量等

```c++
//
// Types
//
class TritonTypeDef<string name, string _mnemonic, list<Trait> traits = []>
    : TypeDef<Triton_Dialect, name, traits> {
    // Used by printer/parser
    let mnemonic = _mnemonic;
}

// Floating-point Type
def TT_Float : AnyTypeOf<[F8E4M3FNUZ, F8E5M2, F8E5M2FNUZ, F16, BF16, F32, F64], "floating-point">;
def TT_FloatTensor : RankedTensorOf<[TT_Float]>;
def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;

// Boolean Type
// TT_Bool -> I1
def TT_BoolTensor : RankedTensorOf<[I1]>;
def TT_BoolLike : AnyTypeOf<[I1, TT_BoolTensor]>;

// Integer Type
def TT_Int : AnyTypeOf<[I1, I8, I16, I32, I64], "integer">;
def TT_IntTensor : RankedTensorOf<[TT_Int]>;
def TT_IntLike : AnyTypeOf<[TT_Int, TT_IntTensor]>;

...
```


最后是Interface `include/triton/Dialect/Triton/IR/TritonInterfaces.td`

```c++
#ifndef TRITON_INTERFACES
#define TRITON_INTERFACES

include "mlir/IR/OpBase.td"

def TensorSizeTrait : NativeOpTrait<"TensorSizeTrait">;  // 用于指定草错具有明确定义的张量大小，它确保操作的操作数和结果具有兼容的张量大小。
def VerifyTensorLayoutsTrait : NativeOpTrait<"VerifyTensorLayoutsTrait">;  // 用于指定操作需要验证张量布局。它确保操作的操作数和结果具有兼容的张量布局。
def SameOperandsEncoding : NativeOpTrait<"SameOperandsEncoding">;  // 用于指定操作要求操作数具有相同的编码。它确保操作的操作数具有相同的编码。
def SameOperandsAndResultEncoding : NativeOpTrait<"SameOperandsAndResultEncoding">;  // 用于指定操作要求操作数和结果具有相同的编码。它确保操作的操作数和结果具有相同的编码。
def SameLoadStoreOperandsShape : NativeOpTrait<"SameLoadStoreOperandsShape">;  // 用于指定操作要求加载和存储操作数具有相同的形状。它确保操作的加载和存储操作数具有相同的形状。
def SameLoadStoreOperandsAndResultShape : NativeOpTrait<"SameLoadStoreOperandsAndResultShape">;  // 用于指定操作要求加载、存储和结果操作数具有相同的形状。它确保操作的加载、存储和结果操作数具有相同的形状。
def SameLoadStoreOperandsEncoding : NativeOpTrait<"SameLoadStoreOperandsEncoding">;  // 用于指定操作要求加载和存储操作数具有相同的编码。它确保操作的加载和存储操作数具有相同的编码。
def SameLoadStoreOperandsAndResultEncoding : NativeOpTrait<"SameLoadStoreOperandsAndResultEncoding">;  // 用于指定操作要求加载、存储和结果操作数具有相同的编码。它确保操作的加载、存储和结果操作数具有相同的编码。

#endif // TRITON_INTERFACES
```

## Triton 中端

### TritonIR转TritonGPUIR

经过前端转换之后，python已经被转换为了TritonIR。前端的工作已经结束。

开始进入中端的工作流程，对TritonIR进行优化，将TritonIR转成TritonGPUIR，再对TritonGPU进行优化。

代码逻辑上，入口函数已经在compiler.py中的compiler函数：

```python
def compile(src, target=None, options=None):
    ...
    # 根据指定的target获取一个backend，这里会返回CUDABackend
    backend = make_backend(target) 
    ...
    # 添加编译stage
    backend.add_stages(stages, options) 
    ...
    # 加载所需dialects
    ir.load_dialects(context)  
    backend.load_dialects(context)
    # 创建Triton IR
    module = src.make_ir(options, context) 
    ...
    for ext, compile_ir in list(stages.items())[first_stage:]:
        # 编译、优化各个阶段IR
        next_module = compile_ir(module, metadata) 
        ...
        module = next_module
    ...
    return CompiledKernel(src, metadata_group, hash)
```

即下面for loop的部分，它会遍历stage的内容，获取对应stage的compile函数

- stages：`{'ttir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6710>, 'ttgir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6d40>, 'llir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6dd0>, 'ptx': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6e60>, 'cubin': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6ef0>}`

这个stage compile的添加是由backend.add_stage执行的，对于不同的backend，实现不同：

- CUDABackend：
  ```python
  def add_stages(self, stages, options):
        stages["ttir"] = lambda src, metadata: self.make_ttir(src, metadata, options)
        stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, self.capability)
        stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options, self.capability)
        stages["ptx"] = lambda src, metadata: self.make_ptx(src, metadata, options, self.capability)
        stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.capability)
    ```
- HIPBackend：
  ```python
    def add_stages(self, stages, options):
        stages["ttir"] = lambda src, metadata: self.make_ttir(src, metadata, options)
        stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options)
        stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
        stages["amdgcn"] = lambda src, metadata: self.make_amdgcn(src, metadata, options)
        stages["hsaco"] = lambda src, metadata: self.make_hsaco(src, metadata, options)
    ```

以CUDABackend为例子，第一个stage对应的compile函数为CUDABackend的成员方法make_ttir，其中passes.common是对于所有ir都有用的，passes.ttir则是triton ir特有的

```python
  @staticmethod
  def make_ttir(mod, metadata, options):
      # 和llvm类似，依赖一个pass manager驱动pass的添加、执行
      pm = ir.pass_manager(mod.context)
      pm.enable_debug()

      # 添加需要的pass
      passes.common.add_inliner(pm)  # 尽可能的inline
      passes.ttir.add_rewrite_tensor_pointer(pm)  # 使用tensor指针重写load/store
      passes.ttir.add_combine(pm)  # 将ops融合
      passes.common.add_canonicalizer(pm)  # 常量折叠，转换等常规优化
      passes.ttir.add_reorder_broadcast(pm)  # 将elementwise的操作移动到broadcast之前
      passes.common.add_cse(pm)  # 公共子表达式消除
      passes.common.add_licm(pm)  # 将循环中不变的执行调整到循环外边
      passes.common.add_symbol_dce(pm)  # 消除死变量
      pm.run(mod)  # 执行pass
      return mod
```

首先来看pass_manager，它在ir.cc中被binding到了PassManager类，它并不是triton设计的，而是mlir中设计的

```c++
  py::class_<PassManager>(m, "pass_manager", py::module_local())
      .def(py::init<MLIRContext *>())
```

下面就是向pass manager中添加pass。

首先在我们看到的python添加之前，还需要做init操作，这一步在libtriton库init时就已经执行了，即PYBIND11_MODULE中的init_triton_pass

```c++
// python/src/main.cc
PYBIND11_MODULE(libtriton, m) {
  m.doc() = "Python bindings to the C++ Triton API";
  init_triton_env_vars(m);
  init_triton_ir(m.def_submodule("ir"));
  init_triton_passes(m.def_submodule("passes"));
  init_triton_interpreter(m.def_submodule("interpreter"));
  init_triton_llvm(m.def_submodule("llvm"));
  FOR_EACH_P(INIT_BACKEND, TRITON_BACKENDS_TUPLE)
}
```

init_triton_pass中会初始化所有的pass

```c++
void init_triton_passes(py::module &&m) {
  init_triton_analysis(m.def_submodule("analysis"));
  init_triton_passes_common(m.def_submodule("common"));
  init_triton_passes_convert(m.def_submodule("convert"));
  init_triton_passes_ttir(m.def_submodule("ttir"));
  init_triton_passes_ttgpuir(m.def_submodule("ttgpuir"));
  init_triton_passes_llvmir(m.def_submodule("llvmir"));
}
```

进入具体的common和ttir的函数

```c++
void init_triton_passes_common(py::module &&m) {
  using namespace mlir;
  ADD_PASS_WRAPPER_0("add_sccp", createSCCPPass);
  ADD_PASS_WRAPPER_0("add_symbol_dce", createSymbolDCEPass);
  ADD_PASS_WRAPPER_0("add_inliner", createInlinerPass);
  ADD_PASS_WRAPPER_0("add_canonicalizer", createCanonicalizerPass);
  ADD_PASS_WRAPPER_0("add_cse", createCSEPass);
  ADD_PASS_WRAPPER_0("add_licm", createLoopInvariantCodeMotionPass);
}

void init_triton_passes_ttir(py::module &&m) {
  using namespace mlir::triton;
  ADD_PASS_WRAPPER_0("add_combine", createCombineOpsPass);
  ADD_PASS_WRAPPER_0("add_reorder_broadcast", createReorderBroadcastPass);
  ADD_PASS_WRAPPER_0("add_rewrite_tensor_pointer",
                     createRewriteTensorPointerPass);
  ADD_PASS_WRAPPER_4("add_convert_to_ttgpuir",
                     createConvertTritonToTritonGPUPass, const std::string &,
                     int, int, int);
}
```

`ADD_PASS_WRAPPER_0`宏中做了具体的函数绑定，即继续回到python侧，发生`passes.ttir.add_combine(pm)`调用时pass通过addPass方法添加到pass manager中

```c++
#define ADD_PASS_WRAPPER_0(name, builder)                                      \
  m.def(name, [](mlir::PassManager &pm) { pm.addPass(builder()); })

```

builder()即为createCombineOpsPass()方法，在Passes.td中可以找到

```c++
def TritonCombineOps : Pass</*cli-arg*/"triton-combine", /*Op*/"mlir::ModuleOp"> {
  let summary = "combine ops";
  let description = [{
    dot(a, b, 0) + c => dot(a, b, c)

    addptr(addptr(ptr, idx0), idx1) => addptr(ptr, AddI(idx0, idx1))

    select(cond, load(ptrs, broadcast(cond), ???), other) =>
        load(ptrs, broadcast(cond), other)
  }];

  let constructor = "mlir::triton::createCombineOpsPass()";

  let dependentDialects = ["mlir::arith::ArithDialect"];
}
```

它的实现则是预先定义好的，不是TableGen生成的，位于lib/Dialect/Triton/Transforms/Combine.cpp中

```c++
std::unique_ptr<mlir::Pass> createCombineOpsPass() {
  return std::make_unique<CombineOpsPass>();
}
```

也就是返回CombineOpsPass对象的指针，它被添加到了Pass Manager中

```c++
class CombineOpsPass : public TritonCombineOpsBase<CombineOpsPass> {
public:
  void runOnOperation() override {
    MLIRContext *context = &getContext();
    RewritePatternSet patterns(context);
    ModuleOp m = getOperation();

    // Dot Add %{
    patterns.add<CombineDotAddIPattern>(context);
    patterns.add<CombineDotAddFPattern>(context);
    patterns.add<CombineDotAddIRevPattern>(context);
    patterns.add<CombineDotAddFRevPattern>(context);
    // %}
    patterns.add<CombineSelectMaskedLoadPattern>(context);
    patterns.add<CombineAddPtrPattern>(context);
    patterns.add<CombineBroadcastConstantPattern>(context);
    patterns.add<CombineBroadcastMulReducePattern>(context);

    if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())
      signalPassFailure();
  }
};
```

其他pass也是相同的添加方式，全部做完后，调用pm.run方法

```c++
py::class_<PassManager>(m, "pass_manager", py::module_local())
      .def(py::init<MLIRContext *>())
      .def("run", [](PassManager &self, ModuleOp &mod) {
        // pass manager this 指针，ttir module
        // TODO: maybe dump module to file and print error for better
        // diagnostics
        
        // *暂时不考虑下列环境变量设置
        auto reproducerPath =
            triton::tools::getStrEnv("TRITON_REPRODUCER_PATH");
        if (!reproducerPath.empty()) {
          auto anchorName = self.getOpAnchorName();
          auto passes = self.getPasses();
          Operation *op = mod.getOperation();
          makeReproducer(anchorName, passes, op, reproducerPath);
        }

        if (triton::tools::getBoolEnv("TRITON_ENABLE_LLVM_DEBUG")) {
          ::llvm::DebugFlag = true;
        }

        if (auto debugOnly = triton::tools::getStrEnv("TRITON_LLVM_DEBUG_ONLY");
            !debugOnly.empty()) {
          llvm::SmallVector<StringRef, 3> split;
          llvm::SmallVector<std::string, 3> storage;
          llvm::SmallVector<const char *, 3> debugTypes;

          StringRef(debugOnly.c_str()).split(split, ',');
          llvm::transform(split, std::back_inserter(debugTypes),
                          [&storage](StringRef str) {
                            // StringRefs are not always null-terminated.
                            // The purpose for this storage pattern is to
                            // produce a collection of C-strings that are.
                            storage.push_back(str.str());
                            return storage.back().c_str();
                          });

          ::llvm::DebugFlag = true;
          ::llvm::setCurrentDebugTypes(debugTypes.data(), debugTypes.size());
        }

        bool haveTiming = ::triton::tools::getBoolEnv("MLIR_ENABLE_TIMING");
        if (haveTiming) {
          self.enableTiming();
        }

        // 实际执行mlir提供的pass manager的run方法，执行刚刚添加过的所有pass
        if (failed(self.run(mod.getOperation())))
          throw std::runtime_error("PassManager::run failed");
      });
```

获取到的是一个优化后的ttir module，接下来，这个module将会作为输入，给到make_ttgir用于ttgir的生成和优化

```python
  @staticmethod
  def make_ttgir(mod, metadata, opt, capability):
      cluster_info = nvidia.ClusterInfo()
      if opt.cluster_dims is not None:
          cluster_info.clusterDimX = opt.cluster_dims[0]
          cluster_info.clusterDimY = opt.cluster_dims[1]
          cluster_info.clusterDimZ = opt.cluster_dims[2]
      # TTIR -> TTGIR
      pm = ir.pass_manager(mod.context)
      pm.enable_debug()
      passes.ttir.add_convert_to_ttgpuir(pm, f"cuda:{capability}", opt.num_warps, 32, opt.num_ctas)
      # optimize TTGIR
      passes.ttgpuir.add_coalesce(pm)
      if capability // 10 >= 8:
          passes.ttgpuir.add_f32_dot_tc(pm)
      # TODO(Qingyi): Move PlanCTAPass to the front of CoalescePass
      nvidia.passes.ttnvgpuir.add_plan_cta(pm, cluster_info)
      passes.ttgpuir.add_remove_layout_conversions(pm)
      passes.ttgpuir.add_optimize_thread_locality(pm)
      passes.ttgpuir.add_accelerate_matmul(pm)
      passes.ttgpuir.add_remove_layout_conversions(pm)
      passes.ttgpuir.add_optimize_dot_operands(pm, capability >= 80)
      passes.common.add_cse(pm)
      if capability // 10 >= 8:
          passes.ttgpuir.add_combine_tensor_select_and_if(pm)
          passes.ttgpuir.add_pipeline(pm, opt.num_stages)
      passes.ttgpuir.add_prefetch(pm)
      passes.ttgpuir.add_optimize_dot_operands(pm, capability >= 80)
      passes.ttgpuir.add_remove_layout_conversions(pm)
      passes.ttgpuir.add_reduce_data_duplication(pm)
      passes.ttgpuir.add_reorder_instructions(pm)
      passes.common.add_cse(pm)
      passes.common.add_symbol_dce(pm)
      if capability // 10 >= 9:
          nvidia.passes.ttnvgpuir.add_fence_insertion(pm)
          nvidia.passes.ttnvgpuir.add_tma_lowering(pm)
      passes.common.add_canonicalizer(pm)
      pm.run(mod)
      metadata["cluster_dims"] = (cluster_info.clusterDimX, cluster_info.clusterDimY, cluster_info.clusterDimZ)
      return mod
```

因为我们已经对这里的pass添加流程很熟悉了，所以直接跳到它的实现中

```c++

class ConvertTritonToTritonGPU
    : public ConvertTritonToTritonGPUBase<ConvertTritonToTritonGPU> {
public:
  ConvertTritonToTritonGPU() = default;
  // constructor with some parameters set explicitly.
  ConvertTritonToTritonGPU(const std::string &target, int numWarps,
                           int threadsPerWarp, int numCTAs) {
    this->numWarps = numWarps;
    this->threadsPerWarp = threadsPerWarp;
    this->numCTAs = numCTAs;
    this->target = target;
  }

  void runOnOperation() override {
    MLIRContext *context = &getContext();
    ModuleOp mod = getOperation();
    // type converter
    TritonGPUTypeConverter typeConverter(context, numWarps, threadsPerWarp,
                                         numCTAs);
    TritonGPUConversionTarget target(*context, typeConverter);
    // rewrite patterns
    RewritePatternSet patterns(context);  // 用于收集转换过程中的RewritePattern
    // add rules
    populateArithPatternsAndLegality(typeConverter, patterns, target);  // 添加arith dialect op的RewritePattern到patternset
    // 会调用patterns.add<GenericOpPattern<arith::AddIOp>表明使用GenericOpPattern处理addi
    populateMathPatternsAndLegality(typeConverter, patterns, target);  // 添加math dialect op的RewritePattern到patternset
    populateTritonPatterns(typeConverter, patterns, numCTAs);  // 添加triton dialect op的RewritePattern到patternset
    // 会调用patterns.insert<GenericOpPattern<triton::LoadOp>, GenericOpPattern<triton::StoreOp> 表明使用GenericOpPattern处理ld st
    populateSCFPatterns(typeConverter, patterns);  // 添加SCF dialect op的RewritePattern到patternset
    populateCFPatterns(typeConverter, patterns);  // 添加CFP dialect op的RewritePattern到patternset

    auto inti = llvm::APSInt(32, false);
    auto i32_ty = IntegerType::get(mod->getContext(), 32);

    mod->setAttr(
        AttrNumWarpsName,
        IntegerAttr::get(i32_ty, llvm::APInt(32, numWarps.getValue())));
    mod->setAttr(
        AttrNumThreadsPerWarp,
        IntegerAttr::get(i32_ty, llvm::APInt(32, threadsPerWarp.getValue())));

    mod->setAttr(AttrNumCTAsName,
                 IntegerAttr::get(i32_ty, llvm::APInt(32, numCTAs.getValue())));

    if (this->target.getValue().empty()) {
      mod.emitError("expected target specification to attach to the module op");
      return signalPassFailure();
    }
    mod->setAttr(AttrTargetName,
                 StringAttr::get(context, this->target.getValue()));

    // 执行op的转换
    if (failed(applyPartialConversion(mod, target, std::move(patterns))))
      return signalPassFailure();

    // update layouts
    //  broadcast src => multicast, dst => broadcasted
    // if (failed(target.refineLayouts(mod, numWarps)))
    //   return signalPassFailure();
  }
};
```

populateTritonPatterns会注册很多用于转换的pattern

```c++
void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,
                            RewritePatternSet &patterns, unsigned numCTAs) {
  MLIRContext *context = patterns.getContext();
  patterns.insert< // TODO: view should have custom pattern that views the
                   // layout
      GenericOpPattern<triton::AdvanceOp>,
      GenericOpPattern<triton::MakeTensorPtrOp>,
      GenericOpPattern<triton::ReshapeOp>, GenericOpPattern<triton::BitcastOp>,
      GenericOpPattern<triton::FpToFpOp>, GenericOpPattern<triton::IntToPtrOp>,
      GenericOpPattern<triton::PtrToIntOp>, GenericOpPattern<triton::SplatOp>,
      TritonBroadcastPattern, GenericOpPattern<triton::AddPtrOp>,
      TritonCatPattern, TritonJoinOpPattern, TritonSplitOpPattern,
      GenericOpPattern<triton::ClampFOp>,
      GenericOpPattern<triton::PreciseSqrtOp>,
      GenericOpPattern<triton::PreciseDivFOp>,
      GenericOpPattern<triton::MulhiUIOp>,
      GenericOpPattern<triton::ElementwiseInlineAsmOp>, TritonReducePattern,
      GenericOpPattern<triton::ReduceReturnOp>, TritonScanPattern,
      GenericOpPattern<triton::ScanReturnOp>,
      GenericOpPattern<triton::MakeRangeOp>, TritonExpandDimsPattern,
      TritonTransPattern, TritonDotPattern, GenericOpPattern<triton::LoadOp>,
      GenericOpPattern<triton::StoreOp>, GenericOpPattern<triton::HistogramOp>,
      GenericOpPattern<triton::ExternElementwiseOp>,
      GenericOpPattern<triton::PrintOp>, GenericOpPattern<triton::AssertOp>,
      GenericOpPattern<triton::AtomicCASOp>,
      GenericOpPattern<triton::AtomicRMWOp>, GenericOpPattern<ReturnOp>,
      GenericOpPattern<triton::ExperimentalDescriptorLoadOp>,
      GenericOpPattern<triton::ExperimentalDescriptorStoreOp>,
      GenericOpPattern<triton::CallOp>, TritonFuncOpPattern>(typeConverter,
                                                             context);
}
```

这个过程被称为conversion（或者rewrite）过程，TritonGPUTypeConverter用于类型的转换，TritonGPUConversionTarget用于op的转换。然后通过populate函数讲一些dialect的节点对应的转换pattern记录在patternset内，用来提供节点的转换方法。最后applyPartialConversion方法执行op的转换。

applyPartialConversion这是mlir中提供的标准接口，位于`mlir/lib/Transforms/Utils/DialectConversion.cpp`中

```c++
LogicalResult
mlir::applyPartialConversion(ArrayRef<Operation *> ops,
                             const ConversionTarget &target,
                             const FrozenRewritePatternSet &patterns,
                             DenseSet<Operation *> *unconvertedOps) {
  OperationConverter opConverter(target, patterns, OpConversionMode::Partial,
                                 unconvertedOps);
  return opConverter.convertOperations(ops);
}
```

进一步调用同文件中的convertOperations方法

```c++
LogicalResult OperationConverter::convertOperations(
    ArrayRef<Operation *> ops,
    function_ref<void(Diagnostic &)> notifyCallback) {
  if (ops.empty())
    return success();
  const ConversionTarget &target = opLegalizer.getTarget();

  // Compute the set of operations and blocks to convert.
  SmallVector<Operation *> toConvert;
  for (auto *op : ops) {
    op->walk<WalkOrder::PreOrder, ForwardDominanceIterator<>>(
        [&](Operation *op) {
          toConvert.push_back(op);
          auto legalityInfo = target.isLegal(op); // 判断当前op是否合法，即在ttgir中依然可以使用的op表示
          if (legalityInfo && legalityInfo->isRecursivelyLegal)
            return WalkResult::skip();
          return WalkResult::advance();
        });
  }

  // Convert each operation and discard rewrites on failure.
  ConversionPatternRewriter rewriter(ops.front()->getContext());
  ConversionPatternRewriterImpl &rewriterImpl = rewriter.getImpl();
  for (auto *op : toConvert)
    if (failed(convert(rewriter, op))) // 转换op
      return rewriterImpl.discardRewrites(), failure();
  ......
  return success();
}
```

判断op是否合法是由ConversionTarget的isLegal方法负责的，它来自于opLegalizer的getTarget()方法，而opLegalizer的实例化则是上一步中的opConverter实例化时被同步实例化的。它返回的target实际上就是applyPartialConversion中传入的ConversionTarget，即TritonGPUConversionTarget。

对于哪些op合法，哪些op不合法，在TritonGPUConversionTarget中会进行指定

```c++
TritonGPUConversionTarget::TritonGPUConversionTarget(
    MLIRContext &context, TritonGPUTypeConverter &typeConverter)
    : ConversionTarget(context) {
  // TODO: we should also verify ops of TritonGPUDialect
  // TritonGPUDialect中的所有OP都合法
  addLegalDialect<triton::gpu::TritonGPUDialect>();  

  // Some ops from SCF are illegal
  // 以下四个scf中的OP都不合法
  addIllegalOp<scf::ExecuteRegionOp, scf::ParallelOp, scf::ReduceOp,
               scf::ReduceReturnOp>();
  
  // 以下四个OP在某些情况下合法
  addDynamicallyLegalDialect<arith::ArithDialect, math::MathDialect,
                             triton::TritonDialect, cf::ControlFlowDialect,
                             scf::SCFDialect>([&](Operation *op) {
    bool hasLegalRegions = true;
    for (auto &region : op->getRegions()) {
      // 由typeConverter来做类型判断是否合法
      hasLegalRegions = hasLegalRegions && typeConverter.isLegal(&region);
    }
    if (hasLegalRegions && typeConverter.isLegal(op)) {
      return true;
    }
    return false;
  });

  // We have requirements for the data layouts

  addDynamicallyLegalOp<triton::DotOp>([](triton::DotOp dotOp) -> bool {
    Attribute aEncoding =
        cast<RankedTensorType>(dotOp.getA().getType()).getEncoding();
    Attribute bEncoding =
        cast<RankedTensorType>(dotOp.getB().getType()).getEncoding();
    if (aEncoding && isa<triton::gpu::DotOperandEncodingAttr>(aEncoding) &&
        bEncoding && isa<triton::gpu::DotOperandEncodingAttr>(bEncoding))
      return true;
    return false;
  });
}
```

这里用到的是TritonGPUTypeConverter，在它的构造函数中声明了RankedTensorType和triton::PointerType两个类型需要转换，

```c++

TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,
                                               int numWarps, int threadsPerWarp,
                                               int numCTAs)
    : context(context), numWarps(numWarps), threadsPerWarp(threadsPerWarp),
      numCTAs(numCTAs) {

  // 当前类型合法直接返回
  addConversion([](Type type) { return type; });

  // Add encoding for tensor
  addConversion([this](RankedTensorType tensorType) -> RankedTensorType {
    // types with encoding are already in the right format
    // TODO: check for layout encodings more specifically
    if (tensorType.getEncoding())
      return tensorType;
    ArrayRef<int64_t> shape = tensorType.getShape();
    triton::gpu::BlockedEncodingAttr encoding =
        getDefaultBlockedEncoding(this->context, shape, this->numWarps,
                                  this->threadsPerWarp, this->numCTAs);
    return RankedTensorType::get(shape, tensorType.getElementType(), encoding);
  });

  // Add encoding for tensor pointer
  addConversion([this](triton::PointerType ptrType) -> triton::PointerType {
    // Check whether tensor pointer `tt.ptr<tensor<>>`
    auto pointeeTensorType =
        dyn_cast<RankedTensorType>(ptrType.getPointeeType());
    if (pointeeTensorType == nullptr)
      return ptrType;

    // Add layout into the tensor
    auto convertedTensorType = convertType(pointeeTensorType);
    return triton::PointerType::get(convertedTensorType,
                                    ptrType.getAddressSpace());
  });
  ...
}
```

经过合法性判断后，来到convert(rewriter, op)这一步

```c++
LogicalResult OperationConverter::convert(ConversionPatternRewriter &rewriter,
                                          Operation *op) {
  // Legalize the given operation.
  if (failed(opLegalizer.legalize(op, rewriter))) {
    if (mode == OpConversionMode::Full)
      return op->emitError()
             << "failed to legalize operation '" << op->getName() << "'";
    if (mode == OpConversionMode::Partial) {
      if (opLegalizer.isIllegal(op))
        return op->emitError()
               << "failed to legalize operation '" << op->getName()
               << "' that was explicitly marked illegal";
      if (trackedOps)
        trackedOps->insert(op);
    }
  } else if (mode == OpConversionMode::Analysis) {
    trackedOps->insert(op);
  }
  return success();
}
```

而它内部主要发挥作用的是legalize方法

```c++
LogicalResult
OperationLegalizer::legalize(Operation *op,
                             ConversionPatternRewriter &rewriter) {
  ...
  // If the operation isn't legal, try to fold it in-place.
  // 常量折叠
  if (succeeded(legalizeWithFold(op, rewriter))) {
    LLVM_DEBUG({
      logSuccess(logger, "operation was folded");
      logger.startLine() << logLineComment;
    });
    return success();
  }

  // Otherwise, we need to apply a legalization pattern to this operation.
  // 尝试具体的pattern
  if (succeeded(legalizeWithPattern(op, rewriter))) {
    LLVM_DEBUG({
      logSuccess(logger, "");
      logger.startLine() << logLineComment;
    });
    return success();
  }
  ...
  return failure();
}
```

它的实现中

```c++
LogicalResult
OperationLegalizer::legalizeWithPattern(Operation *op,
                                        ConversionPatternRewriter &rewriter) {
  ...

  // Try to match and rewrite a pattern on this operation.
  return applicator.matchAndRewrite(op, rewriter, canApply, onFailure,
                                    onSuccess);
}
```

`applicator.matchAndRewrite`调用的是PatternApplicator中的方法

```c++
LogicalResult PatternApplicator::matchAndRewrite (...) {
  const Pattern *bestPattern = nullptr;
  // Find the next pattern with the highest benefit.
  ...
  const auto *pattern = static_cast<const RewritePattern *>(bestPattern);
  result = pattern->matchAndRewrite(op, rewriter);
  ...
}
```

根据当前op的name找到所有的候选pattern，然后会经过一个cost model计算得到当前pattern的benefits指标作为选择的依据，选出最收益最高的Pattern调用它的matchAndRewrite方法

在TritonGPUDialect中的用于TritonIR到TritonGPU IR转换的pattern，在TritonToTritonGPUPasses.cpp中，以populateTritonPatterns为例，它里面用到了大量的GenericOpPattern，比如我们的vector add例子中用到的laod、store：

```c++
GenericOpPattern<triton::LoadOp>, GenericOpPattern<triton::StoreOp>
```

在同文件中，找到GenericOpPattern的实现，一目了然，它实际上直接用了传入的类型，没有做任何的转换，就是原本的op输出了。所以对于GenericOpPattern，我们的IR虽然从TritonIR变为了TritGPUIR，但是，应该不会发生很大的变化。

```c++
// TritonToTritonGPUPass.cpp
template <class Op> struct GenericOpPattern : public OpConversionPattern<Op> {
  using OpConversionPattern<Op>::OpConversionPattern;

  LogicalResult
  matchAndRewrite(Op op, typename Op::Adaptor adaptor,
                  ConversionPatternRewriter &rewriter) const override {
    SmallVector<Type> retTypes;
    if (failed(this->getTypeConverter()->convertTypes(op->getResultTypes(),
                                                      retTypes)))
      return failure();
    rewriter.replaceOpWithNewOp<Op>(op, retTypes, adaptor.getOperands(),
                                    op->getAttrs());

    return success();
  }
};
```

到这里add_convert_to_ttgpuir的工作就全部结束了，它实际上就是两步：

1. 判断op是否合法；
2. 如果合法不处理，如果不合法则通过RewritePatternSet中添加的RewritePattern的matchAndRewrite接口进行转换；

跳过make_ttgir中的胜于pass，执行完pm.run(mod)后，获取到的vector add的ir如下所示，一方面我们这个例子过于简单，另一个方面我们看到大量使用GeneraicOpPattern的情况，ir实际上没有发生大的变换，只是增加了平台相关的module attribute

```c++
#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#loc = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0)
module attributes {"triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.target = "cuda:80", "triton_gpu.threads-per-warp" = 32 : i32} {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0)) attributes {noinline = false} {
    %c1024_i32 = arith.constant 1024 : i32 loc(#loc1)
    %0 = tt.get_program_id x : i32 loc(#loc2)
    %1 = arith.muli %0, %c1024_i32 : i32 loc(#loc3)
    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #blocked> loc(#loc4)
    %3 = tt.splat %1 : i32 -> tensor<1024xi32, #blocked> loc(#loc5)
    %4 = arith.addi %3, %2 : tensor<1024xi32, #blocked> loc(#loc5)
    %5 = tt.splat %arg3 : i32 -> tensor<1024xi32, #blocked> loc(#loc6)
    %6 = arith.cmpi slt, %4, %5 : tensor<1024xi32, #blocked> loc(#loc6)
    %7 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc7)
    %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32>, #blocked>, tensor<1024xi32, #blocked> loc(#loc7)
    %9 = tt.load %8, %6 : tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc8)
    %10 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc9)
    %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr<f32>, #blocked>, tensor<1024xi32, #blocked> loc(#loc9)
    %12 = tt.load %11, %6 : tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc10)
    %13 = arith.addf %9, %12 : tensor<1024xf32, #blocked> loc(#loc11)
    %14 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc12)
    %15 = tt.addptr %14, %4 : tensor<1024x!tt.ptr<f32>, #blocked>, tensor<1024xi32, #blocked> loc(#loc12)
    tt.store %15, %13, %6 : tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc13)
    tt.return loc(#loc14)
  } loc(#loc)
} loc(#loc)
#loc1 = loc(unknown)
#loc2 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":27:24)
#loc3 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":32:24)
#loc4 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":33:41)
#loc5 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":33:28)
#loc6 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":35:21)
#loc7 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":38:24)
#loc8 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":38:16)
#loc9 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":39:24)
#loc10 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":39:16)
#loc11 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":40:17)
#loc12 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:26)
#loc13 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:35)
#loc14 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:4)
```

### TritonGPUDialect

和TritonDialect定义类似

```c++
#ifndef TRITONGPU_DIALECT
#define TRITONGPU_DIALECT

include "mlir/IR/OpBase.td"

def TritonGPU_Dialect : Dialect {
  let name = "triton_gpu";

  let cppNamespace = "::mlir::triton::gpu";

  let hasOperationAttrVerify = 1;

  let description = [{
    Triton GPU Dialect.
  }];

  let dependentDialects = [
    "triton::TritonDialect",  // 将TritonDialect作为依赖引入了，即可以在TritionGPUDialect中使用TritonDialect
    "mlir::gpu::GPUDialect",
    "tensor::TensorDialect",
  ];

  let extraClassDeclaration = [{
    static std::string getNumWarpsAttrName() { return "triton_gpu.num-warps"; }
    static int getNumWarps(ModuleOp mod) {
      if (!mod->hasAttr("triton_gpu.num-warps"))
        llvm::report_fatal_error(
            "TritonGPU module should contain a triton_gpu.num-warps attribute");
      return cast<IntegerAttr>(mod->getAttr("triton_gpu.num-warps")).getInt();
    }
    static int getNumCTAs(ModuleOp mod) {
      if (!mod->hasAttr("triton_gpu.num-ctas"))
        return 1;
      return cast<IntegerAttr>(mod->getAttr("triton_gpu.num-ctas")).getInt();
    }
    void registerTypes();

    static std::string getThreadsPerWarpAttrName() { return "triton_gpu.threads-per-warp"; }

    static int getThreadsPerWarp(ModuleOp mod) {
      Attribute threadsPerWarp = mod->getDiscardableAttr("triton_gpu.threads-per-warp");
      if(!threadsPerWarp) {
        return 32;
      }
      return cast<IntegerAttr>(threadsPerWarp).getInt();
    }
  }];

  let useDefaultTypePrinterParser = 1;
  let useDefaultAttributePrinterParser = 1;
  let usePropertiesForAttributes = 1;
}

#endif
```

根据上面的情况，其实已经可以推测，TritonGPUDialect并没有引入很多新的Ops，而是使用了TritionIR中的OP。

TritonGPUOps.td中看，引入了9个op，类似：

```c++
def TTG_LocalAllocOp : TTG_Op<"local_alloc", [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
  let summary = "allocate tensor";
  let description = [{
    This operation allocates buffer in shared memory and return a descriptor
    containing the address and a view of the buffer.

    Explicitly deallocating a buffer is optional; see local_dealloc.
  }];
  let arguments = (ins Optional<TT_Tensor>:$src);

  let assemblyFormat = [{$src attr-dict `:` functional-type(operands, results)}];

  let results = (outs TT_MemDescType:$result);
}
```

## Triton 后端

最后看一下后端，从TritonGPUIR生成LLVM IR，再生成PTX最后使用ptxas编译为cubin，对应到add_stages中的后三行

```c++
def add_stages(self, stages, options):
    stages["ttir"] = lambda src, metadata: self.make_ttir(src, metadata, options)
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, self.capability)
    stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options, self.capability)
    stages["ptx"] = lambda src, metadata: self.make_ptx(src, metadata, options, self.capability)
    stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.capability)
```

逐个来看一下，首先是make_llir，从它的注释中也能看出，这一步又被分为两个步骤：

1. TritonGPU IR生成LLVM IR（MLIR）：使用了各种converter和pass将各种Dialect转换为LLVM Dialect
2. LLVM IR（MLIR）生成真正的LLVM IR：使用LLVM提供的标准接口将LLVM IR（MLIR）转换为LLVM IR

```python
@staticmethod
def make_llir(src, metadata, options, capability):
    # warp-specialization mutates num_warps
    num_warp_groups = src.get_int_attr("triton_gpu.num-warp-groups-per-cta")
    if num_warp_groups is not None:
        metadata["num_warps"] *= num_warp_groups
    mod = src
    # TritonGPU -> LLVM-IR (MLIR)
    pm = ir.pass_manager(mod.context)
    pm.enable_debug()
    nvidia.passes.ttgpuir.add_decompose_unsupported_conversions(pm)
    passes.ttgpuir.add_combine_tensor_select_and_if(pm)
    passes.convert.add_scf_to_cf(pm)
    passes.convert.add_index_to_llvmir(pm)
    passes.ttgpuir.add_allocate_shared_memory(pm)
    nvidia.passes.ttgpuir.add_to_llvmir(pm, capability)
    nvidia.passes.ttnvgpuir.add_nvgpu_to_llvm(pm)
    passes.convert.add_arith_to_llvmir(pm)
    passes.common.add_canonicalizer(pm)
    passes.common.add_cse(pm)
    passes.common.add_symbol_dce(pm)
    if os.environ.get("TRITON_DISABLE_LINE_INFO", "0") == "0":
        passes.llvmir.add_di_scope(pm)
    pm.run(mod)
    # LLVM-IR (MLIR) -> LLVM-IR (LLVM)
    llvm.init_targets()
    context = llvm.context()
    llvm_mod = llvm.to_module(mod, context)
    nvidia.set_nvvm_reflect_ftz(llvm_mod)

    # Set maxnreg on all kernels, if it was provided.
    if options.maxnreg is not None:
        for k in llvm_mod.get_functions():
            if not k.is_declaration() and k.is_external_linkage():
                k.set_nvvm_maxnreg(options.maxnreg)

    if options.extern_libs:
        paths = [path for (name, path) in options.extern_libs]
        llvm.link_extern_libs(llvm_mod, paths)

    llvm.optimize_module(llvm_mod, llvm.OPTIMIZE_O3)

    # Get some metadata
    metadata["shared"] = src.get_int_attr("triton_gpu.shared")
    ret = str(llvm_mod)
    del llvm_mod
    del context
    return ret
```

### TritonGPUIR生成LLVM IR(MLIR)

TritonGPU IR生成LLVM IR又被分为了两个阶段，首先第一阶段由TritonGPU IR生成LLVM IR(MLIR)

```python
pm = ir.pass_manager(mod.context)
pm.enable_debug()
nvidia.passes.ttgpuir.add_decompose_unsupported_conversions(pm)
passes.ttgpuir.add_combine_tensor_select_and_if(pm)
passes.convert.add_scf_to_cf(pm)
passes.convert.add_index_to_llvmir(pm)
passes.ttgpuir.add_allocate_shared_memory(pm)
nvidia.passes.ttgpuir.add_to_llvmir(pm, capability)
nvidia.passes.ttnvgpuir.add_nvgpu_to_llvm(pm)
passes.convert.add_arith_to_llvmir(pm)
passes.common.add_canonicalizer(pm)
passes.common.add_cse(pm)
passes.common.add_symbol_dce(pm)
if os.environ.get("TRITON_DISABLE_LINE_INFO", "0") == "0":
    passes.llvmir.add_di_scope(pm)
pm.run(mod)
```

对于triton定义的dialect，通过`nvidia.passes.ttgpuir.add_to_llvmir(pm, capability)`来实现转换

```c++
  m.def("add_to_llvmir", [](mlir::PassManager &pm, int32_t capability) {
    pm.addPass(mlir::triton::createConvertTritonGPUToLLVMPass(capability));
  });
```

createConvertTritonGPUToLLVMPass创建了一个ConvertTritonGPUToLLVM对象，具体的操作都在它的runOnOperation中

```c++
struct ConvertTritonGPUToLLVM
    : public triton::impl::ConvertTritonGPUToLLVMBase<ConvertTritonGPUToLLVM> {

  ...

  void runOnOperation() override {
    MLIRContext *context = &getContext();
    ModuleOp mod = getOperation();

    mlir::LowerToLLVMOptions option(context);
    option.overrideIndexBitwidth(32);
    TritonGPUToLLVMTypeConverter typeConverter(context, option);
    TritonLLVMConversionTarget convTarget(*context);
    int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);
    int numCTAs = triton::gpu::TritonGPUDialect::getNumCTAs(mod);
    int threadsPerWarp = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);

    // Allocate shared memory and set barrier
    ModuleAllocation allocation(mod);
    ModuleMembarAnalysis membarPass(&allocation);
    membarPass.run();

    // Lower functions
    {
      mlir::LowerToLLVMOptions option(context);
      TritonGPUToLLVMTypeConverter typeConverter(context, option);
      TritonLLVMFunctionConversionTarget funcTarget(*context);
      RewritePatternSet funcPatterns(context);
      mlir::triton::populateFuncOpConversionPattern(
          typeConverter, funcPatterns, numWarps, patternBenefitDefault);
      mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,
                                                            funcPatterns);
      if (failed(
              applyPartialConversion(mod, funcTarget, std::move(funcPatterns))))
        return signalPassFailure();
    }

    // initSharedMemory is run before the conversion of call and ret ops,
    // because the call op has to know the shared memory base address of each
    // function
    initSharedMemory(typeConverter);
    ModuleAxisInfoAnalysis axisInfoAnalysis(mod);
    OpBuilder::InsertPoint indexInsertPoint;

    RewritePatternSet patterns(context);
    TargetInfo targetInfo(computeCapability);
    int benefit = patternBenefitPrioritizeOverLLVMConversions;
    mlir::triton::NVIDIA::populateConvertLayoutOpToLLVMOptimizedPatterns(
        typeConverter, targetInfo, patterns,
        patternBenefitConvertLayoutOptimizedPattern);
    mlir::triton::NVIDIA::populateConvertLayoutOpToLLVMPatterns(
        typeConverter, targetInfo, patterns, benefit);
    populateDotOpToLLVMPatterns(typeConverter, patterns, benefit);
    populateElementwiseOpToLLVMPatterns(typeConverter, patterns,
                                        axisInfoAnalysis, computeCapability,
                                        targetInfo, benefit);
    populateClampFOpToLLVMPattern(typeConverter, patterns, axisInfoAnalysis,
                                  computeCapability,
                                  patternBenefitClampOptimizedPattern);
    populateLoadStoreOpToLLVMPatterns(typeConverter, targetInfo, patterns,
                                      axisInfoAnalysis, benefit);
    mlir::triton::populateReduceOpToLLVMPatterns(typeConverter, patterns,
                                                 targetInfo, benefit);
    mlir::triton::populateScanOpToLLVMPatterns(typeConverter, patterns,
                                               targetInfo, benefit);
    populateBarrierOpToLLVMPatterns(typeConverter, patterns, benefit);
    populateTensorPtrOpsToLLVMPatterns(typeConverter, patterns, benefit);
    populateClusterOpsToLLVMPatterns(typeConverter, patterns, benefit);
    mlir::triton::populateHistogramOpToLLVMPatterns(typeConverter, patterns,
                                                    targetInfo, benefit);
    mlir::triton::populatePrintOpToLLVMPattern(typeConverter, patterns,
                                               targetInfo, benefit);
    mlir::triton::populateControlFlowOpToLLVMPattern(typeConverter, patterns,
                                                     benefit);
    mlir::triton::NVIDIA::populateSPMDOpToLLVMPattern(typeConverter, patterns,
                                                      benefit);
    mlir::triton::populateSPMDOpToLLVMPattern(typeConverter, patterns,
                                              targetInfo, benefit);
    // TODO(thomas): this should probably be done in a separate step to not
    // interfere with our own lowering of arith ops. Add arith/math's patterns
    // to help convert scalar expression to LLVM.
    mlir::arith::populateArithToLLVMConversionPatterns(typeConverter, patterns);
    mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);
    mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);
    mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,
                                                          patterns);
    mlir::triton::populateViewOpToLLVMPatterns(typeConverter, patterns,
                                               benefit);
    mlir::triton::populateAssertOpToLLVMPattern(typeConverter, patterns,
                                                targetInfo, benefit);
    mlir::triton::populateMemoryOpToLLVMPattern(typeConverter, targetInfo,
                                                patterns, benefit);
    mlir::triton::populateMakeRangeOpToLLVMPattern(typeConverter, targetInfo,
                                                   patterns, benefit);
    if (failed(applyPartialConversion(mod, convTarget, std::move(patterns))))
      return signalPassFailure();

    // Fold CTAId when there is only 1 CTA.
    if (numCTAs == 1) {
      mod.walk([](triton::nvgpu::ClusterCTAIdOp id) {
        OpBuilder b(id);
        Value zero = LLVM::createConstantI32(id->getLoc(), b, 0);
        id.replaceAllUsesWith(zero);
      });
    }
  }
}
```

和TritonIR转TritonGPUIR相似，也是一系列的populateXXXXPatterns + 最后的applyPartialConversion，根据前面的疏离，我们知道，这里核心是TritonLLVMConversionTarget convTarget。

TritonLLVMConversionTarget的构造函数中增加了合法和非法的Dialect，其中非法的里面就有triton::TritonDialect和triton::gpu::TritonGPUDialect，而LLVM::LLVMDialect是合法的。

```c++
class TritonLLVMConversionTarget : public ConversionTarget {
public:
  explicit TritonLLVMConversionTarget(MLIRContext &ctx)
      : ConversionTarget(ctx) {
    // 合法
    addLegalDialect<LLVM::LLVMDialect>();
    addLegalDialect<ROCDL::ROCDLDialect>();
    addLegalDialect<mlir::scf::SCFDialect>();
    // 非法
    addIllegalDialect<triton::TritonDialect>();
    addIllegalDialect<triton::gpu::TritonGPUDialect>();
    addIllegalDialect<triton::nvidia_gpu::TritonNvidiaGPUDialect>();
    addIllegalDialect<mlir::gpu::GPUDialect>();
    addLegalOp<mlir::UnrealizedConversionCastOp>();
  }
};
```

TypeConverter中也增加了更多的类型转换

```c++
TritonGPUToLLVMTypeConverter::TritonGPUToLLVMTypeConverter(
    MLIRContext *ctx, LowerToLLVMOptions &option,
    const DataLayoutAnalysis *analysis)
    : LLVMTypeConverter(ctx, option, analysis) {
  addConversion([&](triton::PointerType type) -> std::optional<Type> {
    return convertTritonPointerType(type);
  });
  addConversion([&](RankedTensorType type) -> std::optional<Type> {
    return convertTritonTensorType(type);
  });
  addConversion([&](MemDescType type) -> std::optional<Type> {
    return convertMemDescType(type);
  });
  addConversion([&](triton::gpu::AsyncTokenType type) -> std::optional<Type> {
    return convertAsyncToken(type);
  });
  addConversion([&](mlir::Float8E4M3FNUZType type) -> std::optional<Type> {
    return IntegerType::get(type.getContext(), 8);
  });
  addConversion([&](mlir::Float8E5M2Type type) -> std::optional<Type> {
    return IntegerType::get(type.getContext(), 8);
  });
  addConversion([&](mlir::Float8E5M2FNUZType type) -> std::optional<Type> {
    return IntegerType::get(type.getContext(), 8);
  });
  // Internally store bfloat16 as int16
  addConversion([&](BFloat16Type type) -> std::optional<Type> {
    return IntegerType::get(type.getContext(), 16);
  });
}
```

指令转换部分看具体的populateXXXXPattern的实现，以populateLoadStoreOpToLLVMPatterns为例子，已经不再是GenericOpConversion了

```c++
void mlir::triton::NVIDIA::populateLoadStoreOpToLLVMPatterns(
    LLVMTypeConverter &typeConverter, const TargetInfo &targetInfo,
    RewritePatternSet &patterns, ModuleAxisInfoAnalysis &axisInfoAnalysis,
    PatternBenefit benefit) {
  patterns.add<AsyncCopyGlobalToLocalOpConversion, AtomicCASOpConversion,
               AtomicRMWOpConversion, LoadOpConversion, StoreOpConversion>(
      typeConverter, targetInfo, axisInfoAnalysis, benefit);
  patterns.add<AsyncCommitGroupOpConversion>(typeConverter, benefit);
  patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);
  patterns.add<AsyncTMACopyGlobalToLocalOpConversion,
               AsyncTMACopyLocalToGlobalOpConversion, TMAStoreWaitConversion>(
      typeConverter, benefit);
}
```

再拿出LoadOpConversion为例子，看它的matchAndRewrite方法，保留核心代码，能看出它被转为了ptx中的ld指令

```c++
struct LoadOpConversion : public ConvertOpToLLVMPattern<triton::LoadOp>,
                          public LoadStoreConversionBase {
  LogicalResult
  matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,
                  ConversionPatternRewriter &rewriter) const override {
      ...
            // Define the instruction opcode
      auto &ld = ptxBuilder.create<>("ld")
                     ->o("volatile", op.getIsVolatile())
                     .global()
                     .o("ca", op.getCache() == triton::CacheModifier::CA)
                     .o("cg", op.getCache() == triton::CacheModifier::CG)
                     .o("L1::evict_first",
                        op.getEvict() == triton::EvictionPolicy::EVICT_FIRST)
                     .o("L1::evict_last",
                        op.getEvict() == triton::EvictionPolicy::EVICT_LAST)
                     .o("L1::cache_hint", hasL2EvictPolicy)
                     .v(nWords)
                     .b(width);
      ...
  }
}
```

对于elementwise指令则直接替换为LLVM和math的指令

```c++

void mlir::triton::populateElementwiseOpToLLVMPatterns(
    LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,
    ModuleAxisInfoAnalysis &axisInfoAnalysis, const TargetInfoBase &targetInfo,
    PatternBenefit benefit) {
#define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \
  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(                       \
      typeConverter, axisInfoAnalysis, benefit);

  POPULATE_UNARY_OP(arith::TruncIOp, LLVM::TruncOp)
  POPULATE_UNARY_OP(arith::ExtSIOp, LLVM::SExtOp)
  POPULATE_UNARY_OP(arith::ExtUIOp, LLVM::ZExtOp)
  POPULATE_UNARY_OP(arith::FPToUIOp, LLVM::FPToUIOp)
  POPULATE_UNARY_OP(arith::UIToFPOp, LLVM::UIToFPOp)
  POPULATE_UNARY_OP(math::FloorOp, math::FloorOp)
  POPULATE_UNARY_OP(math::CeilOp, math::CeilOp)
  POPULATE_UNARY_OP(math::LogOp, math::LogOp)
  POPULATE_UNARY_OP(math::Log2Op, math::Log2Op)
  POPULATE_UNARY_OP(math::CosOp, math::CosOp)
  POPULATE_UNARY_OP(math::SinOp, math::SinOp)
  POPULATE_UNARY_OP(math::SqrtOp, math::SqrtOp)
  POPULATE_UNARY_OP(math::RsqrtOp, math::RsqrtOp)
  POPULATE_UNARY_OP(math::ExpOp, math::ExpOp)
  POPULATE_UNARY_OP(math::Exp2Op, math::Exp2Op)
  POPULATE_UNARY_OP(math::ErfOp, math::ErfOp)
  POPULATE_UNARY_OP(triton::BitcastOp, LLVM::BitcastOp)
  POPULATE_UNARY_OP(triton::IntToPtrOp, LLVM::IntToPtrOp)
  POPULATE_UNARY_OP(triton::PtrToIntOp, LLVM::PtrToIntOp)
#undef POPULATE_UNARY_OP

#define POPULATE_BINARY_OP(SRC_OP, DST_OP)                                     \
  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(                       \
      typeConverter, axisInfoAnalysis, benefit);

  POPULATE_BINARY_OP(arith::SubIOp, LLVM::SubOp) // -
  POPULATE_BINARY_OP(arith::AddIOp, LLVM::AddOp) // +
  POPULATE_BINARY_OP(arith::MulIOp, LLVM::MulOp) // *
  POPULATE_BINARY_OP(arith::DivSIOp, LLVM::SDivOp)
  POPULATE_BINARY_OP(arith::DivUIOp, LLVM::UDivOp)
  POPULATE_BINARY_OP(arith::RemFOp, LLVM::FRemOp) // %
  POPULATE_BINARY_OP(arith::RemSIOp, LLVM::SRemOp)
  POPULATE_BINARY_OP(arith::RemUIOp, LLVM::URemOp)
  POPULATE_BINARY_OP(arith::AndIOp, LLVM::AndOp)   // &
  POPULATE_BINARY_OP(arith::OrIOp, LLVM::OrOp)     // |
  POPULATE_BINARY_OP(arith::XOrIOp, LLVM::XOrOp)   // ^
  POPULATE_BINARY_OP(arith::ShLIOp, LLVM::ShlOp)   // <<
  POPULATE_BINARY_OP(arith::ShRSIOp, LLVM::AShrOp) // >>
  POPULATE_BINARY_OP(arith::ShRUIOp, LLVM::LShrOp) // >>
  // fmin (return non-NaN if either op is non-NaN)
  POPULATE_BINARY_OP(arith::MinNumFOp, LLVM::MinNumOp)
  // fmax (return non-NaN if either op is non-NaN)
  POPULATE_BINARY_OP(arith::MaxNumFOp, LLVM::MaxNumOp)
  POPULATE_BINARY_OP(arith::MinSIOp, LLVM::SMinOp) // smin
  POPULATE_BINARY_OP(arith::MaxSIOp, LLVM::SMaxOp) // smax
  POPULATE_BINARY_OP(arith::MinUIOp, LLVM::UMinOp) // umin
  POPULATE_BINARY_OP(arith::MaxUIOp, LLVM::UMaxOp) // umax
#undef POPULATE_BINARY_OP

  patterns.add<ElementwiseOpConversion<math::FmaOp, LLVM::FMAOp>>(
      typeConverter, axisInfoAnalysis, benefit);

  patterns.add<AddPtrOpConversion>(typeConverter, benefit);
  patterns.add<CmpIOpConversion>(typeConverter, axisInfoAnalysis, benefit);
  patterns.add<CmpFOpConversion>(typeConverter, axisInfoAnalysis, benefit);
  patterns.add<MulhiUIOpConversion>(typeConverter, axisInfoAnalysis, targetInfo,
                                    benefit);
  patterns.add<ExternElementwiseOpConversion>(typeConverter, axisInfoAnalysis,
                                              benefit);
  patterns.add<ElementwiseInlineAsmOpConversion>(typeConverter, benefit);
  patterns.add<AbsIOpConversion>(typeConverter, axisInfoAnalysis, benefit);
  patterns.add<AbsFOpConversion>(typeConverter, axisInfoAnalysis, benefit);
  patterns.add<IndexCastOpLowering>(typeConverter, axisInfoAnalysis, benefit);
  patterns.add<SelectOpConversion>(typeConverter, axisInfoAnalysis, benefit);
}
```

当所有的pattern执行完即可获取到LLVM IR（MLIR）如下：

```c
#loc = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0)
module attributes {"triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, triton_gpu.target = "cuda:80", "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.mlir.global external @global_smem() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8> loc(#loc)
  llvm.func @add_kernel(%arg0: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg1: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg2: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0)) attributes {noinline = false, nvvm.kernel = 1 : ui1, nvvm.maxntid = array<i32: 128>} {
    %0 = llvm.mlir.constant(true) : i1 loc(#loc1)
    %1 = llvm.mlir.constant(512 : i32) : i32 loc(#loc1)
    %2 = llvm.mlir.constant(256 : i32) : i32 loc(#loc1)
    %3 = llvm.mlir.constant(128 : i32) : i32 loc(#loc1)
    %4 = llvm.mlir.constant(64 : i32) : i32 loc(#loc1)
    %5 = llvm.mlir.constant(16 : i32) : i32 loc(#loc1)
    %6 = llvm.mlir.constant(8 : i32) : i32 loc(#loc1)
    %7 = llvm.mlir.constant(2 : i32) : i32 loc(#loc1)
    %8 = llvm.mlir.constant(4 : i32) : i32 loc(#loc1)
    %9 = llvm.mlir.constant(1 : i32) : i32 loc(#loc1)
    %10 = llvm.mlir.constant(0 : i32) : i32 loc(#loc1)
    %11 = llvm.mlir.constant(32 : i32) : i32 loc(#loc1)
    %12 = llvm.mlir.constant(0 : index) : i32 loc(#loc1)
    %13 = llvm.mlir.constant(1024 : i32) : i32 loc(#loc1)
    %14 = llvm.inline_asm asm_dialect = att operand_attrs = [] "mov.u32 $0, %ctaid.x;", "=r"  : () -> i32 loc(#loc2)
    %15 = llvm.mul %14, %13 : i32 loc(#loc3)
    %16 = nvvm.read.ptx.sreg.tid.x : i32 loc(#loc4)
    %17 = llvm.urem %16, %11  : i32 loc(#loc4)
    %18 = llvm.udiv %16, %11  : i32 loc(#loc4)
    %19 = llvm.and %17, %9  : i32 loc(#loc4)
    %20 = llvm.icmp "eq" %19, %10 : i32 loc(#loc4)
    %21 = llvm.select %20, %10, %8 : i1, i32 loc(#loc4)
    %22 = llvm.xor %10, %21  : i32 loc(#loc4)
    %23 = llvm.and %17, %7  : i32 loc(#loc4)
    %24 = llvm.icmp "eq" %23, %10 : i32 loc(#loc4)
    %25 = llvm.select %24, %10, %6 : i1, i32 loc(#loc4)
    %26 = llvm.xor %22, %25  : i32 loc(#loc4)
    %27 = llvm.and %17, %8  : i32 loc(#loc4)
    %28 = llvm.icmp "eq" %27, %10 : i32 loc(#loc4)
    %29 = llvm.select %28, %10, %5 : i1, i32 loc(#loc4)
    %30 = llvm.xor %26, %29  : i32 loc(#loc4)
    %31 = llvm.and %17, %6  : i32 loc(#loc4)
    %32 = llvm.icmp "eq" %31, %10 : i32 loc(#loc4)
    %33 = llvm.select %32, %10, %11 : i1, i32 loc(#loc4)
    %34 = llvm.xor %30, %33  : i32 loc(#loc4)
    %35 = llvm.and %17, %5  : i32 loc(#loc4)
    %36 = llvm.icmp "eq" %35, %10 : i32 loc(#loc4)
    %37 = llvm.select %36, %10, %4 : i1, i32 loc(#loc4)
    %38 = llvm.xor %34, %37  : i32 loc(#loc4)
    %39 = llvm.and %18, %9  : i32 loc(#loc4)
    %40 = llvm.icmp "eq" %39, %10 : i32 loc(#loc4)
    %41 = llvm.select %40, %10, %3 : i1, i32 loc(#loc4)
    %42 = llvm.xor %38, %41  : i32 loc(#loc4)
    %43 = llvm.and %18, %7  : i32 loc(#loc4)
    %44 = llvm.icmp "eq" %43, %10 : i32 loc(#loc4)
    %45 = llvm.select %44, %10, %2 : i1, i32 loc(#loc4)
    %46 = llvm.xor %42, %45  : i32 loc(#loc4)
    %47 = llvm.xor %1, %21  : i32 loc(#loc4)
    %48 = llvm.xor %47, %25  : i32 loc(#loc4)
    %49 = llvm.xor %48, %29  : i32 loc(#loc4)
    %50 = llvm.xor %49, %33  : i32 loc(#loc4)
    %51 = llvm.xor %50, %37  : i32 loc(#loc4)
    %52 = llvm.xor %51, %41  : i32 loc(#loc4)
    %53 = llvm.xor %52, %45  : i32 loc(#loc4)
    %54 = llvm.add %46, %12 : i32 loc(#loc4)
    %55 = llvm.add %53, %12 : i32 loc(#loc4)
    %56 = llvm.add %15, %54 : i32 loc(#loc5)
    %57 = llvm.add %15, %55 : i32 loc(#loc5)
    %58 = llvm.icmp "slt" %56, %arg3 : i32 loc(#loc6)
    %59 = llvm.icmp "slt" %57, %arg3 : i32 loc(#loc6)
    %60 = llvm.getelementptr %arg0[%56] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32 loc(#loc7)
    %61 = llvm.getelementptr %arg0[%57] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32 loc(#loc7)
    %62 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] "mov.u32 $0, 0x0;\0A\09mov.u32 $1, 0x0;\0A\09mov.u32 $2, 0x0;\0A\09mov.u32 $3, 0x0;\0A\09@$5 ld.global.v4.b32 { $0, $1, $2, $3 }, [ $4 + 0 ];", "=r,=r,=r,=r,l,b" %60, %58 : (!llvm.ptr<1>, i1) -> !llvm.struct<(i32, i32, i32, i32)> loc(#loc8)
    %63 = llvm.extractvalue %62[0] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc8)
    %64 = llvm.bitcast %63 : i32 to vector<1xf32> loc(#loc8)
    %65 = llvm.extractvalue %62[1] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc8)
    %66 = llvm.bitcast %65 : i32 to vector<1xf32> loc(#loc8)
    %67 = llvm.extractvalue %62[2] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc8)
    %68 = llvm.bitcast %67 : i32 to vector<1xf32> loc(#loc8)
    %69 = llvm.extractvalue %62[3] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc8)
    %70 = llvm.bitcast %69 : i32 to vector<1xf32> loc(#loc8)
    %71 = llvm.extractelement %64[%12 : i32] : vector<1xf32> loc(#loc8)
    %72 = llvm.extractelement %66[%12 : i32] : vector<1xf32> loc(#loc8)
    %73 = llvm.extractelement %68[%12 : i32] : vector<1xf32> loc(#loc8)
    %74 = llvm.extractelement %70[%12 : i32] : vector<1xf32> loc(#loc8)
    %75 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] "mov.u32 $0, 0x0;\0A\09mov.u32 $1, 0x0;\0A\09mov.u32 $2, 0x0;\0A\09mov.u32 $3, 0x0;\0A\09@$5 ld.global.v4.b32 { $0, $1, $2, $3 }, [ $4 + 0 ];", "=r,=r,=r,=r,l,b" %61, %59 : (!llvm.ptr<1>, i1) -> !llvm.struct<(i32, i32, i32, i32)> loc(#loc8)
    %76 = llvm.extractvalue %75[0] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc8)
    %77 = llvm.bitcast %76 : i32 to vector<1xf32> loc(#loc8)
    %78 = llvm.extractvalue %75[1] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc8)
    %79 = llvm.bitcast %78 : i32 to vector<1xf32> loc(#loc8)
    %80 = llvm.extractvalue %75[2] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc8)
    %81 = llvm.bitcast %80 : i32 to vector<1xf32> loc(#loc8)
    %82 = llvm.extractvalue %75[3] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc8)
    %83 = llvm.bitcast %82 : i32 to vector<1xf32> loc(#loc8)
    %84 = llvm.extractelement %77[%12 : i32] : vector<1xf32> loc(#loc8)
    %85 = llvm.extractelement %79[%12 : i32] : vector<1xf32> loc(#loc8)
    %86 = llvm.extractelement %81[%12 : i32] : vector<1xf32> loc(#loc8)
    %87 = llvm.extractelement %83[%12 : i32] : vector<1xf32> loc(#loc8)
    %88 = llvm.getelementptr %arg1[%56] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32 loc(#loc9)
    %89 = llvm.getelementptr %arg1[%57] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32 loc(#loc9)
    %90 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] "mov.u32 $0, 0x0;\0A\09mov.u32 $1, 0x0;\0A\09mov.u32 $2, 0x0;\0A\09mov.u32 $3, 0x0;\0A\09@$5 ld.global.v4.b32 { $0, $1, $2, $3 }, [ $4 + 0 ];", "=r,=r,=r,=r,l,b" %88, %58 : (!llvm.ptr<1>, i1) -> !llvm.struct<(i32, i32, i32, i32)> loc(#loc10)
    %91 = llvm.extractvalue %90[0] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc10)
    %92 = llvm.bitcast %91 : i32 to vector<1xf32> loc(#loc10)
    %93 = llvm.extractvalue %90[1] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc10)
    %94 = llvm.bitcast %93 : i32 to vector<1xf32> loc(#loc10)
    %95 = llvm.extractvalue %90[2] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc10)
    %96 = llvm.bitcast %95 : i32 to vector<1xf32> loc(#loc10)
    %97 = llvm.extractvalue %90[3] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc10)
    %98 = llvm.bitcast %97 : i32 to vector<1xf32> loc(#loc10)
    %99 = llvm.extractelement %92[%12 : i32] : vector<1xf32> loc(#loc10)
    %100 = llvm.extractelement %94[%12 : i32] : vector<1xf32> loc(#loc10)
    %101 = llvm.extractelement %96[%12 : i32] : vector<1xf32> loc(#loc10)
    %102 = llvm.extractelement %98[%12 : i32] : vector<1xf32> loc(#loc10)
    %103 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] "mov.u32 $0, 0x0;\0A\09mov.u32 $1, 0x0;\0A\09mov.u32 $2, 0x0;\0A\09mov.u32 $3, 0x0;\0A\09@$5 ld.global.v4.b32 { $0, $1, $2, $3 }, [ $4 + 0 ];", "=r,=r,=r,=r,l,b" %89, %59 : (!llvm.ptr<1>, i1) -> !llvm.struct<(i32, i32, i32, i32)> loc(#loc10)
    %104 = llvm.extractvalue %103[0] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc10)
    %105 = llvm.bitcast %104 : i32 to vector<1xf32> loc(#loc10)
    %106 = llvm.extractvalue %103[1] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc10)
    %107 = llvm.bitcast %106 : i32 to vector<1xf32> loc(#loc10)
    %108 = llvm.extractvalue %103[2] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc10)
    %109 = llvm.bitcast %108 : i32 to vector<1xf32> loc(#loc10)
    %110 = llvm.extractvalue %103[3] : !llvm.struct<(i32, i32, i32, i32)>  loc(#loc10)
    %111 = llvm.bitcast %110 : i32 to vector<1xf32> loc(#loc10)
    %112 = llvm.extractelement %105[%12 : i32] : vector<1xf32> loc(#loc10)
    %113 = llvm.extractelement %107[%12 : i32] : vector<1xf32> loc(#loc10)
    %114 = llvm.extractelement %109[%12 : i32] : vector<1xf32> loc(#loc10)
    %115 = llvm.extractelement %111[%12 : i32] : vector<1xf32> loc(#loc10)
    %116 = llvm.fadd %71, %99  : f32 loc(#loc11)
    %117 = llvm.fadd %72, %100  : f32 loc(#loc11)
    %118 = llvm.fadd %73, %101  : f32 loc(#loc11)
    %119 = llvm.fadd %74, %102  : f32 loc(#loc11)
    %120 = llvm.fadd %84, %112  : f32 loc(#loc11)
    %121 = llvm.fadd %85, %113  : f32 loc(#loc11)
    %122 = llvm.fadd %86, %114  : f32 loc(#loc11)
    %123 = llvm.fadd %87, %115  : f32 loc(#loc11)
    %124 = llvm.getelementptr %arg2[%56] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32 loc(#loc12)
    %125 = llvm.getelementptr %arg2[%57] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32 loc(#loc12)
    %126 = llvm.mlir.undef : vector<1xf32> loc(#loc13)
    %127 = llvm.insertelement %116, %126[%10 : i32] : vector<1xf32> loc(#loc13)
    %128 = llvm.bitcast %127 : vector<1xf32> to i32 loc(#loc13)
    %129 = llvm.insertelement %117, %126[%10 : i32] : vector<1xf32> loc(#loc13)
    %130 = llvm.bitcast %129 : vector<1xf32> to i32 loc(#loc13)
    %131 = llvm.insertelement %118, %126[%10 : i32] : vector<1xf32> loc(#loc13)
    %132 = llvm.bitcast %131 : vector<1xf32> to i32 loc(#loc13)
    %133 = llvm.insertelement %119, %126[%10 : i32] : vector<1xf32> loc(#loc13)
    %134 = llvm.bitcast %133 : vector<1xf32> to i32 loc(#loc13)
    %135 = llvm.and %0, %58  : i1 loc(#loc13)
    %136 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] "@$5 st.global.v4.b32 [ $4 + 0 ], { $0, $1, $2, $3 };", "r,r,r,r,l,b" %128, %130, %132, %134, %124, %135 : (i32, i32, i32, i32, !llvm.ptr<1>, i1) -> !llvm.void loc(#loc13)
    %137 = llvm.insertelement %120, %126[%10 : i32] : vector<1xf32> loc(#loc13)
    %138 = llvm.bitcast %137 : vector<1xf32> to i32 loc(#loc13)
    %139 = llvm.insertelement %121, %126[%10 : i32] : vector<1xf32> loc(#loc13)
    %140 = llvm.bitcast %139 : vector<1xf32> to i32 loc(#loc13)
    %141 = llvm.insertelement %122, %126[%10 : i32] : vector<1xf32> loc(#loc13)
    %142 = llvm.bitcast %141 : vector<1xf32> to i32 loc(#loc13)
    %143 = llvm.insertelement %123, %126[%10 : i32] : vector<1xf32> loc(#loc13)
    %144 = llvm.bitcast %143 : vector<1xf32> to i32 loc(#loc13)
    %145 = llvm.and %0, %59  : i1 loc(#loc13)
    %146 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] "@$5 st.global.v4.b32 [ $4 + 0 ], { $0, $1, $2, $3 };", "r,r,r,r,l,b" %138, %140, %142, %144, %125, %145 : (i32, i32, i32, i32, !llvm.ptr<1>, i1) -> !llvm.void loc(#loc13)
    llvm.return loc(#loc14)
  } loc(#loc15)
} loc(#loc)
#di_file = #llvm.di_file<"00.vector-add.py" in "/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add">
#di_subroutine_type = #llvm.di_subroutine_type<callingConvention = DW_CC_normal>
#loc1 = loc(unknown)
#loc2 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":27:24)
#loc3 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":32:24)
#loc4 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":33:41)
#loc5 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":33:28)
#loc6 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":35:21)
#loc7 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":38:24)
#loc8 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":38:16)
#loc9 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":39:24)
#loc10 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":39:16)
#loc11 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":40:17)
#loc12 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:26)
#loc13 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:35)
#loc14 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:4)
#di_compile_unit = #llvm.di_compile_unit<id = distinct[0]<>, sourceLanguage = DW_LANG_C, file = #di_file, producer = "triton", isOptimized = true, emissionKind = LineTablesOnly>
#di_subprogram = #llvm.di_subprogram<id = distinct[0]<>, compileUnit = #di_compile_unit, scope = #di_file, name = "add_kernel", linkageName = "add_kernel", file = #di_file, line = 17, scopeLine = 17, subprogramFlags = "Definition|Optimized", type = #di_subroutine_type>
#loc15 = loc(fused<#di_subprogram>[#loc])
```

### LLVM IR(MLIR)生成LLVM IR

TritonGPU IR转LLVM IR的第二阶段，LLVM IR(MLIR)生成LLVM IR

```python
llvm.init_targets()
context = llvm.context()
llvm_mod = llvm.to_module(mod, context)
nvidia.set_nvvm_reflect_ftz(llvm_mod)

# Set maxnreg on all kernels, if it was provided.
if options.maxnreg is not None:
    for k in llvm_mod.get_functions():
        if not k.is_declaration() and k.is_external_linkage():
            k.set_nvvm_maxnreg(options.maxnreg)

if options.extern_libs:
    paths = [path for (name, path) in options.extern_libs]
    llvm.link_extern_libs(llvm_mod, paths)

llvm.optimize_module(llvm_mod, llvm.OPTIMIZE_O3)

# Get some metadata
metadata["shared"] = src.get_int_attr("triton_gpu.shared")
ret = str(llvm_mod)
```

核心的步骤是`llvm_mod = llvm.to_module(mod, context)`，它在c++侧被绑定到了mlir::translateModuleToLLVMIR接口上。它是MLIR的标准接口，实现在mlir/lib/Target/LLVMIR/ModuleTranslation.cpp。

```c++
m.def(
    "to_module",
    [](mlir::ModuleOp &mod, llvm::LLVMContext &ctx) {
      return mlir::translateModuleToLLVMIR(mod, ctx);
    },
    py::keep_alive<0, 2>());
```

简单看一下translateModuleToLLVMIR的实现

```c++
std::unique_ptr<llvm::Module>
mlir::translateModuleToLLVMIR(Operation *module, llvm::LLVMContext &llvmContext,
                              StringRef name, bool disableVerification) {
  if (!satisfiesLLVMModule(module)) {
    module->emitOpError("can not be translated to an LLVMIR module");
    return nullptr;
  }

  std::unique_ptr<llvm::Module> llvmModule =
      prepareLLVMModule(module, llvmContext, name);
  if (!llvmModule)
    return nullptr;

  LLVM::ensureDistinctSuccessors(module);
  LLVM::legalizeDIExpressionsRecursively(module);

  ModuleTranslation translator(module, std::move(llvmModule));
  llvm::IRBuilder<> llvmBuilder(llvmContext);

  // Convert module before functions and operations inside, so dialect
  // attributes can be used to change dialect-specific global configurations via
  // `amendOperation()`. These configurations can then influence the translation
  // of operations afterwards.
  if (failed(translator.convertOperation(*module, llvmBuilder)))
    return nullptr;

  if (failed(translator.convertComdats()))
    return nullptr;
  if (failed(translator.convertFunctionSignatures()))
    return nullptr;
  if (failed(translator.convertGlobals()))
    return nullptr;
  if (failed(translator.createTBAAMetadata()))
    return nullptr;

  // Convert other top-level operations if possible.
  for (Operation &o : getModuleBody(module).getOperations()) {
    if (!isa<LLVM::LLVMFuncOp, LLVM::GlobalOp, LLVM::GlobalCtorsOp,
             LLVM::GlobalDtorsOp, LLVM::ComdatOp>(&o) &&
        !o.hasTrait<OpTrait::IsTerminator>() &&
        failed(translator.convertOperation(o, llvmBuilder))) {
      return nullptr;
    }
  }

  // Operations in function bodies with symbolic references must be converted
  // after the top-level operations they refer to are declared, so we do it
  // last.
  if (failed(translator.convertFunctions()))
    return nullptr;

  // Once we've finished constructing elements in the module, we should convert
  // it to use the debug info format desired by LLVM.
  // See https://llvm.org/docs/RemoveDIsDebugInfo.html
  translator.llvmModule->setIsNewDbgInfoFormat(UseNewDbgInfoFormat);

  if (!disableVerification &&
      llvm::verifyModule(*translator.llvmModule, &llvm::errs()))
    return nullptr;

  return std::move(translator.llvmModule);
}
```

由各种convertXXXX前缀的方法负责具体部分的转换，以convertOperation为例子

```c++
LogicalResult ModuleTranslation::convertOperation(Operation &op,
                                                  llvm::IRBuilderBase &builder,
                                                  bool recordInsertions) {
  const LLVMTranslationDialectInterface *opIface = iface.getInterfaceFor(&op);
  if (!opIface)
    return op.emitError("cannot be converted to LLVM IR: missing "
                        "`LLVMTranslationDialectInterface` registration for "
                        "dialect for op: ")
           << op.getName();

  InstructionCapturingInserter::CollectionScope scope(builder,
                                                      recordInsertions);
  if (failed(opIface->convertOperation(&op, builder, *this)))
    return op.emitError("LLVM Translation failed for operation: ")
           << op.getName();

  return convertDialectAttributes(&op, scope.getCapturedInstructions());
}
```

实际工作在op interface face对象中执行，这里已经能够看到，它接收了一个IRBuilder参数，这个对象的使用我们在LLVM分享里已经看过，它的职责就是生成LLVM IR

```c++
static LogicalResult
convertOperationImpl(Operation &opInst, llvm::IRBuilderBase &builder,
                     LLVM::ModuleTranslation &moduleTranslation) {

  llvm::IRBuilder<>::FastMathFlagGuard fmfGuard(builder);
  if (auto fmf = dyn_cast<FastmathFlagsInterface>(opInst))
    builder.setFastMathFlags(getFastmathFlags(fmf));

#include "mlir/Dialect/LLVMIR/LLVMConversions.inc"
#include "mlir/Dialect/LLVMIR/LLVMIntrinsicConversions.inc"

  // Emit function calls.  If the "callee" attribute is present, this is a
  // direct function call and we also need to look up the remapped function
  // itself.  Otherwise, this is an indirect call and the callee is the first
  // operand, look it up as a normal value.
  if (auto callOp = dyn_cast<LLVM::CallOp>(opInst)) {
    auto operands = moduleTranslation.lookupValues(callOp.getOperands());
    ArrayRef<llvm::Value *> operandsRef(operands);
    llvm::CallInst *call;
    if (auto attr = callOp.getCalleeAttr()) {
      call = builder.CreateCall(
          moduleTranslation.lookupFunction(attr.getValue()), operandsRef);
    } else {
      llvm::FunctionType *calleeType = llvm::cast<llvm::FunctionType>(
          moduleTranslation.convertType(callOp.getCalleeFunctionType()));
      call = builder.CreateCall(calleeType, operandsRef.front(),
                                operandsRef.drop_front());
    }
    call->setCallingConv(convertCConvToLLVM(callOp.getCConv()));
    call->setTailCallKind(convertTailCallKindToLLVM(callOp.getTailCallKind()));
    moduleTranslation.setAccessGroupsMetadata(callOp, call);
    moduleTranslation.setAliasScopeMetadata(callOp, call);
    moduleTranslation.setTBAAMetadata(callOp, call);
    // If the called function has a result, remap the corresponding value.  Note
    // that LLVM IR dialect CallOp has either 0 or 1 result.
    if (opInst.getNumResults() != 0)
      moduleTranslation.mapValue(opInst.getResult(0), call);
    // Check that LLVM call returns void for 0-result functions.
    else if (!call->getType()->isVoidTy())
      return failure();
    moduleTranslation.mapCall(callOp, call);
    return success();
  }
}
```

经过 LLVM-IR (MLIR) -> LLVM-IR (LLVM)之后，获得如下的LLVM IR，从ops上能看出和LLVM IR（MLIR）的区别，LLVM IR（MLIR）保留有dialect的名字，但是这里已经是纯粹的指令名了。

```c
; ModuleID = 'LLVMDialectModule'
source_filename = "LLVMDialectModule"

define void @add_kernel(ptr addrspace(1) %0, ptr addrspace(1) %1, ptr addrspace(1) %2, i32 %3) local_unnamed_addr !dbg !7 {
  %5 = tail call i32 asm "mov.u32 $0, %ctaid.x;", "=r"() #1, !dbg !10
  %6 = shl i32 %5, 10, !dbg !11
  %7 = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !dbg !12
  %8 = shl i32 %7, 2, !dbg !12
  %9 = and i32 %8, 508, !dbg !12
  %10 = or disjoint i32 %6, %9, !dbg !13
  %11 = or disjoint i32 %10, 512, !dbg !13
  %12 = icmp slt i32 %10, %3, !dbg !14
  %13 = icmp slt i32 %11, %3, !dbg !14
  %14 = sext i32 %10 to i64, !dbg !15
  %15 = getelementptr float, ptr addrspace(1) %0, i64 %14, !dbg !15
  %16 = sext i32 %11 to i64, !dbg !15
  %17 = getelementptr float, ptr addrspace(1) %0, i64 %16, !dbg !15
  %18 = tail call { i32, i32, i32, i32 } asm sideeffect "mov.u32 $0, 0x0;\0A\09mov.u32 $1, 0x0;\0A\09mov.u32 $2, 0x0;\0A\09mov.u32 $3, 0x0;\0A\09@$5 ld.global.v4.b32 { $0, $1, $2, $3 }, [ $4 + 0 ];", "=r,=r,=r,=r,l,b"(ptr addrspace(1) %15, i1 %12) #1, !dbg !16
  %19 = extractvalue { i32, i32, i32, i32 } %18, 0, !dbg !16
  %20 = extractvalue { i32, i32, i32, i32 } %18, 1, !dbg !16
  %21 = extractvalue { i32, i32, i32, i32 } %18, 2, !dbg !16
  %22 = extractvalue { i32, i32, i32, i32 } %18, 3, !dbg !16
  %23 = bitcast i32 %19 to float, !dbg !16
  %24 = bitcast i32 %20 to float, !dbg !16
  %25 = bitcast i32 %21 to float, !dbg !16
  %26 = bitcast i32 %22 to float, !dbg !16
  %27 = tail call { i32, i32, i32, i32 } asm sideeffect "mov.u32 $0, 0x0;\0A\09mov.u32 $1, 0x0;\0A\09mov.u32 $2, 0x0;\0A\09mov.u32 $3, 0x0;\0A\09@$5 ld.global.v4.b32 { $0, $1, $2, $3 }, [ $4 + 0 ];", "=r,=r,=r,=r,l,b"(ptr addrspace(1) %17, i1 %13) #1, !dbg !16
  %28 = extractvalue { i32, i32, i32, i32 } %27, 0, !dbg !16
  %29 = extractvalue { i32, i32, i32, i32 } %27, 1, !dbg !16
  %30 = extractvalue { i32, i32, i32, i32 } %27, 2, !dbg !16
  %31 = extractvalue { i32, i32, i32, i32 } %27, 3, !dbg !16
  %32 = bitcast i32 %28 to float, !dbg !16
  %33 = bitcast i32 %29 to float, !dbg !16
  %34 = bitcast i32 %30 to float, !dbg !16
  %35 = bitcast i32 %31 to float, !dbg !16
  %36 = getelementptr float, ptr addrspace(1) %1, i64 %14, !dbg !17
  %37 = getelementptr float, ptr addrspace(1) %1, i64 %16, !dbg !17
  %38 = tail call { i32, i32, i32, i32 } asm sideeffect "mov.u32 $0, 0x0;\0A\09mov.u32 $1, 0x0;\0A\09mov.u32 $2, 0x0;\0A\09mov.u32 $3, 0x0;\0A\09@$5 ld.global.v4.b32 { $0, $1, $2, $3 }, [ $4 + 0 ];", "=r,=r,=r,=r,l,b"(ptr addrspace(1) %36, i1 %12) #1, !dbg !18
  %39 = extractvalue { i32, i32, i32, i32 } %38, 0, !dbg !18
  %40 = extractvalue { i32, i32, i32, i32 } %38, 1, !dbg !18
  %41 = extractvalue { i32, i32, i32, i32 } %38, 2, !dbg !18
  %42 = extractvalue { i32, i32, i32, i32 } %38, 3, !dbg !18
  %43 = bitcast i32 %39 to float, !dbg !18
  %44 = bitcast i32 %40 to float, !dbg !18
  %45 = bitcast i32 %41 to float, !dbg !18
  %46 = bitcast i32 %42 to float, !dbg !18
  %47 = tail call { i32, i32, i32, i32 } asm sideeffect "mov.u32 $0, 0x0;\0A\09mov.u32 $1, 0x0;\0A\09mov.u32 $2, 0x0;\0A\09mov.u32 $3, 0x0;\0A\09@$5 ld.global.v4.b32 { $0, $1, $2, $3 }, [ $4 + 0 ];", "=r,=r,=r,=r,l,b"(ptr addrspace(1) %37, i1 %13) #1, !dbg !18
  %48 = extractvalue { i32, i32, i32, i32 } %47, 0, !dbg !18
  %49 = extractvalue { i32, i32, i32, i32 } %47, 1, !dbg !18
  %50 = extractvalue { i32, i32, i32, i32 } %47, 2, !dbg !18
  %51 = extractvalue { i32, i32, i32, i32 } %47, 3, !dbg !18
  %52 = bitcast i32 %48 to float, !dbg !18
  %53 = bitcast i32 %49 to float, !dbg !18
  %54 = bitcast i32 %50 to float, !dbg !18
  %55 = bitcast i32 %51 to float, !dbg !18
  %56 = fadd float %23, %43, !dbg !19
  %57 = fadd float %24, %44, !dbg !19
  %58 = fadd float %25, %45, !dbg !19
  %59 = fadd float %26, %46, !dbg !19
  %60 = fadd float %32, %52, !dbg !19
  %61 = fadd float %33, %53, !dbg !19
  %62 = fadd float %34, %54, !dbg !19
  %63 = fadd float %35, %55, !dbg !19
  %64 = getelementptr float, ptr addrspace(1) %2, i64 %14, !dbg !20
  %65 = getelementptr float, ptr addrspace(1) %2, i64 %16, !dbg !20
  %66 = bitcast float %56 to i32, !dbg !21
  %67 = bitcast float %57 to i32, !dbg !21
  %68 = bitcast float %58 to i32, !dbg !21
  %69 = bitcast float %59 to i32, !dbg !21
  tail call void asm sideeffect "@$5 st.global.v4.b32 [ $4 + 0 ], { $0, $1, $2, $3 };", "r,r,r,r,l,b"(i32 %66, i32 %67, i32 %68, i32 %69, ptr addrspace(1) %64, i1 %12) #1, !dbg !21
  %70 = bitcast float %60 to i32, !dbg !21
  %71 = bitcast float %61 to i32, !dbg !21
  %72 = bitcast float %62 to i32, !dbg !21
  %73 = bitcast float %63 to i32, !dbg !21
  tail call void asm sideeffect "@$5 st.global.v4.b32 [ $4 + 0 ], { $0, $1, $2, $3 };", "r,r,r,r,l,b"(i32 %70, i32 %71, i32 %72, i32 %73, ptr addrspace(1) %65, i1 %13) #1, !dbg !21
  ret void, !dbg !22
}

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare noundef i32 @llvm.nvvm.read.ptx.sreg.tid.x() #0

attributes #0 = { mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none) }
attributes #1 = { nounwind }

!llvm.module.flags = !{!0, !1}
!llvm.dbg.cu = !{!2}
!nvvm.annotations = !{!4, !5}
!llvm.ident = !{!6}

!0 = !{i32 2, !"Debug Info Version", i32 3}
!1 = !{i32 4, !"nvvm-reflect-ftz", i32 1}
!2 = distinct !DICompileUnit(language: DW_LANG_C, file: !3, producer: "triton", isOptimized: true, runtimeVersion: 0, emissionKind: LineTablesOnly)
!3 = !DIFile(filename: "00.vector-add.py", directory: "/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add")
!4 = !{ptr @add_kernel, !"kernel", i32 1}
!5 = !{ptr @add_kernel, !"maxntidx", i32 128}
!6 = !{!"clang version 3.8.0 (tags/RELEASE_380/final)"}
!7 = distinct !DISubprogram(name: "add_kernel", linkageName: "add_kernel", scope: !3, file: !3, line: 17, type: !8, scopeLine: 17, spFlags: DISPFlagDefinition | DISPFlagOptimized, unit: !2)
!8 = !DISubroutineType(cc: DW_CC_normal, types: !9)
!9 = !{}
!10 = !DILocation(line: 27, column: 24, scope: !7)
!11 = !DILocation(line: 32, column: 24, scope: !7)
!12 = !DILocation(line: 33, column: 41, scope: !7)
!13 = !DILocation(line: 33, column: 28, scope: !7)
!14 = !DILocation(line: 35, column: 21, scope: !7)
!15 = !DILocation(line: 38, column: 24, scope: !7)
!16 = !DILocation(line: 38, column: 16, scope: !7)
!17 = !DILocation(line: 39, column: 24, scope: !7)
!18 = !DILocation(line: 39, column: 16, scope: !7)
!19 = !DILocation(line: 40, column: 17, scope: !7)
!20 = !DILocation(line: 42, column: 26, scope: !7)
!21 = !DILocation(line: 42, column: 35, scope: !7)
!22 = !DILocation(line: 42, column: 4, scope: !7)
```

### LLVM IR生成PTX

这个过程的入口函数是`make_ptx`

```python
@staticmethod
def make_ptx(src, metadata, opt, capability):
    ptx_version = opt.ptx_version
    if ptx_version is None:
        _, cuda_version = _path_to_binary("ptxas")
        ptx_version = ptx_get_version(cuda_version)

    # PTX 8.3 is the max version supported by llvm 3a83162168.
    #
    # To check if a newer PTX version is supported, increase this value
    # and run a test.  If it's not supported, LLVM will print a warning
    # like "+ptx8.4 is not a recognized feature for this target".
    llvm_ptx_version = min(83, ptx_version)

    triple = 'nvptx64-nvidia-cuda'
    proc = 'sm_90a' if capability == 90 else f'sm_{capability}'
    features = f'+ptx{llvm_ptx_version}'
    ret = llvm.translate_to_asm(src, triple, proc, features, ['nvptx-short-ptr'], opt.enable_fp_fusion, False)
    # Find kernel names (there should only be one)
    names = re.findall(r".visible .entry ([a-zA-Z_][a-zA-Z0-9_]*)", ret)
    assert len(names) == 1
    metadata["name"] = names[0]
    # post-process
    ptx_version = f'{ptx_version//10}.{ptx_version%10}'
    ret = re.sub(r'\.version \d+\.\d+', f'.version {ptx_version}', ret, flags=re.MULTILINE)
    # Remove the debug flag that prevents ptxas from optimizing the code
    ret = re.sub(r",\s*debug|debug,\s*", "", ret)
    if os.environ.get("NVPTX_ENABLE_DUMP", "0") == "1":
        print("// -----// NVPTX Dump //----- //")
        print(ret)
    return ret
```

显而易见，关键一步是`ret = llvm.translate_to_asm(src, triple, proc, features, ['nvptx-short-ptr'], opt.enable_fp_fusion, False)`

它也是在llvm.cc文件中进行的绑定

```c++
  m.def(
      "translate_to_asm",
      [](std::string llvmIR, std::string triple, std::string proc,
         std::string features, std::vector<std::string> flags,
         bool enable_fp_fusion, bool isObject) -> py::object {
        std::string obj;
        {
          // when allow_threads goes out of scope, gil will be released
          py::gil_scoped_release allow_threads;
          // create LLVM module from C++
          llvm::LLVMContext context;
          std::unique_ptr<llvm::MemoryBuffer> buffer =
              llvm::MemoryBuffer::getMemBuffer(llvmIR.c_str());
          llvm::SMDiagnostic error;
          std::unique_ptr<llvm::Module> module =
              llvm::parseIR(buffer->getMemBufferRef(), error, context);
          if (!module) {
            llvm::report_fatal_error(
                "failed to parse IR: " + error.getMessage() +
                "lineno: " + std::to_string(error.getLineNo()));
          }
          obj = translateLLVMIRToASM(*module, triple, proc, features, flags,
                                     enable_fp_fusion, isObject);
        }
        if (isObject)
          return py::bytes(obj);
        else
          return py::str(obj);
      },
      ret::take_ownership);
```

继续看`translateLLVMIRToASM`，可以发现，是我们已经熟悉的流程：

1. set triple
2. create machine
3. addPassesToEmitFile
4. pm.run

```c++

std::string translateLLVMIRToASM(llvm::Module &module,
                                 const std::string &triple,
                                 const std::string &proc,
                                 const std::string &features,
                                 const std::vector<std::string> &flags,
                                 bool enable_fp_fusion, bool isObject) {
  using namespace mlir;
  // options
  auto options = llvm::cl::getRegisteredOptions();
  for (std::string flag : flags) {
    auto *shortPtr = static_cast<llvm::cl::opt<bool> *>(options[flag]);
    assert(shortPtr);
    shortPtr->setValue(true);
  }
  if (triton::tools::getBoolEnv("LLVM_IR_ENABLE_DUMP")) {
    auto optIt = options.find("print-after-all");
    if (optIt != options.end()) {
      auto optPtr = static_cast<llvm::cl::opt<bool> *>(optIt->second);
      *optPtr = true;
    }
  }
  bool disableLLVMOpt = triton::tools::getBoolEnv("DISABLE_LLVM_OPT");
  if (!disableLLVMOpt) {
    // Check to see if we are passing a list of flags to disable optimizations.
    auto flagList = triton::tools::getStrEnv("DISABLE_LLVM_OPT");
    if (!flagList.empty()) {
      llvm::SmallVector<StringRef, 3> split;
      StringRef(flagList.c_str()).split(split, ',');
      for (auto flag : split) {
        auto optIt = options.find(flag);
        if (optIt != options.end()) {
          auto optPtr = static_cast<llvm::cl::opt<bool> *>(optIt->second);
          *optPtr = true;
        }
      }
    }
  }

  // inline everything
  for (llvm::Function &f : module.functions())
    if (!f.hasFnAttribute(llvm::Attribute::NoInline))
      f.addFnAttr(llvm::Attribute::AlwaysInline);
  // verify and store llvm
  llvm::legacy::PassManager pm;
  pm.add(llvm::createAlwaysInlinerLegacyPass());
  pm.add(llvm::createVerifierPass());

  const bool enabledTiming = triton::tools::getBoolEnv("LLVM_ENABLE_TIMING");
  if (enabledTiming) {
    llvm::TimePassesIsEnabled = true;
    llvm::TimePassesPerRun = true;
  }

  pm.run(module);

  SmallString<0> timePassesStr;
  raw_svector_ostream reportStream(timePassesStr);

  if (enabledTiming) {
    reportAndResetTimings(&reportStream);
    llvm::dbgs() << reportStream.str();
    timePassesStr.clear();
  }
  // module->print(llvm::outs(), nullptr);

  // create machine
  module.setTargetTriple(triple);
  std::string error;
  auto target =
      llvm::TargetRegistry::lookupTarget(module.getTargetTriple(), error);
  llvm::TargetOptions opt;
  if (enable_fp_fusion)
    opt.AllowFPOpFusion = llvm::FPOpFusion::Fast;
  opt.UnsafeFPMath = false;
  opt.NoInfsFPMath = false;
  opt.NoNaNsFPMath = true;
  opt.TrapUnreachable = true;
  std::unique_ptr<llvm::TargetMachine> machine{target->createTargetMachine(
      module.getTargetTriple(), proc, features, opt, llvm::Reloc::PIC_,
      std::nullopt,
      disableLLVMOpt ? llvm::CodeGenOptLevel::None
                     : llvm::CodeGenOptLevel::Aggressive)};
  // set data layout
  module.setDataLayout(machine->createDataLayout());
  // emit machine code
  std::string result;
  {
    llvm::raw_string_ostream stream(result);
    llvm::buffer_ostream pstream(stream);
    for (llvm::Function &f : module.functions())
      f.addFnAttr(llvm::Attribute::AlwaysInline);
    llvm::legacy::PassManager pass;
    // emit
    auto fileType = isObject ? llvm::CodeGenFileType::ObjectFile
                             : llvm::CodeGenFileType::AssemblyFile;
    machine->addPassesToEmitFile(pass, pstream, nullptr, fileType);
    pass.run(module);

    if (enabledTiming) {
      reportAndResetTimings(&reportStream);
      llvm::dbgs() << reportStream.str();
      timePassesStr.clear();
    }
  }
  return result;
}
```

### PTX生成cubin

获得ptx之后，LLVM的工作已经结束，剩下的是ptxas的工作，实现在`make_cubin`中，首先将ptx flush到本地保存为一个.ptx文件，起一个ptxas进行编译出cubin

```python
@staticmethod
def make_cubin(src, metadata, opt, capability):
    ptxas, _ = _path_to_binary("ptxas")
    with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.ptx') as fsrc, \
        tempfile.NamedTemporaryFile(delete=False, mode='r', suffix='.log') as flog:
        fsrc.write(src)
        fsrc.flush()
        fbin = fsrc.name + '.o'

        line_info = '' if os.environ.get('TRITON_DISABLE_LINE_INFO') else ' -lineinfo'
        fmad = '' if opt.enable_fp_fusion else ' --fmad=false'
        suffix = 'a ' if capability == 90 else ' '
        if os.environ.get("DISABLE_PTXAS_OPT", "0") == "1":
            cmd = f'{ptxas}{line_info}{fmad} -v --opt-level 0 --gpu-name=sm_{capability}{suffix}{fsrc.name} -o {fbin} 2> {flog.name}'
        else:
            cmd = f'{ptxas}{line_info}{fmad} -v --gpu-name=sm_{capability}{suffix}{fsrc.name} -o {fbin} 2> {flog.name}'

        try:
            subprocess.run(cmd, shell=True, check=True)
        except subprocess.CalledProcessError as e:
            with open(flog.name) as log_file:
                log = log_file.read()
            if e.returncode == 255:
                raise RuntimeError(f'Internal Triton PTX codegen error: \n{log}')
            elif e.returncode == 128 + signal.SIGSEGV:
                raise RuntimeError(
                    f'Please run `ptxas {fsrc.name}` to confirm that this is a bug in `ptxas`\n{log}')
            else:
                raise RuntimeError(f'`ptxas` failed with error code {e.returncode}: \n{log}')
        finally:
            if os.path.exists(fsrc.name):
                os.remove(fsrc.name)
            if os.path.exists(flog.name):
                os.remove(flog.name)

        with open(fbin, 'rb') as f:
            cubin = f.read()
        if os.path.exists(fbin):
            os.remove(fbin)
    return cubin
```

## Kernel Run

kernel编译完成之后，进入到kernel执行过程，

```python
def compiler(...):
  # launch kernel
  launch_metadata = kernel.launch_metadata(grid, stream, *non_constexpr_vals)
  kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,
              self.CompiledKernel.launch_enter_hook, self.CompiledKernel.launch_exit_hook, *non_constexpr_vals)
```

kernel的类型是CompiledKernel，所以这里的run函数是CompiledKernel中的成员`self.run = driver.active.launcher_cls(self.src, self.metadata)`

```python
class CompiledKernel:
    ...

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared > max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
        self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, device)

    ...
```

对于CudaDriver，它的launcher_cls时CudaLauncher类型

```python
class CudaDriver(GPUDriver):

    def __init__(self):
        self.utils = CudaUtils()  # TODO: make static
        self.launcher_cls = CudaLauncher
        super().__init__()

    def get_current_target(self):
        device = self.get_current_device()
        capability = self.get_device_capability(device)
        capability = capability[0] * 10 + capability[1]
        warp_size = 32
        return GPUTarget("cuda", capability, warp_size)

    @staticmethod
    def is_active():
        import torch
        return torch.cuda.is_available() and (torch.version.hip is None)
```

实际调用的是__triton_launcher的launch方法

```python
class CudaLauncher(object):

    def __init__(self, src, metadata):
        ids = {"ids_of_const_exprs": src.fn.constexprs if hasattr(src, "fn") else tuple()}
        constants = src.constants if hasattr(src, "constants") else dict()
        cst_key = lambda i: src.fn.arg_names.index(i) if isinstance(i, str) else i
        constants = {cst_key(key): value for key, value in constants.items()}
        signature = {cst_key(key): value for key, value in src.signature.items()}
        src = make_launcher(constants, signature, ids)
        mod = compile_module_from_src(src, "__triton_launcher")
        self.launch = mod.launch

    def __call__(self, *args, **kwargs):
        self.launch(*args, **kwargs)
```

这里的写法比较特殊，首先在make_launcher中通过文本形式提供了一个kernel launch的c版本实现，代码就是通过dlsym cuLaunchKernel来调用

```python
   src = f"""
#include \"cuda.h\"
#include <stdbool.h>
#include <Python.h>
#include <dlfcn.h>

static inline void gpuAssert(CUresult code, const char *file, int line)
{{
   if (code != CUDA_SUCCESS)
   {{
      const char* prefix = "Triton Error [CUDA]: ";
      const char* str;
      cuGetErrorString(code, &str);
      char err[1024] = {{0}};
      strcat(err, prefix);
      strcat(err, str);
      PyGILState_STATE gil_state;
      gil_state = PyGILState_Ensure();
      PyErr_SetString(PyExc_RuntimeError, err);
      PyGILState_Release(gil_state);
   }}
}}

#define CUDA_CHECK(ans) {{ gpuAssert((ans), __FILE__, __LINE__); }}

typedef CUresult (*cuLaunchKernelEx_t)(const CUlaunchConfig* config, CUfunction f, void** kernelParams, void** extra);

static cuLaunchKernelEx_t getLaunchKernelExHandle() {{
  // Open the shared library
  void* handle = dlopen("libcuda.so.1", RTLD_LAZY);
  if (!handle) {{
    PyErr_SetString(PyExc_RuntimeError, "Failed to open libcuda.so.1");
    return NULL;
  }}
  // Clear any existing error
  dlerror();
  cuLaunchKernelEx_t cuLaunchKernelExHandle = (cuLaunchKernelEx_t)dlsym(handle, "cuLaunchKernelEx");
  // Check for errors
  const char *dlsym_error = dlerror();
  if (dlsym_error) {{
    PyErr_SetString(PyExc_RuntimeError, "Failed to retrieve cuLaunchKernelEx from libcuda.so.1");
    return NULL;
  }}
  return cuLaunchKernelExHandle;
}}

static void _launch(int gridX, int gridY, int gridZ, int num_warps, int num_ctas, int clusterDimX, int clusterDimY, int clusterDimZ, int shared_memory, CUstream stream, CUfunction function{', ' + arg_decls if len(arg_decls) > 0 else ''}) {{
  void *params[] = {{ {', '.join(f"&arg{i}" for i in params)} }};
  if (gridX*gridY*gridZ > 0) {{
    if (num_ctas == 1) {{
      CUDA_CHECK(cuLaunchKernel(function, gridX, gridY, gridZ, 32*num_warps, 1, 1, shared_memory, stream, params, 0));
    }} else {{
      CUlaunchAttribute launchAttr[2];
      launchAttr[0].id = CU_LAUNCH_ATTRIBUTE_CLUSTER_DIMENSION;
      launchAttr[0].value.clusterDim.x = clusterDimX;
      launchAttr[0].value.clusterDim.y = clusterDimY;
      launchAttr[0].value.clusterDim.z = clusterDimZ;
      launchAttr[1].id = CU_LAUNCH_ATTRIBUTE_CLUSTER_SCHEDULING_POLICY_PREFERENCE;
      launchAttr[1].value.clusterSchedulingPolicyPreference = CU_CLUSTER_SCHEDULING_POLICY_SPREAD;
      CUlaunchConfig config;
      config.gridDimX = gridX * clusterDimX;
      config.gridDimY = gridY * clusterDimY;
      config.gridDimZ = gridZ * clusterDimZ;
      config.blockDimX = 32 * num_warps;
      config.blockDimY = 1;
      config.blockDimZ = 1;
      config.sharedMemBytes = shared_memory;
      config.hStream = stream;
      config.attrs = launchAttr;
      config.numAttrs = 2;
      static cuLaunchKernelEx_t cuLaunchKernelExHandle = NULL;
      if (cuLaunchKernelExHandle == NULL) {{
        cuLaunchKernelExHandle = getLaunchKernelExHandle();
      }}
      CUDA_CHECK(cuLaunchKernelExHandle(&config, function, params, 0));
    }}
  }}
}}

typedef struct _DevicePtrInfo {{
    CUdeviceptr dev_ptr;
    bool valid;
}} DevicePtrInfo;

static inline DevicePtrInfo getPointer(PyObject *obj, int idx) {{
  DevicePtrInfo ptr_info;
  ptr_info.dev_ptr = 0;
  ptr_info.valid = true;
  if (PyLong_Check(obj)) {{
    ptr_info.dev_ptr = PyLong_AsUnsignedLongLong(obj);
    return ptr_info;
  }}
  if (obj == Py_None) {{
    // valid nullptr
    return ptr_info;
  }}
  PyObject *ptr = PyObject_GetAttrString(obj, "data_ptr");
  if(ptr){{
    PyObject *empty_tuple = PyTuple_New(0);
    PyObject *ret = PyObject_Call(ptr, empty_tuple, NULL);
    Py_DECREF(empty_tuple);
    Py_DECREF(ptr);
    if (!PyLong_Check(ret)) {{
      PyErr_SetString(PyExc_TypeError, "data_ptr method of Pointer object must return 64-bit int");
      ptr_info.valid = false;
      return ptr_info;
    }}
    ptr_info.dev_ptr = PyLong_AsUnsignedLongLong(ret);
    if(!ptr_info.dev_ptr)
      return ptr_info;
    uint64_t dev_ptr;
    int status = cuPointerGetAttribute(&dev_ptr, CU_POINTER_ATTRIBUTE_DEVICE_POINTER, ptr_info.dev_ptr);
    if (status == CUDA_ERROR_INVALID_VALUE) {{
        PyErr_Format(PyExc_ValueError,
                     "Pointer argument (at %d) cannot be accessed from Triton (cpu tensor?)", idx);
        ptr_info.valid = false;
    }}
    ptr_info.dev_ptr = dev_ptr;
    Py_DECREF(ret);  // Thanks ChatGPT!
    return ptr_info;
  }}
  PyErr_SetString(PyExc_TypeError, "Pointer argument must be either uint64 or have data_ptr method");
  ptr_info.valid = false;
  return ptr_info;
}}

static PyObject* launch(PyObject* self, PyObject* args) {{
  int gridX, gridY, gridZ;
  uint64_t _stream;
  uint64_t _function;
  PyObject *launch_enter_hook = NULL;
  PyObject *launch_exit_hook = NULL;
  PyObject *kernel_metadata = NULL;
  PyObject *launch_metadata = NULL;
  {' '.join([f"{_extracted_type(ty)} _arg{i}; " for i, ty in signature.items()])}
  if(!PyArg_ParseTuple(args, \"{format}\", &gridX, &gridY, &gridZ, &_stream, &_function,
                                           &kernel_metadata, &launch_metadata,
                                           &launch_enter_hook, &launch_exit_hook {args_list})) {{
    return NULL;
  }}

  int num_warps, num_ctas, shared_memory, clusterDimX, clusterDimY, clusterDimZ;
  if (!PyArg_ParseTuple(kernel_metadata, \"iiiiii\", &num_warps, &num_ctas, &shared_memory, &clusterDimX, &clusterDimY, &clusterDimZ)) {{
    PyErr_SetString(PyExc_TypeError, "kernel_metadata must be a tuple");
    return NULL;
  }}

  // extract launch metadata
  if (launch_enter_hook != Py_None){{
    PyObject* args = Py_BuildValue("(O)", launch_metadata);
    PyObject* ret = PyObject_CallObject(launch_enter_hook, args);
    Py_DECREF(args);
    if (!ret)
      return NULL;
  }}

  // raise exception asap
  {"; ".join([f"DevicePtrInfo ptr_info{i} = getPointer(_arg{i}, {i}); if (!ptr_info{i}.valid) return NULL;" if ty[0] == "*" else "" for i, ty in signature.items()])};
  Py_BEGIN_ALLOW_THREADS;
  _launch(gridX, gridY, gridZ, num_warps, num_ctas, clusterDimX, clusterDimY, clusterDimZ, shared_memory, (CUstream)_stream, (CUfunction)_function{', ' + ', '.join(f"ptr_info{i}.dev_ptr" if ty[0]=="*" else f"_arg{i}"for i, ty in signature.items()) if len(signature) > 0 else ''});
  Py_END_ALLOW_THREADS;
  if (PyErr_Occurred()) {{
    return NULL;
  }}

  if(launch_exit_hook != Py_None){{
    PyObject* args = Py_BuildValue("(O)", launch_metadata);
    PyObject* ret = PyObject_CallObject(launch_exit_hook, args);
    Py_DECREF(args);
    if (!ret)
      return NULL;

  }}

  // return None
  Py_INCREF(Py_None);
  return Py_None;
}}

static PyMethodDef ModuleMethods[] = {{
  {{"launch", launch, METH_VARARGS, "Entry point for all kernels with this signature"}},
  {{NULL, NULL, 0, NULL}} // sentinel
}};

static struct PyModuleDef ModuleDef = {{
  PyModuleDef_HEAD_INIT,
  \"__triton_launcher\",
  NULL, //documentation
  -1, //size
  ModuleMethods
}};

PyMODINIT_FUNC PyInit___triton_launcher(void) {{
  PyObject *m = PyModule_Create(&ModuleDef);
  if(m == NULL) {{
    return NULL;
  }}
  PyModule_AddFunctions(m, ModuleMethods);
  return m;
}}
"""
```

然后在`compile_module_from_src`中保存为main.c文件

```python
def compile_module_from_src(src, name):
    key = hashlib.sha256(src.encode("utf-8")).hexdigest()
    cache = get_cache_manager(key)
    cache_path = cache.get_file(f"{name}.so")
    if cache_path is None:
        with tempfile.TemporaryDirectory() as tmpdir:
            src_path = os.path.join(tmpdir, "main.c")
            with open(src_path, "w") as f:
                f.write(src)
            so = _build(name, src_path, tmpdir, library_dirs(), include_dir, libraries)
            with open(so, "rb") as f:
                cache_path = cache.put(f.read(), f"{name}.so", binary=True)
    import importlib.util
    spec = importlib.util.spec_from_file_location(name, cache_path)
    mod = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(mod)
    return mod
```

也就是mod.launch方法调用了c实现的_launch函数，传入了CUfunction，但是这个实现里面没有load module和get function，而是直接接收了CUfunction指针，因此返回去找获取CUfunction的位置，能看到CompiledKernel的init中通过CudaDriver的load binary获得了CUfuntino

```python
self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, device)
```

它在driver.c中被绑定到了loadBinary函数上，如我们所料cuModuleLoadData和cuModuleGetFunction

```c++

static PyObject *loadBinary(PyObject *self, PyObject *args) {
  const char *name;
  const char *data;
  Py_ssize_t data_size;
  int shared;
  int device;
  if (!PyArg_ParseTuple(args, "ss#ii", &name, &data, &data_size, &shared,
                        &device)) {
    return NULL;
  }
  CUfunction fun;
  CUmodule mod;
  int32_t n_regs = 0;
  int32_t n_spills = 0;
  // create driver handles
  CUcontext pctx = 0;

  Py_BEGIN_ALLOW_THREADS;
  CUDA_CHECK_AND_RETURN_NULL_ALLOW_THREADS(cuCtxGetCurrent(&pctx));
  if (!pctx) {
    CUDA_CHECK_AND_RETURN_NULL_ALLOW_THREADS(
        cuDevicePrimaryCtxRetain(&pctx, device));
    CUDA_CHECK_AND_RETURN_NULL_ALLOW_THREADS(cuCtxSetCurrent(pctx));
  }

  CUDA_CHECK_AND_RETURN_NULL_ALLOW_THREADS(cuModuleLoadData(&mod, data));
  CUDA_CHECK_AND_RETURN_NULL_ALLOW_THREADS(
      cuModuleGetFunction(&fun, mod, name));
  // get allocated registers and spilled registers from the function
  CUDA_CHECK_AND_RETURN_NULL_ALLOW_THREADS(
      cuFuncGetAttribute(&n_regs, CU_FUNC_ATTRIBUTE_NUM_REGS, fun));
  CUDA_CHECK_AND_RETURN_NULL_ALLOW_THREADS(
      cuFuncGetAttribute(&n_spills, CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES, fun));
  n_spills /= 4;
  // set dynamic shared memory if necessary
  int shared_optin;
  CUDA_CHECK_AND_RETURN_NULL_ALLOW_THREADS(cuDeviceGetAttribute(
      &shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN,
      device));
  if (shared > 49152 && shared_optin > 49152) {
    CUDA_CHECK_AND_RETURN_NULL_ALLOW_THREADS(
        cuFuncSetCacheConfig(fun, CU_FUNC_CACHE_PREFER_SHARED));
    int shared_total, shared_static;
    CUDA_CHECK_AND_RETURN_NULL_ALLOW_THREADS(cuDeviceGetAttribute(
        &shared_total, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR,
        device));
    CUDA_CHECK_AND_RETURN_NULL_ALLOW_THREADS(cuFuncGetAttribute(
        &shared_static, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun));
    CUDA_CHECK_AND_RETURN_NULL_ALLOW_THREADS(
        cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES,
                           shared_optin - shared_static));
  }
  Py_END_ALLOW_THREADS;

  if (PyErr_Occurred()) {
    return NULL;
  }
  return Py_BuildValue("(KKii)", (uint64_t)mod, (uint64_t)fun, n_regs,
                       n_spills);
}
```

至此已经完成了kernel的调用过程。

## Cache过程

### Kernel Cache

在前面部分实际上已经看到过cache过程，为了避免kernel在多次launch过程中重复编译，需要进行cache

回到最开始JITFunction.run中


```python
class JITFunction(KernelInterface[T]):

    def run(self, *args, grid, warmup, **kwargs):
        # parse options
        # 获取当前设备
        device = driver.active.get_current_device()
        # 获取当前设备的stream
        stream = driver.active.get_current_stream(device)
        kwargs["debug"] = self.debug

        # Execute pre run hooks with args and kwargs
        # 执行hook，用户可以通过add_kernel.add_pre_run_hook(my_hook)方式添加pre_run_hook
        for hook in self.pre_run_hooks:
            hook(*args, **kwargs)
        
        # todo：不清楚binder的作用
        if self.binder is None:
            self.create_binder()

        bound_args, sig_and_spec, constexpr_vals, non_constexpr_vals, excess_kwargs = self.binder(*args, **kwargs)

        # compute cache key
        # 类似："*fp16*fp16*fp16*fp16i32i32DDDDDD((32, 128), {'num_ctas': 1, 'debug': None})"的string作为key
        key = ''.join(sig_and_spec) + str((constexpr_vals, excess_kwargs))
        # self.cache用于存储编译后的kernel，如果是第一次执行，cache就是一个空的defaultdict
        kernel = self.cache[device].get(key, None)

        # 第一次执行，没有kernel cache，因此要进入编译流程
        if kernel is None:

            ...

            kernel = self.compile(
                src,
                target=target,
                options=options.__dict__,
            )
            # 将编译完的kernel存入cache中，方便之后再次执行
            self.cache[device][key] = kernel

        ...

        return kernel
```

首先回去cache中查找`kernel = self.cache[device].get(key, None)`，如果没有找到才进入后面的编译过程，如果找到了则直接执行kernel launch

cache本身也非常简单就是一个普通的dict：`self.cache = defaultdict(dict)`

cache的key是由参数类型和常量值来组合的：`*fp32*fp32*fp32i32DDDD((1024,), {'debug': None})`

todo：但是这里比较疑惑，不会出现参数列表完全相同的kernel嘛？

### File Cache

除了kernel的cache，还有一个用于存储culaunchkernel的main.c等文件的FileCacheManager，triton中通过get_cache_manager有三个地方使用了它：

1. kernel metadata json file
2. __triton_launcher.so：culaunchkernel main.c
3. compile中保存每一步的ir文件

```python
__cache_cls = FileCacheManager
__cache_cls_nme = "DEFAULT"


def get_cache_manager(key) -> CacheManager:
    import os

    user_cache_manager = os.environ.get("TRITON_CACHE_MANAGER", None)
    global __cache_cls
    global __cache_cls_nme

    if user_cache_manager is not None and user_cache_manager != __cache_cls_nme:
        module_path, clz_nme = user_cache_manager.split(":")
        module = importlib.import_module(module_path)
        __cache_cls = getattr(module, clz_nme)
        __cache_cls_nme = user_cache_manager

    return __cache_cls(key)
```

`__cache_cls(key)`会直接调用FileCacheManager的__init__方法，传入key，get_cache_manager创建一个对应key的file cache对象，里面保存了key和cache dir

```python
class FileCacheManager(CacheManager):

    def __init__(self, key, override=False, dump=False):
        self.key = key
        self.lock_path = None
        if dump:
            self.cache_dir = default_dump_dir()
            self.cache_dir = os.path.join(self.cache_dir, self.key)
            self.lock_path = os.path.join(self.cache_dir, "lock")
            os.makedirs(self.cache_dir, exist_ok=True)
        elif override:
            self.cache_dir = default_override_dir()
            self.cache_dir = os.path.join(self.cache_dir, self.key)
        else:
            # create cache directory if it doesn't exist
            self.cache_dir = os.getenv("TRITON_CACHE_DIR", "").strip() or default_cache_dir()
            if self.cache_dir:
                self.cache_dir = os.path.join(self.cache_dir, self.key)
                self.lock_path = os.path.join(self.cache_dir, "lock")
                os.makedirs(self.cache_dir, exist_ok=True)
            else:
                raise RuntimeError("Could not create or locate cache dir")
```

对于ir module的cache，它的key是根据src（也就是输入的python src的name）、backend和各种环境信息来生成的hash：

```python
key = f"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}"
hash = hashlib.sha256(key.encode("utf-8")).hexdigest()
```

FileCacheManager使用中通过put成员方法，保存到cache_dir下

```c++
  def put(self, data, filename, binary=True) -> str:
      if not self.cache_dir:
          raise RuntimeError("Could not create or locate cache dir")
      binary = isinstance(data, bytes)
      if not binary:
          data = str(data)
      assert self.lock_path is not None
      filepath = self._make_path(filename)
      # Random ID to avoid any collisions
      rnd_id = str(uuid.uuid4())
      # we use the PID in case a bunch of these around so we can see what PID made it
      pid = os.getpid()
      # use tempfile to be robust against program interruptions
      temp_path = f"{filepath}.tmp.pid_{pid}_{rnd_id}"
      mode = "wb" if binary else "w"
      with open(temp_path, mode) as f:
          f.write(data)
      # Replace is guaranteed to be atomic on POSIX systems if it succeeds
      # so filepath cannot see a partial write
      os.replace(temp_path, filepath)
      return filepath
```

作为用户，可以通过`TRITON_KERNEL_DUMP`、`TRITON_CACHE_DIR`等环境变量来控制cache的保存，生成一些文件名是hash值的文件夹到我们指定或者默认路径下：

```shell
(Pdb) p os.path.join(Path.home(), ".triton", "cache")
'/root/.triton/cache'`）
```

## Triton Autotune

在Triton中支持了两种超参数搜索的方法

- triton.autotune
- triton.heuristics

首先看autotune的使用，可以使用`export TRITON_PRINT_AUTOTUNING=1`来查看每个kernel的时间和最佳配置。

```python
@triton.autotune(configs=[
    triton.Config(kwargs={'BLOCK_SIZE': 128}, num_warps=4),
    triton.Config(kwargs={'BLOCK_SIZE': 1024}, num_warps=8),
  ],
  key=['x_size'] # the two above configs will be evaluated anytime
                 # the value of x_size changes
)
@triton.jit
def kernel(x_ptr, x_size, **META):
    BLOCK_SIZE = META['BLOCK_SIZE']
```

triton.config中默认支持了五种参数，以及用户自定义的参数：

- num_warps：kernel使用的warps数（隐含了使用的线程数，例如warps=8，则意味着并行的线程为8*32个=256个）
- num_stages：控制编译阶段
- num_ctas：SM90（Hopper架构）开始引入了block cluster的概念，num_ctas用于指定一个block cluster中的blocks数量。block cluster提供跨thread block的shared memory访问。Hopper架构在L1和L2 Cache之间增加了一层SM to SM network，thread block cluster内部的SM可以通过该网络访问其他SM的shared memory。
- maxnreg：一个线程可以使用的寄存器数量，直接对应到了ptx中的.maxnreg
- pre_hook：kernel执行前hook


从Autotuner的run函数中可以看出，Autotuner的核心是通过`self._bench`函数计算获得每组config的下的kernel时间，选择时间最短的config作为kernel实际执行的配置。通过self.cache记录key对应的config。key为kernel的参数值，因为不同的参数值影响block size等数据划分，进行需要的config不同，所以要根据参数值来进行cache

```python
{(512, 512, 512, 'torch.float16', 'torch.float16', 'torch.float16'): <triton.runtime.autotuner.Config object at 0x7fa4f47f4a90>}
```

run的整体逻辑

```python
def run(self, *args, **kwargs):
    self.nargs = dict(zip(self.arg_names, args))
    used_cached_result = True
    if len(self.configs) > 1:
        # 根据当前kernel所有参数值和dtype来获取key
        all_args = {**self.nargs, **kwargs}
        _args = []
        for name in self.arg_names:
            if name in all_args:
                _args.append(all_args[name])
        key = [_args[i] for i in self.key_idx]
        for arg in _args:
            if hasattr(arg, "dtype"):
                key.append(str(arg.dtype))
        key = tuple(key)
        # 如果key没有出现在cache中，进行bench
        if key not in self.cache:
            # prune configs
            used_cached_result = False
            pruned_configs = self.prune_configs(kwargs)
            bench_start = time.time()
            timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
            bench_end = time.time()
            self.bench_time = bench_end - bench_start
            self.cache[key] = builtins.min(timings, key=timings.get)
            self.pre_hook(args, reset_only=True)
            self.configs_timings = timings
        # 获取cache中对应的config
        config = self.cache[key]
    else:
        config = self.configs[0]
    self.best_config = config
    if os.getenv("TRITON_PRINT_AUTOTUNING", None) == "1" and not used_cached_result:
        print(f"Triton autotuning for function {self.base_fn.__name__} finished after "
              f"{self.bench_time:.2f}s; best config selected: {self.best_config};")
    # 执行pre hook函数
    if config.pre_hook is not None:
        config.pre_hook({**self.nargs, **kwargs, **config.all_kwargs()})
    
    # 执行被封装的函数
    ret = self.fn.run(
        *args,
        **kwargs,
        **config.all_kwargs(),
    )
    self.nargs = None
    return ret
```

在self._bench中做的就是按照config来执行kernel，记录执行时间

```python
def _bench(self, *args, config, **meta):
    from ..compiler.errors import CompileTimeAssertionFailure

    # check for conflicts, i.e. meta-parameters both provided
    # as kwargs and by the autotuner
    conflicts = meta.keys() & config.kwargs.keys()
    if conflicts:
        raise ValueError(f"Conflicting meta-parameters: {', '.join(conflicts)}."
                          " Make sure that you don't re-define auto-tuned symbols.")
    # augment meta-parameters with tunable ones
    current = dict(meta, **config.all_kwargs())
    full_nargs = {**self.nargs, **current}

    def kernel_call():
        if config.pre_hook:
            config.pre_hook(full_nargs)
        self.pre_hook(args)
        try:
            self.fn.run(
                *args,
                **current,
            )
        except Exception as e:
            try:
                self.post_hook(args, exception=e)
            finally:
                # Throw exception raised by `self.fn.run`
                raise

        self.post_hook(args, exception=None)

    try:
        if self.use_cuda_graph:
            import torch
            with torch.cuda.stream(torch.cuda.Stream()):
                bench_res = do_bench_cudagraph(kernel_call, rep=self.num_reps, return_mode="median")
            return bench_res
        return do_bench(kernel_call, warmup=self.num_warmups, rep=self.num_reps, quantiles=(0.5, 0.2, 0.8))
    except (OutOfResources, CompileTimeAssertionFailure):
        return float("inf") if self.use_cuda_graph else [float("inf"), float("inf"), float("inf")]
```

核心代码在kernel_call和do_bench中，do_bench会根据传入的期望的warmup和rep时间来计算需要的warmup次数和执行次数，返回执行次数对应的分位数

```python
def do_bench(fn, warmup=25, rep=100, grad_to_none=None, quantiles=None, fast_flush=True, return_mode="mean"):
    """
    Benchmark the runtime of the provided function. By default, return the median runtime of :code:`fn` along with
    the 20-th and 80-th performance percentile.

    :param fn: Function to benchmark
    :type fn: Callable
    :param warmup: Warmup time (in ms)
    :type warmup: int
    :param rep: Repetition time (in ms)
    :type rep: int
    :param grad_to_none: Reset the gradient of the provided tensor to None
    :type grad_to_none: torch.tensor, optional
    :param quantiles: Performance percentile to return in addition to the median.
    :type quantiles: list[float]
    :param fast_flush: Use faster kernel to flush L2 between measurements
    :type fast_flush: bool
    """
    assert return_mode in ["min", "max", "mean", "median"]
    import torch

    fn()
    torch.cuda.synchronize()

    # We maintain a buffer of 256 MB that we clear
    # before each kernel call to make sure that the L2
    # doesn't contain any input data before the run
    if fast_flush:
        cache = torch.empty(int(256e6 // 4), dtype=torch.int, device='cuda')
    else:
        cache = torch.empty(int(256e6), dtype=torch.int8, device='cuda')

    # Estimate the runtime of the function
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)
    start_event.record()
    # 先执行5次获得估计的平均执行时间
    for _ in range(5):
        cache.zero_()
        fn()
    end_event.record()
    torch.cuda.synchronize()
    estimate_ms = start_event.elapsed_time(end_event) / 5

    # compute number of warmup and repeat
    # 根据期望的warmup时间和repeat时间除以估计的平均执行时间，获得对应的warmup和repeat执行次数
    n_warmup = max(1, int(warmup / estimate_ms))
    n_repeat = max(1, int(rep / estimate_ms))
    start_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]
    end_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]
    # Warm-up
    for _ in range(n_warmup):
        fn()
    # Benchmark
    for i in range(n_repeat):
        # we don't want `fn` to accumulate gradient values
        # if it contains a backward pass. So we clear the
        # provided gradients
        if grad_to_none is not None:
            for x in grad_to_none:
                x.grad = None
        # we clear the L2 cache before each run
        cache.zero_()
        # record time of `fn`
        start_event[i].record()
        fn()
        end_event[i].record()
    # Record clocks
    torch.cuda.synchronize()
    # 获取rep的执行时间列表
    times = torch.tensor([s.elapsed_time(e) for s, e in zip(start_event, end_event)], dtype=torch.float)
    if quantiles is not None:
        ret = torch.quantile(times, torch.tensor(quantiles, dtype=torch.float)).tolist()
        if len(ret) == 1:
            ret = ret[0]
        return ret
    return getattr(torch, return_mode)(times).item()
```

回到run方法，获取完每个config的timing之后，根据选择configs中时间最短的config，存入cache中，之后每次执行，从cache中直接获取，从而绕开bench的逻辑。

```python
key = tuple(key)
if key not in self.cache:
    # prune configs
    used_cached_result = False
    pruned_configs = self.prune_configs(kwargs)
    bench_start = time.time()
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
    bench_end = time.time()
    self.bench_time = bench_end - bench_start
    self.cache[key] = builtins.min(timings, key=timings.get)
    self.pre_hook(args, reset_only=True)
    self.configs_timings = timings
config = self.cache[key]
```

完成autotuner的cache之后，继续调用self.fn.run方法

```python
ret = self.fn.run(
      *args,
      **kwargs,
      **config.all_kwargs(),
  )
```

## Triton Heuristic

heuristic是对应autotune的另一个超参搜索方式

它指定了参数的计算方式。

```python
@triton.heuristics(values={'BLOCK_SIZE': lambda args: 2 ** int(math.ceil(math.log2(args[1])))})
@triton.jit
def kernel(x_ptr, x_size, **META):
    BLOCK_SIZE = META['BLOCK_SIZE'] # smallest power-of-two >= x_size
```

它的处理逻辑比较简单，直接根据kernel的参数、指定的公式，计算获得超参数对应的值

```python
class Heuristics(KernelInterface):

    def __init__(self, fn, arg_names, values) -> None:
        self.fn = fn
        self.values = values
        self.arg_names = arg_names

    def run(self, *args, **kwargs):
        for v, heur in self.values.items():
            kwargs[v] = heur({**dict(zip(self.arg_names, args)), **kwargs})
        return self.fn.run(*args, **kwargs)
```

其中

```python
{**dict(zip(self.arg_names, args)), **kwargs}
```

展开后

```python
{'x_ptr': tensor([0.3990, 0.5167, 0.0249,  ..., 0.0424, 0.5099, 0.7277], device='cuda:0'), 'y_ptr': tensor([0.9722, 0.7910, 0.4690,  ..., 0.6300, 0.7042, 0.2457], device='cuda:0'), 'output_ptr': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'n_elements': 98432, 'grid': <function add.<locals>.<lambda> at 0x7efe3e829430>, 'warmup': False, 'BLOCK_SIZE': 131072}
```

使用时，`args["n_elements"]`即为`{**dict(zip(self.arg_names, args)), **kwargs}`中对应的参数

```python
@triton.heuristics(values={'BLOCK_SIZE': lambda args: 2 ** int(math.ceil(math.log2(args["n_elements"])))})
@triton.jit
def add_kernel(
    x_ptr,  # *Pointer* to first input vector.
    y_ptr,  # *Pointer* to second input vector.
    output_ptr,  # *Pointer* to output vector.
    n_elements,  # Size of the vector.
    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
    # NOTE: `constexpr` so it can be used as a shape value.
):
```

## References

- [OpenAI/Triton MLIR 迁移工作简介 - by Chunwei Yan from Nivida Tirton开发人员](https://superjomn.github.io/posts/triton-mlir-publish/) 高质量
- [机器学习编译器代码生成相关 MLIR Dialect - by Lei MLIR开发人员](https://www.lei.chat/zh/posts/mlir-codegen-dialects-for-machine-learning-compilers/) 高质量
- [MLIR Tutorial by 周可行 北京大学](https://github.com/KEKE046/mlir-tutorial) 高质量
- [MLIR Operation Definition Specification (ODS)](https://mlir.llvm.org/docs/DefiningDialects/Operations/)
- [MLIR Defining Dialects](https://mlir.llvm.org/docs/DefiningDialects/)
- [Triton Document Dialect](https://triton-lang.org/main/dialects/)
- [窥探Triton的lower(一)](https://zhuanlan.zhihu.com/p/695171704)
- [窥探Triton的lower(二)](https://zhuanlan.zhihu.com/p/695255185)
- [窥探Triton的lower(三)](https://zhuanlan.zhihu.com/p/696133729)
- [Triton Dialect](https://zhuanlan.zhihu.com/p/685209384)
- [2023 EuroLLVM - MLIR Dialect Design and Composition for Front-End Compilers](https://www.youtube.com/watch?v=hIt6J1_E21c)
- [LLVM系列第二十七章：理解IRBuilder](https://blog.csdn.net/Zhanglin_Wu/article/details/125955324)
- [LLVM IRBuilder Doc](https://llvm.org/doxygen/classllvm_1_1IRBuilder.html#details)
- [Hopper 架构特性学习笔记 Part1 Distributed Shared Memory](https://zhuanlan.zhihu.com/p/708645371?utm_psn=1797581865554698240)


# Triton笔记四：更多的例子

- [Triton笔记四：更多的例子](#triton笔记四更多的例子)
  - [Low-memory Dropout](#low-memory-dropout)
  - [Layer Normalization](#layer-normalization)
    - [Layer Norm Forward](#layer-norm-forward)
    - [Layer Norm Backward](#layer-norm-backward)
  - [References](#references)

## Low-memory Dropout

实现见[seeded_dropout.py](../triton-examples/03.dropout/seeded_dropout.py)

triton中提供了rand的op来帮助我们实现seeded dropout，这样我们就可以省略掉搬迁数据，内存拷贝的操作，而且可以通过seed对齐kernel调用的结果。

1. smaller memory footprint
2. less data movement
3. simplifies the management of persisting randomness across multiple invocations of the kernels.

```python
tl.rand(seed, offset)
```

## Layer Normalization

在进行layer norm的实现前，需要掌握layer norm的前传和反传的公式，前向传播大家很熟悉，反向传播的推导过程可以参考：[Backpropagation through a layer norm](https://liorsinai.github.io/mathematics/2022/05/18/layernorm.html)和[手推公式之“层归一化”梯度](https://developer.aliyun.com/article/978388)。

### Layer Norm Forward

### Layer Norm Backward

作者实现时将layer norm backward分为了两个阶段，称为parallel reduction strategy：

1. 每一个program它负责的row的$\delta_w$和$\delta_b$放入一个buffer中，这个buffer会一直在L2 Cache中
2. reduce所有的buffer，获得$\delta_w$和$\delta_b$

下图中假设输入x有M行，N列，2个buffer（GROUP_SIZE_M=2）

![layer norm two stage.png](../.images/layer%20norm%20two%20stage.png)

## References

- [【BBuf的CUDA笔记】十四，OpenAI Triton入门笔记二](https://zhuanlan.zhihu.com/p/682343740) 高质量
- [手推公式之“层归一化”梯度](https://developer.aliyun.com/article/978388)
- [Backpropagation through a layer norm](https://liorsinai.github.io/mathematics/2022/05/18/layernorm.html)
- [The Fundamentals of Autograd](https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html#the-fundamentals-of-autograd)
- [What's Automatic Differentiation?](https://huggingface.co/blog/andmholm/what-is-automatic-differentiation)
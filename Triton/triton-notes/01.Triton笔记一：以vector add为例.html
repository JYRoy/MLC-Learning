<!DOCTYPE html><html><head>
      <title>01.Triton笔记一：以vector add为例</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
      
      
      
      
      
      <style>
      code[class*=language-],pre[class*=language-]{color:#333;background:0 0;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.4;-moz-tab-size:8;-o-tab-size:8;tab-size:8;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{padding:.8em;overflow:auto;border-radius:3px;background:#f5f5f5}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal;background:#f5f5f5}.token.blockquote,.token.comment{color:#969896}.token.cdata{color:#183691}.token.doctype,.token.macro.property,.token.punctuation,.token.variable{color:#333}.token.builtin,.token.important,.token.keyword,.token.operator,.token.rule{color:#a71d5d}.token.attr-value,.token.regex,.token.string,.token.url{color:#183691}.token.atrule,.token.boolean,.token.code,.token.command,.token.constant,.token.entity,.token.number,.token.property,.token.symbol{color:#0086b3}.token.prolog,.token.selector,.token.tag{color:#63a35c}.token.attr-name,.token.class,.token.class-name,.token.function,.token.id,.token.namespace,.token.pseudo-class,.token.pseudo-element,.token.url-reference .token.variable{color:#795da3}.token.entity{cursor:help}.token.title,.token.title .token.punctuation{font-weight:700;color:#1d3e81}.token.list{color:#ed6a43}.token.inserted{background-color:#eaffea;color:#55a532}.token.deleted{background-color:#ffecec;color:#bd2c00}.token.bold{font-weight:700}.token.italic{font-style:italic}.language-json .token.property{color:#183691}.language-markup .token.tag .token.punctuation{color:#333}.language-css .token.function,code.language-css{color:#0086b3}.language-yaml .token.atrule{color:#63a35c}code.language-yaml{color:#183691}.language-ruby .token.function{color:#333}.language-markdown .token.url{color:#795da3}.language-makefile .token.symbol{color:#795da3}.language-makefile .token.variable{color:#183691}.language-makefile .token.builtin{color:#0086b3}.language-bash .token.keyword{color:#0086b3}pre[data-line]{position:relative;padding:1em 0 1em 3em}pre[data-line] .line-highlight-wrapper{position:absolute;top:0;left:0;background-color:transparent;display:block;width:100%}pre[data-line] .line-highlight{position:absolute;left:0;right:0;padding:inherit 0;margin-top:1em;background:hsla(24,20%,50%,.08);background:linear-gradient(to right,hsla(24,20%,50%,.1) 70%,hsla(24,20%,50%,0));pointer-events:none;line-height:inherit;white-space:pre}pre[data-line] .line-highlight:before,pre[data-line] .line-highlight[data-end]:after{content:attr(data-start);position:absolute;top:.4em;left:.6em;min-width:1em;padding:0 .5em;background-color:hsla(24,20%,50%,.4);color:#f4f1ef;font:bold 65%/1.5 sans-serif;text-align:center;vertical-align:.3em;border-radius:999px;text-shadow:none;box-shadow:0 1px #fff}pre[data-line] .line-highlight[data-end]:after{content:attr(data-end);top:auto;bottom:.4em}html body{font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ol,html body>ul{margin-bottom:16px}html body ol,html body ul{padding-left:2em}html body ol.no-list,html body ul.no-list{padding:0;list-style-type:none}html body ol ol,html body ol ul,html body ul ol,html body ul ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:700;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::after,html body code::before{letter-spacing:-.2em;content:'\00a0'}html body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:after,html body pre code:before,html body pre tt:after,html body pre tt:before{content:normal}html body blockquote,html body dl,html body ol,html body p,html body pre,html body ul{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body code,html body pre{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview ul{list-style:disc}.markdown-preview ul ul{list-style:circle}.markdown-preview ul ul ul{list-style:square}.markdown-preview ol{list-style:decimal}.markdown-preview ol ol,.markdown-preview ul ol{list-style-type:lower-roman}.markdown-preview ol ol ol,.markdown-preview ol ul ol,.markdown-preview ul ol ol,.markdown-preview ul ul ol{list-style-type:lower-alpha}.markdown-preview .newpage,.markdown-preview .pagebreak{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center!important}.markdown-preview:not([data-for=preview]) .code-chunk .code-chunk-btn-group{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .status{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0;min-height:100vh}@media screen and (min-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{font-size:14px!important;padding:1em}}@media print{html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div,html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% - 300px);padding:2em calc(50% - 457px - 300px / 2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
      <!-- The content below will be included at the end of the <head> element. --><script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function () {
    // your code here
  });
</script></head><body for="html-export">
    
    
      <div class="crossnote markdown-preview  ">
      
<h1 id="triton笔记一">Triton笔记一 </h1>
<ul>
<li><a href="#triton%E7%AC%94%E8%AE%B0%E4%B8%80">Triton笔记一</a>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#triton%E5%AE%89%E8%A3%85">Triton安装</a></li>
<li><a href="#gpu-architecture">GPU Architecture</a></li>
<li><a href="#volta-v100-sm">Volta V100 SM</a></li>
<li><a href="#memory-hierachy">Memory Hierachy</a>
<ul>
<li><a href="#gpu-%E7%BC%96%E7%A8%8B%E7%9A%84%E4%B8%89%E4%B8%AA%E6%8C%91%E6%88%98">GPU 编程的三个挑战</a></li>
</ul>
</li>
<li><a href="#programming-model">Programming Model</a></li>
<li><a href="#vector-add-%E4%BE%8B%E5%AD%90">Vector ADD 例子</a>
<ul>
<li><a href="#vector-add-kernel">Vector ADD Kernel</a></li>
<li><a href="#vector-add-host-code">Vector ADD Host Code</a></li>
<li><a href="#vector-add-benchmark">Vector ADD Benchmark</a></li>
<li><a href="#vector-add-triton-vs-ptx">Vector ADD Triton vs PTX</a></li>
</ul>
</li>
<li><a href="#heigh-level-system-architecture">Heigh-level system architecture</a></li>
<li><a href="#compiler-backend">Compiler backend</a>
<ul>
<li><a href="#triton-vs-cuda-programming-model">Triton vs CUDA Programming Model</a></li>
<li><a href="#machine-independent-passes">Machine-independent Passes</a>
<ul>
<li><a href="#pre-fetching">Pre-fetching</a></li>
<li><a href="#tile-level-peephole-optimization">Tile-level Peephole Optimization</a></li>
</ul>
</li>
<li><a href="#machine-dependent-passes">Machine-dependent Passes</a>
<ul>
<li><a href="#hierarchy-tiling">Hierarchy Tiling</a></li>
<li><a href="#memory-coalescing">Memory Coalescing</a></li>
<li><a href="#shared-memory-allocation">Shared Memory Allocation</a></li>
<li><a href="#shared-memory-synchronization">Shared Memory Synchronization</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#how-does-it-fit-in-a-dnn-stack">How does it fit in a DNN stack?</a></li>
<li><a href="#how-does-the-compiler-simplify-users-work">How does the compiler simplify user’s work?</a></li>
<li><a href="#references">References</a></li>
</ul>
</li>
</ul>
<h2 id="overview">Overview </h2>
<p>Triton最早始源于<a href="https://github.com/ptillet?tab=repositories">Philippe Tillet</a>在哈佛攻读phd时在2019年的International Workshop on Machine Learning and Programming Language上发表的一篇论文《<a href="http://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf">Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations</a> 》，二作和三作都是他的导师。</p>
<p><img src="../.images/Triton%20paper%20title.png" alt="Triton paper title.png"></p>
<p>这个时候Triton还只是基于C的tile-level（我理解tile就是分块，也就是现在triton里说的block program）的DSL，后来作者加入了OpenAI，继续Triton的工作，Triton就演变成了我们现在看到的样子。</p>
<p>2021年7月，Triton 1.0发布，Philippe Tillet在OpenAI官网发布了一个blog:<a href="https://openai.com/index/triton/">Introducing Triton: Open-source GPU programming for neural networks</a>来正式介绍Triton的设计想法和目标。</p>
<p>What is triton？</p>
<p>Triton是一个开源的Python-like语言（实际上我更想称它为Python-based的DSL语言），它用于帮助我们在没有CUDA经验的情况下就能写出高效<s>GPU代码</s>(kernel)，而达成这个目标的方法就是编译。所以Triton不只是一个Python-based的DSL语言，同时又是一个编译器。由于GPU编程本身的复杂性和难度，使得用户写出一个高效的算法变得比较困难，虽然现在有一些系统来简化这个过程，但是依然缺乏灵活性，因此Triton应运而生。</p>
<blockquote>
<p>We’re releasing Triton 1.0, an open-source Python-like programming language which enables researchers with no CUDA experience to write highly efficient GPU code — most of the time on par with what an expert would be able to produce.</p>
</blockquote>
<p>更具体来说，Triton想要用户用较少的努力就可以写出一个达到硬件性能巅峰的kernel。比如要写一个FP16的矩阵乘法的kernel，让它的性能达到cublas的水平，只需要不到25行代码。甚至能写出比Torch的性能高两倍的kernel。</p>
<p>除了Triton，还有非常多DNN领域的DSL和编译器：</p>
<ul>
<li>Tensor-level IRs：XLA，GLOW</li>
<li>polyhedral machinery：Tensor Comprehensions，Diesel</li>
<li>loop synthesis techniques：Halide，TVM，PlaidML</li>
</ul>
<p>这些DSL虽然能够作为完成的系统工程应用在非常广泛的模型上，但是它们的性能依然要差于厂商专门优化过的库，下面是Triton论文中给出的2019年时测试的结果，可以看到在矩阵乘的例子上，Triton已经非常接近cublas 10的结果了。</p>
<p><img src="../.images/Performance%20of%20C%20vs%20Roofline.png" alt="Performance of C vs Roofline.png"></p>
<p>横轴写的TFLOP/GB的含义不明，姑且认为就是N值，纵轴表示在当前计算量下生成的kernel的在GTX1070上的TFLOPS，roofline则是1070的理论TFLOPS上限。（TFLOPS：teraFLOPS，每秒一万亿 (=10^12) 次的浮点运算）。</p>
<p>社区中也有非常多的完全基于Triton实现的Python项目，例如：<a href="https://github.com/ModelTC/lightllm/tree/main">LightLLM</a>和智源的<a href="https://github.com/FlagOpen/FlagGems">FlagGem</a></p>
<p>智源目前的工作，利用triton开发多平台的算子库打通不同的芯片生态。</p>
<p>核心还是利用llvm的多平台+triton的tile基础，来保证兼容性和性能。</p>
<p><img src="../.images/Triton%20OP%20on%20multi-arch.png" alt="多元生态共建打通编程接口和算子库"></p>
<h2 id="triton安装">Triton安装 </h2>
<p>From pips</p>
<pre data-role="codeBlock" data-info="shell" class="language-shell shell"><code>pip <span class="token function">install</span> triton
</code></pre><p>From source</p>
<pre data-role="codeBlock" data-info="shell" class="language-shell shell"><code><span class="token function">git</span> clone https://github.com/triton-lang/triton.git<span class="token punctuation">;</span>
<span class="token builtin class-name">cd</span> triton/python<span class="token punctuation">;</span>
pip <span class="token function">install</span> ninja cmake wheel<span class="token punctuation">;</span> <span class="token comment"># build-time dependencies</span>
pip <span class="token function">install</span> <span class="token parameter variable">-e</span> <span class="token builtin class-name">.</span>
</code></pre><h2 id="gpu-architecture">GPU Architecture </h2>
<h2 id="volta-v100-sm">Volta V100 SM </h2>
<p><img src="../.images/V100%20SM%20Arch.png" alt="V100 SM Arch"></p>
<p>V100上设计的一个SM中有4个subcore（sub-partation），每个subcore中结构是相同的。</p>
<table>
<thead>
<tr>
<th></th>
<th>V100</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP32 units</td>
<td>64</td>
</tr>
<tr>
<td>FP64 units</td>
<td>32</td>
</tr>
<tr>
<td>INT32 units</td>
<td>64</td>
</tr>
<tr>
<td>Tensor Cores</td>
<td>8</td>
</tr>
<tr>
<td>Register File</td>
<td>256KB</td>
</tr>
<tr>
<td>Unified L1/Shared Memory</td>
<td>128K</td>
</tr>
<tr>
<td>Active Threads</td>
<td>2048</td>
</tr>
</tbody>
</table>
<p>Active Threads和cuda core没有直接关系，我们最多可以放2048个threads到SM上，但是 warp schedule每个时钟周期只能选择4*32个线程去执行。</p>
<p>SM：</p>
<ul>
<li>4个subcore</li>
<li>L1 instruction cache：4个subcore共享的</li>
<li>128kb的SRAM：用户可配置为一部分作为L1 Data Cache，另一部分作为Shared Memory使用</li>
<li>Texture：纹理单元，用于纹理数据的读取和过滤，可以理解为图形学领域的tensor</li>
</ul>
<p>subcore：</p>
<ul>
<li>L0 Instruction Cache保存接下来执行的指令</li>
<li>Warp Schedule/Dispatch Unit：俗称前端，选择执行发送给后端执行，一个warp 32个线程为单位</li>
<li>Register File：寄存器，线程可以直接访问的寄存器数据，32bit为一个寄存器，也可以两个组合在一起成为一个64bit的寄存器</li>
<li>16个fp32的cuda core、16个int的cuda core、8个fp64的cuda core、2个Tensor core</li>
<li>Load/Store单元，用于和外部的L1、L2 Cache以及更外部的内存做交互</li>
<li>Sepical Function Unit：特殊函数做优化的</li>
</ul>
<h2 id="memory-hierachy">Memory Hierachy </h2>
<p><img src="../.images/CUDA%20Memory%20Hierarchy.png" alt="CUDA Memory Hierarchy"></p>
<p>可以理解为三级的结构：</p>
<ol>
<li>片上SM内：Registers、Shared Memory、L1/Tex Cache，片上的延迟比L2低3到5倍</li>
<li>片上SM共享：L2，所有SM对Global Memory的访问都需要经过L2，L2的带宽是global memory的三倍，延迟低三倍</li>
<li>片外：Global Memory</li>
</ol>
<h3 id="gpu-编程的三个挑战">GPU 编程的三个挑战 </h3>
<p>现代的GPU架构由三个主要部分构成：</p>
<ol>
<li>DRAM：Dynamic Random Access Memory，GPU中的主要内存，通常用作显存；</li>
<li>SRAM：Static Random Access Memory，价格更加昂贵，但是访问速度更快，所以主要用于存储临时数据和需要频繁访问的数据，L1/2 Cache；</li>
<li>ALUs：Arithmetic Logic Units，算术逻辑单元，负责执行算术运算（如加法、减法、乘法等）和逻辑运算（如与、或、非等）；</li>
</ol>
<p><img src="../.images/Basic%20architecture%20of%20a%20GPU.png" alt="Basic architecture of a GPU.png"></p>
<p>在优化CUDA代码的时候，需要综合考虑到这三个组件：</p>
<ol>
<li>DRAM的内存传输必须合并成大型事务，以利用现代内存接口的大总线宽度（内存合并访问）；</li>
<li>数据必须在重复使用前手动存储到SRAM，并且进行管理来最小化检索时的共享内存冲突（minimize shared memory bank conflicts）；</li>
<li>计算必须在SM之间和它们内部被小心地划分和调度，来提升指令或者线程级别的并行度，并且利用专用的ALU（如Tensor Core）；</li>
</ol>
<p>即使对一个非常有经验的CUDA开发人员来说这些问题依然是一个挑战。而Triton的目标就是能完全自动化这些优化，使得用户能够关注他们代码的高层逻辑。Triton 的目标是广泛适用，因此不会自动安排跨 SM 的工作 - 将一些重要的算法考虑因素（如tiling, inter-SM synchronization）留给开发人员自行决定。</p>
<p>文章中给出的图很明确地写明了CUDA和Triton的区别：</p>
<p><img src="../.images/Triton%20vs%20CUDA.png" alt="Triton vs CUDA.png"></p>
<p><img src="../.images/Triton%20vs%20CUDA%20vs%20Torch%20OP.png" alt="Triton vs CUDA vs Torch OP.png"></p>
<h2 id="programming-model">Programming Model </h2>
<p>在类似的DSL+Compiler的中，Triton和Numba最为接近（Numba我之前也没有详细看过）：</p>
<ol>
<li>Kernel通过被装饰的Python函数来定义；</li>
<li>以不同的program_id并行启动在grid实例上；</li>
</ol>
<p>Triton编程模型中最大的特点就是Triton不是SIMT的模式，而是只通过对block的操作来实现并行，从而规避了很多和CUDA Thread相关的问题（如memory coalescing, shared memory synchronization/conflicts, tensor core scheduling）</p>
<p>文中给出了一个Vector Add的例子，对比了Triton和Numba的使用</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code>BLOCK <span class="token operator">=</span> <span class="token number">512</span>

<span class="token comment"># This is a GPU kernel in Numba.</span>
<span class="token comment"># Different instances of this</span>
<span class="token comment"># function may run in parallel.</span>
<span class="token decorator annotation punctuation">@jit</span>
<span class="token keyword keyword-def">def</span> <span class="token function">add</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> Z<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">:</span>
   <span class="token comment"># In Numba/CUDA, each kernel </span>
   <span class="token comment"># instance itself uses an SIMT execution</span>
   <span class="token comment"># model, where instructions are executed in</span>
   <span class="token comment"># parallel for different values of threadIdx</span>
   tid <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x
   bid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x
   <span class="token comment"># scalar index</span>
   idx <span class="token operator">=</span> bid <span class="token operator">*</span> BLOCK <span class="token operator">+</span> tid
   <span class="token keyword keyword-if">if</span> <span class="token builtin">id</span> <span class="token operator">&lt;</span> N<span class="token punctuation">:</span>
     <span class="token comment"># There is no pointer in Numba.</span>
     <span class="token comment"># Z,X,Y are dense tensors</span>
     Z<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> X<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">+</span> Y<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>


<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
grid <span class="token operator">=</span> <span class="token punctuation">(</span>ceil_div<span class="token punctuation">(</span>N<span class="token punctuation">,</span> BLOCK<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
block <span class="token operator">=</span> <span class="token punctuation">(</span>BLOCK<span class="token punctuation">,</span><span class="token punctuation">)</span>
add<span class="token punctuation">[</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><pre data-role="codeBlock" data-info="python" class="language-python python"><code>BLOCK <span class="token operator">=</span> <span class="token number">512</span>

<span class="token comment"># This is a GPU kernel in Triton.</span>
<span class="token comment"># Different instances of this</span>
<span class="token comment"># function may run in parallel.</span>
<span class="token decorator annotation punctuation">@jit</span>
<span class="token keyword keyword-def">def</span> <span class="token function">add</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> Z<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">:</span>
   <span class="token comment"># In Triton, each kernel instance</span>
   <span class="token comment"># executes block operations on a</span>
   <span class="token comment"># single thread: there is no construct</span>
   <span class="token comment"># analogous to threadIdx</span>
   pid <span class="token operator">=</span> program_id<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
   <span class="token comment"># block of indices</span>
   idx <span class="token operator">=</span> pid <span class="token operator">*</span> BLOCK <span class="token operator">+</span> arange<span class="token punctuation">(</span>BLOCK<span class="token punctuation">)</span>
   mask <span class="token operator">=</span> idx <span class="token operator">&lt;</span> N
   <span class="token comment"># Triton uses pointer arithmetics  </span>
   <span class="token comment"># rather than indexing operators</span>
   x <span class="token operator">=</span> load<span class="token punctuation">(</span>X <span class="token operator">+</span> idx<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>
   y <span class="token operator">=</span> load<span class="token punctuation">(</span>Y <span class="token operator">+</span> idx<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>
   store<span class="token punctuation">(</span>Z <span class="token operator">+</span> idx<span class="token punctuation">,</span> x <span class="token operator">+</span> y<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
grid <span class="token operator">=</span> <span class="token punctuation">(</span>ceil_div<span class="token punctuation">(</span>N<span class="token punctuation">,</span> BLOCK<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
<span class="token comment"># no thread-block</span>
add<span class="token punctuation">[</span>grid<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><p>虽然这可能对并行计算没有很大的帮助（如elementwise）但是确实可以一定程度上简化复杂的计算。</p>
<h2 id="vector-add-例子">Vector ADD 例子 </h2>
<p>为了能够给后面的学习打一个好的基础，首先来具体分析一下Vector Add，以它为例子，入门Triton的基本语法。</p>
<p>这也是Triton官方文档的tutorial中的第一个例子，这个例子比上面的代码在变量命名、注释方面更加详细(完整代码…/triton-examples/00.vector-add.py)：</p>
<h3 id="vector-add-kernel">Vector ADD Kernel </h3>
<p>这个例子中涉及了使用triton编写kernel的几个要点：</p>
<ol>
<li>kernel函数必须要被<code>@trition.jit</code>装饰器所装饰（暂时先不关心jit内做了什么）</li>
<li>tirton的kernle支持的参数类型只有：torch Tensor、int、float、tl.constexpr</li>
<li>pid：program_id，每一个program处理一个block size大小的数据，对于vector add这个例子，如果输入是x[2048]个元素，block size设置为1024，则triton会使用两个program来处理数据
<ol>
<li>Triton中program的概念类似于CUDA中的grid，都是一个三维数组，在CUDA中，通过block.x/y/z来获取grid的坐标，在Triton中通过tl.program_id(axis=0/1/2)来获取program的坐标</li>
</ol>
</li>
<li>我们只需要考虑一个block中要处理哪些数据，然后通过offset和mask选出这些数据。这里的block并不用再细分thread的概念了，相比CUDA还是简单很多</li>
<li>load/store
<ol>
<li>通过load操作从源数据中读取当前block要处理的数据，load会从HBM中加载数据到共享内存或者本地内存中，这取决于Triton编译器</li>
<li>通过store操作将当前block的结果存储到输出中，store会从存储中将计算结果加载到HBM中</li>
</ol>
</li>
<li>和正常写python一些实现计算操作</li>
</ol>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-import">import</span> torch

<span class="token keyword keyword-import">import</span> triton
<span class="token keyword keyword-import">import</span> triton<span class="token punctuation">.</span>language <span class="token keyword keyword-as">as</span> tl

<span class="token decorator annotation punctuation">@triton<span class="token punctuation">.</span>jit</span>
<span class="token keyword keyword-def">def</span> <span class="token function">add_kernel</span><span class="token punctuation">(</span>x_ptr<span class="token punctuation">,</span>  <span class="token comment"># *Pointer* to first input vector.</span>
               y_ptr<span class="token punctuation">,</span>  <span class="token comment"># *Pointer* to second input vector.</span>
               output_ptr<span class="token punctuation">,</span>  <span class="token comment"># *Pointer* to output vector.</span>
               n_elements<span class="token punctuation">,</span>  <span class="token comment"># Size of the vector.</span>
               BLOCK_SIZE<span class="token punctuation">:</span> tl<span class="token punctuation">.</span>constexpr<span class="token punctuation">,</span>  <span class="token comment"># Number of elements each program should process.</span>
               <span class="token comment"># NOTE: `constexpr` so it can be used as a shape value.</span>
               <span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># There are multiple 'programs' processing different data. We identify which program</span>
    <span class="token comment"># we are here:</span>
    pid <span class="token operator">=</span> tl<span class="token punctuation">.</span>program_id<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># We use a 1D launch grid so axis is 0.</span>
    <span class="token comment"># This program will process inputs that are offset from the initial data.</span>
    <span class="token comment"># For instance, if you had a vector of length 256 and block_size of 64, the programs</span>
    <span class="token comment"># would each access the elements [0:64, 64:128, 128:192, 192:256].</span>
    <span class="token comment"># Note that offsets is a list of pointers:</span>
    block_start <span class="token operator">=</span> pid <span class="token operator">*</span> BLOCK_SIZE
    offsets <span class="token operator">=</span> block_start <span class="token operator">+</span> tl<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> BLOCK_SIZE<span class="token punctuation">)</span>  <span class="token comment"># 当前program要处理的元素的坐标列表</span>
    <span class="token comment"># Create a mask to guard memory operations against out-of-bounds accesses.</span>
    mask <span class="token operator">=</span> offsets <span class="token operator">&lt;</span> n_elements
    <span class="token comment"># Load x and y from DRAM, masking out any extra elements in case the input is not a</span>
    <span class="token comment"># multiple of the block size.</span>
    x <span class="token operator">=</span> tl<span class="token punctuation">.</span>load<span class="token punctuation">(</span>x_ptr <span class="token operator">+</span> offsets<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>
    y <span class="token operator">=</span> tl<span class="token punctuation">.</span>load<span class="token punctuation">(</span>y_ptr <span class="token operator">+</span> offsets<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>
    output <span class="token operator">=</span> x <span class="token operator">+</span> y
    <span class="token comment"># Write x + y back to DRAM.</span>
    tl<span class="token punctuation">.</span>store<span class="token punctuation">(</span>output_ptr <span class="token operator">+</span> offsets<span class="token punctuation">,</span> output<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>
</code></pre><p>通过可视化来直观理解一下数据操作，假设当前输入由10个元素，block size设置为4，则：</p>
<p><img src="../.images/triton-vector-add.png" alt=""></p>
<p>所以，整体看下来kernel的编写是向量化编程，它的复杂度介于Numpy和CUDA之间。</p>
<h3 id="vector-add-host-code">Vector ADD Host Code </h3>
<p>有了kernel之后，需要有一个host函数来调用kernel，也就是下面的代码：</p>
<ol>
<li>创建output tensor</li>
<li>获取block/program个数，也就是一个grid级别的tuple</li>
<li>kernel启动：<code>kernel_func[programs_shape_tuple](*args)</code>，这个和cuda启动kernel非常相似</li>
</ol>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-def">def</span> <span class="token function">add</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> y<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># We need to preallocate the output.</span>
    output <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    <span class="token keyword keyword-assert">assert</span> x<span class="token punctuation">.</span>is_cuda <span class="token keyword keyword-and">and</span> y<span class="token punctuation">.</span>is_cuda <span class="token keyword keyword-and">and</span> output<span class="token punctuation">.</span>is_cuda
    n_elements <span class="token operator">=</span> output<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># The SPMD launch grid denotes the number of kernel instances that run in parallel.</span>
    <span class="token comment"># It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -&gt; Tuple[int].</span>
    <span class="token comment"># In this case, we use a 1D grid where the size is the number of blocks:</span>
    grid <span class="token operator">=</span> <span class="token keyword keyword-lambda">lambda</span> meta<span class="token punctuation">:</span> <span class="token punctuation">(</span>triton<span class="token punctuation">.</span>cdiv<span class="token punctuation">(</span>n_elements<span class="token punctuation">,</span> meta<span class="token punctuation">[</span><span class="token string">"BLOCK_SIZE"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span>  <span class="token comment"># tuple！！</span>
    <span class="token comment"># NOTE:</span>
    <span class="token comment">#  - Each torch.tensor object is implicitly converted into a pointer to its first element.</span>
    <span class="token comment">#  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.</span>
    <span class="token comment">#  - Don't forget to pass meta-parameters as keywords arguments.</span>
    add_kernel<span class="token punctuation">[</span>grid<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> output<span class="token punctuation">,</span> n_elements<span class="token punctuation">,</span> BLOCK_SIZE<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">)</span>
    <span class="token comment"># We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still</span>
    <span class="token comment"># running asynchronously at this point.</span>
    <span class="token keyword keyword-return">return</span> output
</code></pre><p>接下来对add函数的使用就和使用普通函数没有什么区别了</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code>output_triton <span class="token operator">=</span> add<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
</code></pre><h3 id="vector-add-benchmark">Vector ADD Benchmark </h3>
<p>Triton提供了一套内置的Benchmark工具来评估我们自己实现的Kernel和Torch之间的性能差距，例如对比我们实现的Vector ADD，只要使用triton.testing.do_bench就可以获得相关的性能指标。</p>
<p>do_bench的具体使用参考：<a href="https://triton-lang.org/main/python-api/generated/triton.testing.do_bench.html#triton.testing.do_bench">triton.testing.do_bench</a></p>
<p>这和我们写pytest之类的单元测试是没有什么区别的：</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token decorator annotation punctuation">@triton<span class="token punctuation">.</span>testing<span class="token punctuation">.</span>perf_report</span><span class="token punctuation">(</span>
    triton<span class="token punctuation">.</span>testing<span class="token punctuation">.</span>Benchmark<span class="token punctuation">(</span>
        x_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'size'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment"># Argument names to use as an x-axis for the plot.</span>
        x_vals<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token operator">**</span>i <span class="token keyword keyword-for">for</span> i <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment"># Different possible values for `x_name`.</span>
        x_log<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment"># x axis is logarithmic.</span>
        line_arg<span class="token operator">=</span><span class="token string">'provider'</span><span class="token punctuation">,</span>  <span class="token comment"># Argument name whose value corresponds to a different line in the plot.</span>
        line_vals<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'triton'</span><span class="token punctuation">,</span> <span class="token string">'torch'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment"># Possible values for `line_arg`.</span>
        line_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Triton'</span><span class="token punctuation">,</span> <span class="token string">'Torch'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment"># Label name for the lines.</span>
        styles<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'blue'</span><span class="token punctuation">,</span> <span class="token string">'-'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'green'</span><span class="token punctuation">,</span> <span class="token string">'-'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment"># Line styles.</span>
        ylabel<span class="token operator">=</span><span class="token string">'GB/s'</span><span class="token punctuation">,</span>  <span class="token comment"># Label name for the y-axis.</span>
        plot_name<span class="token operator">=</span><span class="token string">'vector-add-performance'</span><span class="token punctuation">,</span>  <span class="token comment"># Name for the plot. Used also as a file name for saving the plot.</span>
        args<span class="token operator">=</span><span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">,</span>  <span class="token comment"># Values for function arguments not in `x_names` and `y_name`.</span>
    <span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword keyword-def">def</span> <span class="token function">benchmark</span><span class="token punctuation">(</span>size<span class="token punctuation">,</span> provider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>size<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
    y <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>size<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
    quantiles <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.8</span><span class="token punctuation">]</span>  <span class="token comment"># 除了中位数外还要返回的数据百分位</span>
    <span class="token keyword keyword-if">if</span> provider <span class="token operator">==</span> <span class="token string">'torch'</span><span class="token punctuation">:</span>
        ms<span class="token punctuation">,</span> min_ms<span class="token punctuation">,</span> max_ms <span class="token operator">=</span> triton<span class="token punctuation">.</span>testing<span class="token punctuation">.</span>do_bench<span class="token punctuation">(</span><span class="token keyword keyword-lambda">lambda</span><span class="token punctuation">:</span> x <span class="token operator">+</span> y<span class="token punctuation">,</span> quantiles<span class="token operator">=</span>quantiles<span class="token punctuation">)</span>
    <span class="token keyword keyword-if">if</span> provider <span class="token operator">==</span> <span class="token string">'triton'</span><span class="token punctuation">:</span>
        ms<span class="token punctuation">,</span> min_ms<span class="token punctuation">,</span> max_ms <span class="token operator">=</span> triton<span class="token punctuation">.</span>testing<span class="token punctuation">.</span>do_bench<span class="token punctuation">(</span><span class="token keyword keyword-lambda">lambda</span><span class="token punctuation">:</span> add<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> quantiles<span class="token operator">=</span>quantiles<span class="token punctuation">)</span>
    gbps <span class="token operator">=</span> <span class="token keyword keyword-lambda">lambda</span> ms<span class="token punctuation">:</span> <span class="token number">3</span> <span class="token operator">*</span> x<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> x<span class="token punctuation">.</span>element_size<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> ms <span class="token operator">*</span> <span class="token number">1e-6</span>  <span class="token comment"># numel()用于获取tensor的元素个数， element_size获取tensor中每个元素的的bytes大小，总的数据吞吐量GB/s计算过程为3个tensor的总bytes大小 / runtime计算的毫秒耗时, * 1e-6后或者GB/s千兆字节每秒的结果</span>
    <span class="token keyword keyword-return">return</span> gbps<span class="token punctuation">(</span>ms<span class="token punctuation">)</span><span class="token punctuation">,</span> gbps<span class="token punctuation">(</span>max_ms<span class="token punctuation">)</span><span class="token punctuation">,</span> gbps<span class="token punctuation">(</span>min_ms<span class="token punctuation">)</span>

benchmark<span class="token punctuation">.</span>run<span class="token punctuation">(</span>print_data<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> show_plots<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> save_path<span class="token operator">=</span><span class="token string">"."</span><span class="token punctuation">)</span>
</code></pre><p>最后绘制出来的size-性能差距如图所示：</p>
<p><img src="../triton-examples/00.vector-add/vector-add-performance.png" alt="vector-add-performance.png"></p>
<p>通过csv可以看到具体的数据，我们写的kernel和torch的kernel性能几乎没有差距：</p>
<pre data-role="codeBlock" data-info="shell" class="language-shell shell"><code>           size      Triton       Torch
<span class="token number">0</span>        <span class="token number">4096.0</span>    <span class="token number">9.600000</span>    <span class="token number">9.600000</span>
<span class="token number">1</span>        <span class="token number">8192.0</span>   <span class="token number">19.200000</span>   <span class="token number">19.200000</span>
<span class="token number">2</span>       <span class="token number">16384.0</span>   <span class="token number">38.400001</span>   <span class="token number">38.400001</span>
<span class="token number">3</span>       <span class="token number">32768.0</span>   <span class="token number">76.800002</span>   <span class="token number">63.999998</span>
<span class="token number">4</span>       <span class="token number">65536.0</span>  <span class="token number">127.999995</span>  <span class="token number">127.999995</span>
<span class="token number">5</span>      <span class="token number">131072.0</span>  <span class="token number">219.428568</span>  <span class="token number">219.428568</span>
<span class="token number">6</span>      <span class="token number">262144.0</span>  <span class="token number">341.333321</span>  <span class="token number">340.741786</span>
<span class="token number">7</span>      <span class="token number">524288.0</span>  <span class="token number">472.615390</span>  <span class="token number">472.615390</span>
<span class="token number">8</span>     <span class="token number">1048576.0</span>  <span class="token number">534.260858</span>  <span class="token number">558.545450</span>
<span class="token number">9</span>     <span class="token number">2097152.0</span>  <span class="token number">599.414644</span>  <span class="token number">599.414644</span>
<span class="token number">10</span>    <span class="token number">4194304.0</span>  <span class="token number">638.337688</span>  <span class="token number">646.736871</span>
<span class="token number">11</span>    <span class="token number">8388608.0</span>  <span class="token number">664.216187</span>  <span class="token number">664.216187</span>
<span class="token number">12</span>   <span class="token number">16777216.0</span>  <span class="token number">675.628857</span>  <span class="token number">677.958629</span>
<span class="token number">13</span>   <span class="token number">33554432.0</span>  <span class="token number">680.304525</span>  <span class="token number">682.666643</span>
<span class="token number">14</span>   <span class="token number">67108864.0</span>  <span class="token number">684.449075</span>  <span class="token number">686.840182</span>
<span class="token number">15</span>  <span class="token number">134217728.0</span>  <span class="token number">686.091204</span>  <span class="token number">687.741160</span>
</code></pre><h3 id="vector-add-triton-vs-ptx">Vector ADD Triton vs PTX </h3>
<p>让我们逆向一下通过Triton生成的PTX，看看它对应的CUDA C代码是什么样子的。</p>
<p>在host代码里添加save ptx的代码</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-def">def</span> <span class="token function">add</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> y<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># We need to preallocate the output.</span>
    output <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    <span class="token keyword keyword-assert">assert</span> x<span class="token punctuation">.</span>is_cuda <span class="token keyword keyword-and">and</span> y<span class="token punctuation">.</span>is_cuda <span class="token keyword keyword-and">and</span> output<span class="token punctuation">.</span>is_cuda
    n_elements <span class="token operator">=</span> output<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># The SPMD launch grid denotes the number of kernel instances that run in parallel.</span>
    <span class="token comment"># It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -&gt; Tuple[int].</span>
    <span class="token comment"># In this case, we use a 1D grid where the size is the number of blocks:</span>
    grid <span class="token operator">=</span> <span class="token keyword keyword-lambda">lambda</span> meta<span class="token punctuation">:</span> <span class="token punctuation">(</span>triton<span class="token punctuation">.</span>cdiv<span class="token punctuation">(</span>n_elements<span class="token punctuation">,</span> meta<span class="token punctuation">[</span><span class="token string">"BLOCK_SIZE"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
    <span class="token comment"># NOTE:</span>
    <span class="token comment">#  - Each torch.tensor object is implicitly converted into a pointer to its first element.</span>
    <span class="token comment">#  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.</span>
    <span class="token comment">#  - Don't forget to pass meta-parameters as keywords arguments.</span>
    add_kernel<span class="token punctuation">[</span>grid<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> output<span class="token punctuation">,</span> n_elements<span class="token punctuation">,</span> BLOCK_SIZE<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">)</span>
    <span class="token comment"># We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still</span>
    <span class="token comment"># running asynchronously at this point.</span>

    <span class="token comment"># save ptx</span>
    <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token builtin">dir</span><span class="token punctuation">(</span>add_kernel<span class="token punctuation">.</span>cache<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword keyword-with">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"add_kernel.ptx"</span><span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> <span class="token keyword keyword-as">as</span> a<span class="token punctuation">:</span>
        <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>add_kernel<span class="token punctuation">.</span>cache<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>asm<span class="token punctuation">[</span><span class="token string">"ptx"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">file</span><span class="token operator">=</span>a<span class="token punctuation">)</span>
    <span class="token keyword keyword-return">return</span> output
</code></pre><p>可以获得一个add_kernel.ptx文件，打开能看到完整的ptx，比较长，只放代码在这里：</p>
<pre data-role="codeBlock" data-info="c" class="language-c c"><code><span class="token comment">//</span>
<span class="token comment">// Generated by LLVM NVPTX Back-End</span>
<span class="token comment">//</span>

<span class="token punctuation">.</span>version <span class="token number">8.2</span>
<span class="token punctuation">.</span>target sm_86
<span class="token punctuation">.</span>address_size <span class="token number">64</span>

	<span class="token comment">// .globl	add_kernel_0d1d2d3de</span>

<span class="token punctuation">.</span>visible <span class="token punctuation">.</span>entry <span class="token function">add_kernel_0d1d2d3de</span><span class="token punctuation">(</span>
	<span class="token punctuation">.</span>param <span class="token punctuation">.</span>u64 add_kernel_0d1d2d3de_param_0<span class="token punctuation">,</span>
	<span class="token punctuation">.</span>param <span class="token punctuation">.</span>u64 add_kernel_0d1d2d3de_param_1<span class="token punctuation">,</span>
	<span class="token punctuation">.</span>param <span class="token punctuation">.</span>u64 add_kernel_0d1d2d3de_param_2<span class="token punctuation">,</span>
	<span class="token punctuation">.</span>param <span class="token punctuation">.</span>u32 add_kernel_0d1d2d3de_param_3
<span class="token punctuation">)</span>
<span class="token punctuation">.</span>maxntid <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span>
<span class="token punctuation">{</span>
	<span class="token punctuation">.</span>reg <span class="token punctuation">.</span>pred 	<span class="token operator">%</span>p<span class="token operator">&lt;</span><span class="token number">7</span><span class="token operator">&gt;</span><span class="token punctuation">;</span>
	<span class="token punctuation">.</span>reg <span class="token punctuation">.</span>b32 	<span class="token operator">%</span>r<span class="token operator">&lt;</span><span class="token number">33</span><span class="token operator">&gt;</span><span class="token punctuation">;</span>
	<span class="token punctuation">.</span>reg <span class="token punctuation">.</span>f32 	<span class="token operator">%</span>f<span class="token operator">&lt;</span><span class="token number">25</span><span class="token operator">&gt;</span><span class="token punctuation">;</span>
	<span class="token punctuation">.</span>reg <span class="token punctuation">.</span>b64 	<span class="token operator">%</span>rd<span class="token operator">&lt;</span><span class="token number">11</span><span class="token operator">&gt;</span><span class="token punctuation">;</span>
	<span class="token punctuation">.</span>loc	<span class="token number">1</span> <span class="token number">17</span> <span class="token number">0</span>
$L__func_begin0<span class="token operator">:</span>
	<span class="token punctuation">.</span>loc	<span class="token number">1</span> <span class="token number">17</span> <span class="token number">0</span>

	ld<span class="token punctuation">.</span>param<span class="token punctuation">.</span>u64 	<span class="token operator">%</span>rd7<span class="token punctuation">,</span> <span class="token punctuation">[</span>add_kernel_0d1d2d3de_param_0<span class="token punctuation">]</span><span class="token punctuation">;</span>
	ld<span class="token punctuation">.</span>param<span class="token punctuation">.</span>u64 	<span class="token operator">%</span>rd8<span class="token punctuation">,</span> <span class="token punctuation">[</span>add_kernel_0d1d2d3de_param_1<span class="token punctuation">]</span><span class="token punctuation">;</span>

<span class="token comment">// ...... 省略</span>

$L__pubTypes_end0<span class="token operator">:</span>
	<span class="token punctuation">}</span>
	<span class="token punctuation">.</span>section	<span class="token punctuation">.</span>debug_loc	<span class="token punctuation">{</span>	<span class="token punctuation">}</span>
</code></pre><p>因为官方没有现成的逆向工具，我们通过chatgpt来逆向一下，可以获得一个看上去正确的cuda代码 😃。通过这个方式，我们能一定程度上分析cuda c和triton之间的对应关系，进一步猜测Triton代码会做些什么</p>
<pre data-role="codeBlock" data-info="c" class="language-c c"><code><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;cuda_runtime.h&gt;</span></span>

__global__ <span class="token keyword keyword-void">void</span> <span class="token function">add_kernel_0d1d2d3de</span><span class="token punctuation">(</span><span class="token keyword keyword-float">float</span> <span class="token operator">*</span>x_ptr<span class="token punctuation">,</span> <span class="token keyword keyword-float">float</span> <span class="token operator">*</span>y_ptr<span class="token punctuation">,</span> <span class="token keyword keyword-float">float</span> <span class="token operator">*</span>output_ptr<span class="token punctuation">,</span> <span class="token keyword keyword-int">int</span> n_elements<span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token keyword keyword-const">const</span> <span class="token keyword keyword-int">int</span> BLOCK_SIZE <span class="token operator">=</span> <span class="token number">1024</span><span class="token punctuation">;</span>
    <span class="token keyword keyword-const">const</span> <span class="token keyword keyword-int">int</span> ELEMENTS_PER_THREAD <span class="token operator">=</span> BLOCK_SIZE <span class="token operator">/</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span> <span class="token comment">// 每个线程处理的元素数</span>

    <span class="token comment">// 计算块的起始位置</span>
    <span class="token keyword keyword-int">int</span> pid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token keyword keyword-int">int</span> block_start <span class="token operator">=</span> pid <span class="token operator">*</span> BLOCK_SIZE<span class="token punctuation">;</span>

    <span class="token comment">// 计算当前线程的起始位置</span>
    <span class="token keyword keyword-int">int</span> thread_start <span class="token operator">=</span> block_start <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> ELEMENTS_PER_THREAD<span class="token punctuation">;</span>

    <span class="token comment">// 每个线程处理ELEMENTS_PER_THREAD个元素</span>
    <span class="token keyword keyword-for">for</span> <span class="token punctuation">(</span><span class="token keyword keyword-int">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> ELEMENTS_PER_THREAD<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token keyword keyword-int">int</span> index <span class="token operator">=</span> thread_start <span class="token operator">+</span> i<span class="token punctuation">;</span>
        <span class="token keyword keyword-if">if</span> <span class="token punctuation">(</span>index <span class="token operator">&lt;</span> n_elements<span class="token punctuation">)</span> <span class="token punctuation">{</span>
            output_ptr<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> x_ptr<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">+</span> y_ptr<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><ul>
<li><code>__global__ void add_kernel_0d1d2d3de(float *input1, float *input2, float *output, int n)</code> 和 <code>def add_kernel(x_ptr, y_ptr,  output_ptr, n_elements, BLOCK_SIZE: tl.constexpr, )</code>: 这里参数上是一一对应的，只是没有了BLOCK_SIZE参数，triton中的Tensor参数实际上就是一个裸指针</li>
<li>tl.program_id(0)映射到blockIdx.x</li>
<li>tl.arange(0, BLOCK_SIZE_N)映射到threadIdx.x</li>
<li>mask 映射到了 if condition</li>
</ul>
<h2 id="heigh-level-system-architecture">Heigh-level system architecture </h2>
<p>Triton的良好性能是由围绕TritonIR、LLVM IR构成的模块化体系结构带来的，它采用的结构是LLVM式的front、middle、backend的结构，其中multi-dimensional blocks of values是一等公民。</p>
<p>从high-level角度来看，Triton C时期的设计如下：</p>
<p><img src="../.images/overview%20of%20triton.png" alt="overview of triton.png"></p>
<p>从Python应用的角度来看，Triton的编译过程属于JIT，kernel的编译发生在python程序运行时，将Python逐渐lowering到PTX。</p>
<p>具体的IR转换过程如下图所示：</p>
<ul>
<li>Triton通过将Python的AST转换为Triton IR</li>
<li>Triton优化过程将Triton IR转换为LLVM IR/MLIR</li>
<li>调用LLVM后端生成PTX</li>
<li>调用ptxas编译为cubin</li>
</ul>
<p><img src="../.images/high-level%20system%20architecture.png" alt="high-level system architecture"></p>
<p>jit装饰器的职责就是遍历python函数的AST生成符合SSA形式的Triton IR的。之后由Triton后端来对TritonIR进行优化和并行化相关的工作。之后转换成LLVM IR，然后借用LLVM后端生成PTX。</p>
<p>一张更加完整的架构图，其中Triton IR、Triton GPU IR是Triton的中间表达，Triton的大量优化都是在它们上实现的，它和LLVM IR类似，都是硬件无关的</p>
<p><img src="../.images/Triton%20compiler%20architecture.png" alt="Triton compiler architecture.png"></p>
<p>更深入的来看从Triton IR到LLVM IR的过程，主要经历了一下几步：</p>
<p><img src="../.images/zoom%20in%20compiler%20pipeline.png" alt="zoom in compiler pipeline.png"></p>
<h2 id="compiler-backend">Compiler backend </h2>
<h3 id="triton-vs-cuda-programming-model">Triton vs CUDA Programming Model </h3>
<p>从Triton官方文档中也可以看的出来Triton的分块设计思路，</p>
<p><img src="../.images/Triton%20vs%20CUDA%20Programming%20Model.png" alt="Triton vs CUDA Programming Model.png"></p>
<h3 id="machine-independent-passes">Machine-independent Passes </h3>
<p>Triton compiler后端会执行一些pass</p>
<p>machine-independent pass主要有两个：</p>
<ol>
<li>pre-fetching</li>
<li>tile-level peephole optimization</li>
</ol>
<h4 id="pre-fetching">Pre-fetching </h4>
<h4 id="tile-level-peephole-optimization">Tile-level Peephole Optimization </h4>
<h3 id="machine-dependent-passes">Machine-dependent Passes </h3>
<p>还有machine-dependent的passes，根据论文中的内容，主要有四个：</p>
<ol>
<li>hierarchical tiling</li>
<li>memory coalescing</li>
<li>shared memory allocation</li>
<li>shared memory synchronization</li>
</ol>
<h4 id="hierarchy-tiling">Hierarchy Tiling </h4>
<p><img src="../.images/Hierarchical%20tiling.png" alt="Hierarchical tiling.png"></p>
<p>按照GPU的架构将数据进行层次化的分块，tile -&gt; micro tile -&gt; nano tile</p>
<p>这里作者在右侧的Machine Model里用了一些比较奇怪的写法，Core、SIMD单元、RegisterFile之类的，和我们通常理解的GPU架构作对应的话：</p>
<ol>
<li>Device：一个GPU</li>
<li>Core：SM</li>
</ol>
<p><img src="../.images/Fermi%20Arch.png" alt="Fermi Arch.png"></p>
<ol start="3">
<li>SIMD：SP</li>
<li>Shared Memory：一个SM上的Shared Memory</li>
<li>RegisterFile：一个SM上的RegisterFile</li>
<li>ALU：CUDA Core（单精度）、DP unit（双精度）</li>
<li>LD/ST：load store unit</li>
</ol>
<p><img src="../.images/Fermi%20SM.png" alt="Fermi SM.png"></p>
<h4 id="memory-coalescing">Memory Coalescing </h4>
<p>Memory Coalescing是通用的优化方法并不局限于Triton，通常理解上是一个wrap内的所有thread的访存操作都是连续的，那么这些访存操作就可以合并，从而降低全局内存的访问开销。</p>
<p>当相邻线程同时访问相邻的内存时，内存访问的操作被称为合并（Coalescing）</p>
<p><img src="../.images/Memory%20coalesced.png" alt="Memory coalesced.png"></p>
<h4 id="shared-memory-allocation">Shared Memory Allocation </h4>
<p>例如Shared Memory Allocation pass用于将计算密集型的block级别OP（如tl.dot）的操作数存储到到shared memory中的时间和位置。</p>
<p>通过分析感兴趣的变量（操作数）的生存周期，利用线性时间存储分配算法来完成共享内存的分配：</p>
<p><img src="../.images/allocate%20memoy%20by%20analyzing%20the%20live%20range.png" alt="alt text"></p>
<h4 id="shared-memory-synchronization">Shared Memory Synchronization </h4>
<p>共享内存的读写操作在Triton的编程模型中都是异步的。所以还涉及了一个Shared Memory Synchronize pass，在生成的GPU code中自动插入barriers来保证正确性。</p>
<p>通过使用forward data-flow分析来检测read-after-writes和write-after-read动作：</p>
<p><img src="../.images/forward%20data-flow%20analysis.png" alt="forward data-flow analysis.png"></p>
<h2 id="how-does-it-fit-in-a-dnn-stack">How does it fit in a DNN stack? </h2>
<p>我们所接触的正常的DNN Stack，通常的Compiler都是作用在图这一层的</p>
<p><img src="../.images/Normal%20DNN%20Stack.png" alt="Normal DNN Stack.png"></p>
<p>Triton被用于实现自定义的op，在对上层引入，在kernel层经过转换后生成ptx代码</p>
<p><img src="../.images/Triton%20as%20part%20of%20DNN%20Stack.png" alt="Triton as part of DNN Stack.png"></p>
<h2 id="how-does-the-compiler-simplify-users-work">How does the compiler simplify user’s work? </h2>
<p>对于简化用户编码来说，Triton确实做到了，我们只需要使用简单的Python代码就能实现一个性能达到某些优化后版本的性能。Triton的一些列预置优化降低了开发kernel的门槛。</p>
<p>但是，与之对应的，对于前端用户，它实际上是<strong>黑盒</strong>的，我们不知道Triton内部对这个kernel到底做了什么，只能知道它的功能是正确的。因此想要进一步去优化kernel需要深入进入Triton内部，已经分析PTX等IR来做，而不是通过我们实现的Python代码。</p>
<p>它还有一个明显的缺点：tile size必须是2的幂，不支持切片（todo：这个要进一步去分析底层）</p>
<p>因此，Triton更合适的是<strong>迅速</strong>构建一个原型，做一些实验我们的<strong>新算子</strong>是否有效。</p>
<h2 id="references">References </h2>
<ul>
<li><a href="https://openai.com/index/triton/">Introducing Triton: Open-source GPU programming for neural networks</a>  必看</li>
<li><a href="http://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf">Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations</a> Triton论文 必看</li>
<li><a href="https://triton-lang.org/main/index.html">Triton’s documentation!</a>  必看</li>
<li><a href="https://www.youtube.com/watch?v=AtbnRIzpwho&amp;t=194s">Lightning Talk: Triton Compiler - Thomas Raoux, OpenAI</a>  低质量</li>
<li><a href="https://isamu-website.medium.com/understanding-the-triton-tutorials-part-1-6191b59ba4c">Understanding the Triton Tutorials Part 1</a>  高质量</li>
<li><a href="https://fkong.tech/posts/2023-04-23-triton-cuda/">Demystify OpenAI Triton</a> 高质量</li>
<li><a href="https://youtu.be/Na9_2G6niMw?si=dAKKhEPkgiMkmvmU">Advance CUDA编程基础 (C++ programming) - Nvidia &amp; 字节内部分享</a> 必看</li>
</ul>

      </div>
      
      
    
    
    
    
    
    
  
    </body></html>
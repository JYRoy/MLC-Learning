# 08.Triton笔记八：基于Triton的项目调研

- [08.Triton笔记八：基于Triton的项目调研](#08triton笔记八基于triton的项目调研)
  - [Overview](#overview)
    - [FlagGems](#flaggems)
      - [源码结构](#源码结构)
      - [绑定torch注册的方式](#绑定torch注册的方式)
      - [kernel实现](#kernel实现)
    - [LightLLM](#lightllm)
      - [源码结构](#源码结构-1)
      - [httpserver](#httpserver)
      - [router](#router)
      - [model](#model)
      - [detokensization](#detokensization)
  - [References](#references)

## Overview

目前社区中有很开源项目是基于Triton来实现的，尤其是大模型时代开始之后，本文是对这些项目的汇总解读

- FlagGems：
  - https://github.com/FlagOpen/FlagGems
  - 智源研究院
  - 基于Triton的大模型算子库
- LightLLM
  - 商汤
  - https://github.com/ModelTC/lightllm/tree/main
  - 基于Python的大模型推理和服务框架，使用Triton实现了在各大模型中公用的算子集合，例如attention、layernorm等

### FlagGems

FlagGems是一个基于Triton实现的高效python算子库，它只提供大模型时代常用的一些训练和推理算子。它可以和torch进行深度的结合，通过

```python
import flag_gems
flag_gems.enable()
```

或者

```python
import torch
import flag_gems

M, N, K = 1024, 1024, 1024
A = torch.randn((M, K), dtype=torch.float16, device="cuda")
B = torch.randn((K, N), dtype=torch.float16, device="cuda")
with flag_gems.use_gems():
    C = torch.mm(A, B)
```

来选择使用flaggems还是torch的实现

#### 源码结构

```shell
|-- __init__.py
|-- fused
|   |-- __init__.py
|   |-- gelu_and_mul.py
|   |-- rotary_embedding.py
|   |-- silu_and_mul.py
|   |-- skip_layernorm.py
|   `-- skip_rms_norm.py
|-- ops
|   |-- __init__.py
|   |-- abs.py
|   |-- add.py
|   |-- addmm.py
|   |-- all.py
|   |-- amax.py
|   |-- any.py
|   |-- argmax.py
|   |-- bitwise_and.py
|   |-- bitwise_not.py
|   |-- bitwise_or.py
|   |-- bmm.py
|   |-- clamp.py
|   |-- cos.py
|   |-- cross_entropy_loss.py
|   |-- cumsum.py
|   |-- div.py
|   |-- dropout.py
|   |-- embedding.py
|   |-- eq.py
|   |-- exp.py
|   |-- flip.py
|   |-- ge.py
|   |-- gelu.py
|   |-- groupnorm.py
|   |-- gt.py
|   |-- isclose.py
|   |-- isfinite.py
|   |-- isinf.py
|   |-- isnan.py
|   |-- layernorm.py
|   |-- le.py
|   |-- log_softmax.py
|   |-- lt.py
|   |-- max.py
|   |-- mean.py
|   |-- min.py
|   |-- mm.py
|   |-- mul.py
|   |-- mv.py
|   |-- ne.py
|   |-- neg.py
|   |-- outer.py
|   |-- pow.py
|   |-- prod.py
|   |-- rand.py
|   |-- rand_like.py
|   |-- randn.py
|   |-- reciprocal.py
|   |-- relu.py
|   |-- rms_norm.py
|   |-- rsqrt.py
|   |-- sigmoid.py
|   |-- silu.py
|   |-- sin.py
|   |-- softmax.py
|   |-- sub.py
|   |-- sum.py
|   |-- tanh.py
|   |-- triu.py
|   |-- var_mean.py
|   |-- vector_norm.py
|   `-- where.py
`-- utils
    |-- __init__.py
    |-- code_cache.py
    |-- code_utils.py
    |-- libentry.py
    |-- pointwise_dynamic.py
    |-- random_utils.py
    |-- shape_utils.py
    `-- type_utils.py
```

#### 绑定torch注册的方式

因为FlagGems定位是一个纯python项目，所以它的算子注册方式是通过torch library来实现的，算子注册的实现在src/flag_gems/__init__.py中

```python
import torch

from .fused import *  # noqa: F403
from .ops import *  # noqa: F403

__version__ = "2.0"

aten_lib = torch.library.Library("aten", "IMPL")


def enable(lib=aten_lib):
    lib.impl("abs", abs, "CUDA")
    lib.impl("add.Tensor", add, "CUDA")
    lib.impl("addmm", addmm, "CUDA")
    lib.impl("bitwise_and.Tensor", bitwise_and_tensor, "CUDA")
    lib.impl("bitwise_and.Scalar", bitwise_and_scalar, "CUDA")
    lib.impl("bitwise_and.Scalar_Tensor", bitwise_and_scalar_tensor, "CUDA")
    lib.impl("bitwise_not", bitwise_not, "CUDA")
    lib.impl("bitwise_or.Tensor", bitwise_or_tensor, "CUDA")
    lib.impl("bitwise_or.Scalar", bitwise_or_scalar, "CUDA")
    lib.impl("bitwise_or.Scalar_Tensor", bitwise_or_scalar_tensor, "CUDA")
    lib.impl("bmm", bmm, "CUDA")
    lib.impl("clamp", clamp, "CUDA")
    lib.impl("clamp.Tensor", clamp_tensor, "CUDA")
    lib.impl("cos", cos, "CUDA")
    lib.impl("cumsum", cumsum, "CUDA")
    lib.impl("div.Tensor", div, "CUDA")
    lib.impl("native_dropout", native_dropout, "AutogradCUDA")
    lib.impl("embedding", embedding, "AutogradCUDA")
    lib.impl("eq.Tensor", eq, "CUDA")
    lib.impl("eq.Scalar", eq_scalar, "CUDA")
    lib.impl("exp", exp, "CUDA")
    lib.impl("ge.Tensor", ge, "CUDA")
    lib.impl("ge.Scalar", ge_scalar, "CUDA")
    lib.impl("gelu", gelu, "CUDA")
    lib.impl("native_group_norm", group_norm, "AutogradCUDA")
    lib.impl("gt.Tensor", gt, "CUDA")
    lib.impl("gt.Scalar", gt_scalar, "CUDA")
    lib.impl("isfinite", isfinite, "CUDA")
    lib.impl("isinf", isinf, "CUDA")
    lib.impl("isnan", isnan, "CUDA")
    lib.impl("native_layer_norm", layer_norm, "AutogradCUDA")
    lib.impl("le.Tensor", le, "CUDA")
    lib.impl("le.Scalar", le_scalar, "CUDA")
    lib.impl("lt.Tensor", lt, "CUDA")
    lib.impl("lt.Scalar", lt_scalar, "CUDA")
    lib.impl("rms_norm", rms_norm, "CUDA")
    lib.impl("rand", rand, "CUDA")
    lib.impl("randn", randn, "CUDA")
    lib.impl("rand_like", rand_like, "CUDA")

    lib.impl("mean", mean, "CUDA")
    lib.impl("mean.dim", mean_dim, "CUDA")
    lib.impl("mm", mm, "CUDA")
    lib.impl("mul.Tensor", mul, "CUDA")
    lib.impl("mv", mv, "CUDA")
    lib.impl("ne.Tensor", ne, "CUDA")
    lib.impl("ne.Scalar", ne_scalar, "CUDA")
    lib.impl("neg", neg, "CUDA")
    lib.impl("pow.Scalar", pow_scalar, "CUDA")
    lib.impl("pow.Tensor_Scalar", pow_tensor_scalar, "CUDA")
    lib.impl("pow.Tensor_Tensor", pow_tensor_tensor, "CUDA")
    lib.impl("reciprocal", reciprocal, "CUDA")
    lib.impl("relu", relu, "AutogradCUDA")
    lib.impl("rsqrt", rsqrt, "CUDA")
    lib.impl("sigmoid", sigmoid, "AutogradCUDA")
    lib.impl("silu", silu, "AutogradCUDA")
    lib.impl("sin", sin, "CUDA")
    lib.impl("softmax.int", softmax, "AutogradCUDA")
    lib.impl("sub.Tensor", sub, "CUDA")
    lib.impl("tanh", tanh, "AutogradCUDA")
    lib.impl("triu", triu, "CUDA")
    lib.impl("var_mean.correction", var_mean, "CUDA")
    lib.impl("linalg_vector_norm", vector_norm, "CUDA")
    lib.impl("where.self", where_self, "CUDA")
    lib.impl("where.ScalarSelf", where_scalar_self, "CUDA")
    lib.impl("where.ScalarOther", where_scalar_other, "CUDA")
    lib.impl("max", max, "CUDA")
    lib.impl("max.dim", max_dim, "CUDA")
    lib.impl("min", min, "CUDA")
    lib.impl("min.dim", min_dim, "CUDA")
    lib.impl("amax", amax, "CUDA")
    lib.impl("argmax", argmax, "CUDA")
    lib.impl("prod", prod, "CUDA")
    lib.impl("prod.dim_int", prod_dim, "CUDA")
    lib.impl("sum", sum, "CUDA")
    lib.impl("sum.dim_IntList", sum_dim, "CUDA")
    lib.impl("all", all, "CUDA")
    lib.impl("all.dim", all_dim, "CUDA")
    lib.impl("all.dims", all_dims, "CUDA")
    lib.impl("any", any, "CUDA")
    lib.impl("any.dim", any_dim, "CUDA")
    lib.impl("any.dims", any_dims, "CUDA")
    lib.impl("log_softmax.int", log_softmax, "AutogradCUDA")
    lib.impl("outer", outer, "AutogradCUDA")
    lib.impl("cross_entropy_loss", cross_entropy_loss, "AutogradCUDA")
    lib.impl("isclose", isclose, "CUDA")
    lib.impl("allclose", allclose, "CUDA")
    lib.impl("flip", flip, "CUDA")


class use_gems:
    def __init__(self):
        self.lib = torch.library.Library("aten", "IMPL")

    def __enter__(self):
        enable(self.lib)

    def __exit__(self, exc_type, exc_val, exc_tb):
        del self.lib


__all__ = [
    "enable",
    "use_gems",
]
```

#### kernel实现

以addmm为例子，实现方面和之前看过的mm没有什么区别，不再具体分析

```python
import logging

import torch
import triton
import triton.language as tl

from ..utils import libentry


@libentry()
@triton.autotune(
    configs=[
        triton.Config(
            {"BLOCK_SIZE_M": 128, "BLOCK_SIZE_N": 256, "BLOCK_SIZE_K": 64},
            num_stages=3,
            num_warps=8,
        ),
        triton.Config(
            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 256, "BLOCK_SIZE_K": 32},
            num_stages=4,
            num_warps=4,
        ),
        triton.Config(
            {"BLOCK_SIZE_M": 128, "BLOCK_SIZE_N": 128, "BLOCK_SIZE_K": 32},
            num_stages=4,
            num_warps=4,
        ),
        triton.Config(
            {"BLOCK_SIZE_M": 128, "BLOCK_SIZE_N": 64, "BLOCK_SIZE_K": 32},
            num_stages=4,
            num_warps=4,
        ),
        triton.Config(
            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 128, "BLOCK_SIZE_K": 32},
            num_stages=4,
            num_warps=4,
        ),
        triton.Config(
            {"BLOCK_SIZE_M": 128, "BLOCK_SIZE_N": 32, "BLOCK_SIZE_K": 32},
            num_stages=4,
            num_warps=4,
        ),
        triton.Config(
            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 32, "BLOCK_SIZE_K": 32},
            num_stages=5,
            num_warps=2,
        ),
        triton.Config(
            {"BLOCK_SIZE_M": 32, "BLOCK_SIZE_N": 64, "BLOCK_SIZE_K": 32},
            num_stages=5,
            num_warps=2,
        ),
    ],
    key=["M", "N", "K"],
)
@triton.jit(do_not_specialize=["alpha", "beta"])
def addmm_kernel(
    a_ptr,
    b_ptr,
    bias_ptr,
    c_ptr,
    alpha,
    beta,
    M,
    N,
    K,
    stride_am,
    stride_ak,
    stride_bk,
    stride_bn,
    stride_cm,
    stride_cn,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)
    bias_ptrs = bias_ptr + offs_bn

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs,
            mask=(offs_am[:, None] < M) & (offs_k[None, :] < K - k * BLOCK_SIZE_K),
            other=0.0,
        )
        b = tl.load(
            b_ptrs,
            mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K) & (offs_bn[None, :] < N),
            other=0.0,
        )
        accumulator += tl.dot(a, b, allow_tf32=False)
        a_ptrs += BLOCK_SIZE_K * stride_ak
        b_ptrs += BLOCK_SIZE_K * stride_bk
    bias = tl.load(bias_ptrs, mask=offs_bn < N, other=0.0)
    accumulator = accumulator * alpha + bias * beta
    c = accumulator.to(bias.dtype)

    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    tl.store(c_ptrs, c, mask=c_mask)


def addmm(bias, mat1, mat2, *, beta=1, alpha=1):
    logging.debug("GEMS ADDMM")
    assert mat1.shape[1] == mat2.shape[0], "Incompatible dimensions"
    M, K = mat1.shape
    _, N = mat2.shape

    mat1 = mat1.contiguous()
    mat2 = mat2.contiguous()
    out = torch.empty((M, N), device=mat1.device, dtype=mat1.dtype)

    grid = lambda META: (
        triton.cdiv(M, META["BLOCK_SIZE_M"]),
        triton.cdiv(N, META["BLOCK_SIZE_N"]),
    )
    with torch.cuda.device(mat1.device):
        addmm_kernel[grid](
            mat1,
            mat2,
            bias,
            out,
            alpha,
            beta,
            M,
            N,
            K,
            mat1.stride(0),
            mat1.stride(1),
            mat2.stride(0),
            mat2.stride(1),
            out.stride(0),
            out.stride(1),
        )
    return out
```

以fused中的gelu_and_mul为例子，因为torch中没有这样的算子，所以要通过torch.autograd.Function来实现的

```python
import logging

import torch
import triton
import triton.language as tl

from ..utils import pointwise_dynamic

try:
    from triton.language.extra.cuda.libdevice import erf, pow, tanh
except ImportError:
    try:
        from triton.language.math import erf, pow, tanh
    except ImportError:
        from triton.language.libdevice import erf, pow, tanh


@pointwise_dynamic(promotion_methods=[(0, 1, "DEFAULT")])
@triton.jit
def gelu_none_and_mul_kernel(x, y):
    x_fp32 = x.to(tl.float32)
    x_gelu = 0.5 * x_fp32 * (1 + erf(x_fp32 * 0.7071067811))
    return x_gelu * y


@pointwise_dynamic(promotion_methods=[(0, 1, "DEFAULT")])
@triton.jit
def gelu_tanh_and_mul_kernel(x, y):
    x_fp32 = x.to(tl.float32)
    x_gelu = (
        0.5
        * x_fp32
        * (
            1
            + tanh(x_fp32 * 0.79788456 * (1 + 0.044715 * pow(x_fp32.to(tl.float32), 2)))
        )
    )
    return x_gelu * y


class GeluAndMul(torch.autograd.Function):
    @staticmethod
    def forward(ctx, A, B, approximate="none"):
        logging.debug("GEMS GELU AND MUL FORWARD")
        if approximate == "none":
            return gelu_none_and_mul_kernel(A, B)
        elif approximate == "tanh":
            return gelu_tanh_and_mul_kernel(A, B)
        else:
            raise ValueError(f"Invalid approximate value: {approximate}")


def gelu_and_mul(A, B, approximate="none"):
    return GeluAndMul.apply(A, B, approximate)
```

通过给的测试用例了解这种情况是怎么使用的，对于普通的mul op，还是调用torch的，但是对于torch中没有的，要主动使用flag_gems的实现，因此对于想要融合算子的场景，FlagGems需要手动融合。

```python
@pytest.mark.parametrize("shape", POINTWISE_SHAPES)
@pytest.mark.parametrize("dtype", FLOAT_DTYPES)
@pytest.mark.parametrize("approximate", ["none", "tanh"])
def test_accuracy_gelu_and_mul(shape, approximate, dtype):
    inp1 = torch.randn(shape, dtype=dtype, device="cuda")
    inp2 = torch.randn(shape, dtype=dtype, device="cuda")
    ref_inp1 = to_reference(inp1, True)
    ref_inp2 = to_reference(inp2, True)

    ref_out = torch.mul(
        torch.nn.functional.gelu(ref_inp1, approximate=approximate), ref_inp2
    )
    with flag_gems.use_gems():
        res_out = flag_gems.gelu_and_mul(inp1, inp2, approximate)

    gems_assert_close(res_out, ref_out, dtype)
```

### LightLLM

LightLLM是一个纯python项目，它集成了包括LLaMA、Baichuan、Yi等在内的国内外的开源语言模型，并构筑了一套标准化的python推理框架且实现了一套通用的优化方法。

```
BLOOM
LLaMA
LLaMA V2
StarCoder
Qwen-7b
ChatGLM2-6b
Baichuan-7b
Baichuan2-7b
Baichuan2-13b
Baichuan-13b
InternLM-7b
Yi-34b
Qwen-VL
Qwen-VL-Chat
Llava-7b
Llava-13b
Mixtral
Stablelm
MiniCPM
Phi-3
CohereForAI
```

LightLLM项目舍弃了以前常用的ONNX和TensorRT，使用PyTorch来进行显存管理和算子管理，使用Triton来进行kernel实现，使用huggingface来进行模型仓库资源的加载。

只要针对大模型常见的一些问题进行了优化：

- 显存碎片化，导致无法达到较高的吞吐量
- 请求调度效率低，请求的结束不可预知，长度动态变化，容易造成GPU利用率低
- kernel定制化难度高，传统的都是CUDA实现，想实现一个高效的定制化kernel难度高

根据官方文档介绍，LightLLM的核心feature为：

- 三进程架构：用于异步化处理tokensize和detokenize操作，避免耗时的cpu处理阻碍模型推理时的gpu调度执行和降低gpu利用率
- Token Attention：一种以token为粒度进行kv cache显存管理的特性
- Efficient Router：配合Token Attention用于精确的管理调度请求的合并推理

![LightLLM Arch.png](../.images/LightLLM%20Arch.png)

LightLLM的组件设计比较简单，整体分为四个部分：

1. httpserver：由fastapi控制的接收用户推理请求的主进程，同时负责tokensize
2. Router：组织调度模块，调用model和detoken来处理推理和输出，是一个单独的进程
3. detokensization：解码模块，独立进程
4. model：模型推理实现，通过rpc隔离开的单独进程

所谓的三进程是httpserver、router和detokensization三个进程，进程间通过pipe管道进行进程初始化状态的通信，通过zmq来进行主要通信。

#### 源码结构

```shell
|-- __init__.py
|-- common
|   |-- __init__.py
|   |-- basemodel
|   |-- build_utils.py
|   |-- infer_utils.py
|   |-- int8kv_mem_manager.py
|   |-- mem_manager.py
|   |-- mem_utils.py
|   |-- ppl_int4kv_mem_manager.py
|   |-- ppl_int8kv_mem_manager.py
|   `-- req_manager.py
|-- models
|   |-- __init__.py
|   |-- baichuan13b
|   |-- baichuan2_13b
|   |-- baichuan2_7b
|   |-- baichuan7b
|   |-- bloom
|   |-- chatglm2
|   |-- cohere
|   |-- gemma_2b
|   |-- internlm
|   |-- internlm2
|   |-- internlm2_wquant
|   |-- internlm_wquant
|   |-- internlm_xcomposer
|   |-- llama
|   |-- llama_awquant
|   |-- llama_quik
|   |-- llama_wquant
|   |-- llava
|   |-- minicpm
|   |-- mistral
|   |-- mixtral
|   |-- phi3
|   |-- qwen
|   |-- qwen2
|   |-- qwen2_wquant
|   |-- qwen_vl
|   |-- qwen_wquant
|   |-- stablelm
|   |-- starcoder
|   |-- starcoder2
|   |-- starcoder_wquant
|   `-- yi
|-- server
|   |-- __init__.py
|   |-- api_lightllm.py
|   |-- api_models.py
|   |-- api_server.py
|   |-- api_tgi.py
|   |-- build_prompt.py
|   |-- detokenization
|   |-- embed_cache
|   |-- health_monitor
|   |-- httpserver
|   |-- io_struct.py
|   |-- metrics
|   |-- multimodal_params.py
|   |-- req_id_generator.py
|   |-- router
|   |-- sampling_params.py
|   |-- tokenizer.py
|   `-- visualserver
`-- utils
    |-- __init__.py
    |-- graceful_utils.py
    |-- health_check.py
    |-- infer_utils.py
    |-- log_utils.py
    |-- net_utils.py
    |-- petrel_helper.py
    `-- start_utils.py
```

common中是模型的layer和kernel的公共实现

```shell
|-- __init__.py
|-- basemodel
|   |-- __init__.py
|   |-- basemodel.py
|   |-- cuda_kernel
|   |   |-- __init__.py
|   |   |-- fast_llm_wquant.py
|   |   |-- lmdeploy_wquant.py
|   |   |-- ppl_awquant.py
|   |   `-- ppl_wquant.py
|   |-- infer_struct.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   |-- base_layer_infer.py
|   |   |-- post_layer_infer.py
|   |   |-- pre_layer_infer.py
|   |   |-- template
|   |   |   |-- __init__.py
|   |   |   |-- post_layer_infer_template.py
|   |   |   |-- pre_layer_infer_template.py
|   |   |   |-- transformer_layer_infer_cohere_template.py
|   |   |   |-- transformer_layer_infer_template.py
|   |   |   |-- transformer_layer_infer_template_awquant.py
|   |   |   `-- transformer_layer_infer_template_wquant.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   |-- base_layer_weight.py
|   |   |-- hf_load_utils.py
|   |   |-- pre_and_post_layer_weight.py
|   |   `-- transformer_layer_weight.py
|   |-- splitfuse_infer_struct.py
|   `-- triton_kernel
|       |-- __init__.py
|       |-- apply_penalty.py
|       |-- copy_kv_index_to_req.py
|       |-- dequantize_gemm_int4.py
|       |-- dequantize_gemm_int8.py
|       |-- destindex_copy_kv.py
|       |-- multimodal_emb.py
|       |-- quantize_gemm_int8.py
|       `-- splitfuse_copy_kv_index_to_req.py
|-- build_utils.py
|-- infer_utils.py
|-- int8kv_mem_manager.py
|-- mem_manager.py
|-- mem_utils.py
|-- ppl_int4kv_mem_manager.py
|-- ppl_int8kv_mem_manager.py
`-- req_manager.py
```

models中是具体的每一个模型的实现

- layer_infer：模型前向的实现，一般包含三个py实现对应了预测的三个计算阶段
  - pre_layer_infer.py：主要包含两个计算（1）embedding（2）layer norm
  - transformer_layer_infer.py：transformer layer的计算实现，包括attention、ffn、kv cache
  - post_layer_infer.py：输出层的预测实现
- layer_weights：加载基于huggingface上的模型权重的具体实现

```shell
|-- __init__.py
|-- baichuan13b
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- baichuan2_13b
|   |-- __init__.py
|   `-- model.py
|-- baichuan2_7b
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- pre_and_post_layer_weight.py
|   `-- model.py
|-- baichuan7b
|   |-- __init__.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- bloom
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   |-- post_layer_infer.py
|   |   |-- pre_layer_infer.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   |-- hf_load_utils.py
|   |   |-- pre_and_post_layer_weight.py
|   |   `-- transformer_layer_weight.py
|   |-- model.py
|   `-- triton_kernel
|       |-- __init__.py
|       |-- context_flashattention_nopad.py
|       |-- layernorm.py
|       |-- token_attention_nopad_att1.py
|       |-- token_attention_nopad_reduceV.py
|       |-- token_attention_nopad_softmax.py
|       `-- token_flashattention_nopad.py
|-- chatglm2
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   |-- pre_and_post_layer_weight.py
|   |   `-- transformer_layer_weight.py
|   |-- model.py
|   `-- triton_kernel
|       |-- __init__.py
|       `-- rotary_emb.py
|-- cohere
|   |-- __init__.py
|   |-- infer_struct.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   |-- post_layer_infer.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   |-- pre_and_post_layer_weight.py
|   |   `-- transformer_layer_weight.py
|   |-- model.py
|   |-- splitfuse_infer_struct.py
|   `-- triton_kernels
|       |-- __init__.py
|       |-- layernorm.py
|       `-- rotary_emb.py
|-- gemma_2b
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   |-- pre_layer_infer.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   |-- pre_and_post_layer_weight.py
|   |   `-- transformer_layer_weight.py
|   |-- model.py
|   `-- triton_kernel
|       |-- __init__.py
|       `-- gelu_and_mul.py
|-- internlm
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- internlm2
|   |-- __init__.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   |-- pre_and_post_layer_weight.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- internlm2_wquant
|   |-- __init__.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- internlm_wquant
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- internlm_xcomposer
|   |-- __init__.py
|   |-- infer_struct.py
|   |-- internlm_visual.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- llama
|   |-- __init__.py
|   |-- infer_struct.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   |-- post_layer_infer.py
|   |   |-- pre_layer_infer.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   |-- ds_load_utils.py
|   |   |-- pre_and_post_layer_weight.py
|   |   `-- transformer_layer_weight.py
|   |-- model.py
|   |-- splitfuse_infer_struct.py
|   |-- triton_kernel
|   |   |-- __init__.py
|   |   |-- context_flashattention_nopad.py
|   |   |-- flash_decoding.py
|   |   |-- flash_decoding_stage1.py
|   |   |-- flash_decoding_stage2.py
|   |   |-- gqa_decode_flashattention_nopad.py
|   |   |-- gqa_flash_decoding.py
|   |   |-- gqa_flash_decoding_stage1.py
|   |   |-- gqa_flash_decoding_stage2.py
|   |   |-- ppl_fp16_flash_decoding.py
|   |   |-- ppl_int4kv_copy_kv.py
|   |   |-- ppl_int4kv_flash_decoding.py
|   |   |-- ppl_int8kv_flash_decoding.py
|   |   |-- ppl_quant_copy_kv.py
|   |   |-- rmsnorm.py
|   |   |-- rotary_emb.py
|   |   |-- silu_and_mul.py
|   |   |-- splitfuse_context_flashattention_nopad.py
|   |   |-- token_attention_nopad_att1.py
|   |   |-- token_attention_nopad_reduceV.py
|   |   |-- token_attention_nopad_softmax.py
|   |   `-- token_attention_softmax_and_reducev.py
|   `-- yarn_rotary_utils.py
|-- llama_awquant
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- llama_quik
|   |-- __init__.py
|   |-- cuda_kernel
|   |   |-- __init__.py
|   |   `-- quik_awquant.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   |-- qlinear.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- llama_wquant
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- llava
|   |-- __init__.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- pre_and_post_layer_weight.py
|   |-- llava_visual.py
|   `-- model.py
|-- minicpm
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   |-- pre_and_post_layer_weight.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- mistral
|   |-- __init__.py
|   |-- infer_struct.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- model.py
|   `-- triton_kernel
|       |-- __init__.py
|       |-- context_flashattention_nopad.py
|       |-- init_att_sliding_window_info.py
|       |-- token_attention_nopad_att1.py
|       |-- token_attention_nopad_reduceV.py
|       `-- token_attention_softmax_and_reducev.py
|-- mixtral
|   |-- __init__.py
|   |-- infer_struct.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- phi3
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- transformer_layer_weight.py
|   |-- model.py
|   `-- triton_kernel
|       |-- __init__.py
|       |-- context_flashattention_nopad.py
|       |-- destindex_copy_kv.py
|       |-- flash_decoding.py
|       |-- flash_decoding_stage1.py
|       |-- flash_decoding_stage2.py
|       `-- rotary_emb.py
|-- qwen
|   |-- __init__.py
|   |-- infer_struct.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   |-- pre_and_post_layer_weight.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- qwen2
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   |-- pre_and_post_layer_weight.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- qwen2_wquant
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- qwen_vl
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- pre_layer_infer.py
|   |-- model.py
|   `-- qwen_visual.py
|-- qwen_wquant
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- stablelm
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   |-- pre_and_post_layer_weight.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- starcoder
|   |-- __init__.py
|   |-- infer_struct.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   |-- pre_layer_infer.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   |-- pre_and_post_layer_weight.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- starcoder2
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   |-- pre_and_post_layer_weight.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
|-- starcoder_wquant
|   |-- __init__.py
|   |-- layer_infer
|   |   |-- __init__.py
|   |   `-- transformer_layer_infer.py
|   |-- layer_weights
|   |   |-- __init__.py
|   |   `-- transformer_layer_weight.py
|   `-- model.py
`-- yi
    |-- __init__.py
    |-- layer_weights
    |   |-- __init__.py
    |   `-- transformer_layer_weight.py
    `-- model.py
```

server中是LightLLM另一个重要部分的实现：httpserver和router

router的主要作用是调度模块，接收请求、组装batch后调用模型，将模型的输出结果给到detokenizer。

```shell
|-- __init__.py
|-- api_lightllm.py
|-- api_models.py
|-- api_server.py
|-- api_tgi.py
|-- build_prompt.py
|-- detokenization
|   |-- __init__.py
|   |-- decode.py
|   `-- manager.py
|-- embed_cache
|   |-- __init__.py
|   |-- impl
|   |   |-- __init__.py
|   |   `-- naive_memory_cache.py
|   |-- interface.py
|   |-- manager.py
|   `-- utils.py
|-- health_monitor
|   |-- __init__.py
|   `-- manager.py
|-- httpserver
|   |-- __init__.py
|   `-- manager.py
|-- io_struct.py
|-- metrics
|   |-- __init__.py
|   |-- manager.py
|   `-- metrics.py
|-- multimodal_params.py
|-- req_id_generator.py
|-- router
|   |-- __init__.py
|   |-- dynamic_prompt
|   |   |-- __init__.py
|   |   |-- radix_cache.py
|   |   `-- shared_arr.py
|   |-- manager.py
|   |-- model_infer
|   |   |-- __init__.py
|   |   |-- infer_batch.py
|   |   |-- mode_backend
|   |   |   |-- __init__.py
|   |   |   |-- base_backend.py
|   |   |   |-- beamsearch
|   |   |   |   |-- __init__.py
|   |   |   |   |-- impl.py
|   |   |   |   |-- post_process.py
|   |   |   |   `-- pre_process.py
|   |   |   |-- continues_batch
|   |   |   |   |-- __init__.py
|   |   |   |   |-- impl.py
|   |   |   |   |-- impl_for_return_all_prompt_logprobs.py
|   |   |   |   |-- post_process.py
|   |   |   |   `-- pre_process.py
|   |   |   |-- diverse_backend
|   |   |   |   |-- __init__.py
|   |   |   |   |-- impl.py
|   |   |   |   `-- post_process.py
|   |   |   `-- splitfuse
|   |   |       |-- __init__.py
|   |   |       |-- impl.py
|   |   |       `-- pre_process.py
|   |   `-- model_rpc.py
|   |-- pause_strategy.py
|   |-- req_queue
|   |   |-- __init__.py
|   |   |-- base_queue.py
|   |   |-- continues_batch
|   |   |   |-- __init__.py
|   |   |   |-- beam_impl.py
|   |   |   `-- impl.py
|   |   `-- splitfuse
|   |       |-- __init__.py
|   |       `-- impl.py
|   |-- stats.py
|   `-- token_load.py
|-- sampling_params.py
|-- tokenizer.py
`-- visualserver
    |-- __init__.py
    |-- manager.py
    `-- model_infer
        |-- __init__.py
        `-- model_rpc.py
```

#### httpserver

#### router

#### model

#### detokensization

## References

- [lightllm代码解读——之模型推理](https://zhuanlan.zhihu.com/p/666731524)



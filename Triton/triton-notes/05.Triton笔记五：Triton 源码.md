# Triton 源码

- [Triton 源码](#triton-源码)
  - [Overview](#overview)
  - [Triton 源码结构](#triton-源码结构)
  - [Triton 前端](#triton-前端)
    - [JIT decortor](#jit-decortor)
    - [生成AST](#生成ast)
    - [生成Triton IR](#生成triton-ir)
  - [Triton 中端](#triton-中端)
  - [Triton 后端](#triton-后端)
  - [References](#references)

## Overview

Triton 3.0 代码为例：release/3.0.x:72b422d4906de2ae0209543faa6b8cc17ea4b11f

Triton整体依然可以认为是三段式的：前端 + 中端 + 后端

![Triton compiler arch.png](../.images/Triton%20compiler%20arch.png)

- 前端：Python源码生成AST，再由AST生成Triton IR
- 中端：执行各种优化pass，Triton IR生成TritonGPU IR
- 后端：TritonGPU IR生成LLVM IR，LLVM IR由LLVM后端生成PTX，使用ptxas最终编译称为cubin

目前Triton是基于MLIR开发的，所以从这个视角看，Triton中有两个Dialect：

- Triton Dialect：硬件无关
- TritonGPU Dialect：硬件相关

## Triton 源码结构

先熟悉下triton中所有的module

```python
# triton/__init__.py
"""isort:skip_file"""
__version__ = '3.0.0'

# ---------------------------------------
# Note: import order is significant here.

# submodules
from .runtime import (
    autotune,
    Config,
    heuristics,
    JITFunction,
    KernelInterface,
    reinterpret,
    TensorWrapper,
    OutOfResources,
    InterpreterError,
    MockTensor,
)
from .runtime.jit import jit
from .compiler import compile, CompilationError
from .errors import TritonError

from . import language
from . import testing
from . import tools

__all__ = [
    "autotune",
    "cdiv",
    "CompilationError",
    "compile",
    "Config",
    "heuristics",
    "impl",
    "InterpreterError",
    "jit",
    "JITFunction",
    "KernelInterface",
    "language",
    "MockTensor",
    "next_power_of_2",
    "ops",
    "OutOfResources",
    "reinterpret",
    "runtime",
    "TensorWrapper",
    "TritonError",
    "testing",
    "tools",
]

# -------------------------------------
# misc. utilities that  don't fit well
# into any specific module
# -------------------------------------


def cdiv(x: int, y: int):
    return (x + y - 1) // y


def next_power_of_2(n: int):
    """Return the smallest power of 2 greater than or equal to n"""
    n -= 1
    n |= n >> 1
    n |= n >> 2
    n |= n >> 4
    n |= n >> 8
    n |= n >> 16
    n |= n >> 32
    n += 1
    return n

```

其中比较重要的是：

- triton.language：triton kernel相关的，包括算子以及一些类型定义；
- triton.jit：装饰器，用于触发triton compiler；
- triton.autotune：装饰器，用于出发auto tune自动调参工具；
- triton.heuristics：指定某个超参数的计算过程；
- triton.Config：超参数的搜索空间；
- triton.testing：benchmark test相关的接口；

`language/__init__.py`中能够看到提供的所有操作。

```python
# triton/language/__init__.py
__all__ = [
    "PropagateNan",
    "TRITON_MAX_TENSOR_NUMEL",
    "_experimental_descriptor_load",
    "_experimental_descriptor_store",
    "abs",
    "advance",
    "arange",
    "argmax",
    "argmin",
    "associative_scan",
    "atomic_add",
    "atomic_and",
    "atomic_cas",
    "atomic_max",
    "atomic_min",
    "atomic_or",
    "atomic_xchg",
    "atomic_xor",
    "bfloat16",
    "block_type",
    "broadcast",
    "broadcast_to",
    "builtin",
    "cat",
    "cast",
    "cdiv",
    "ceil",
    "clamp",
    "const",
    "const_pointer_type",
    "constexpr",
    "cos",
    "cumprod",
    "cumsum",
    "debug_barrier",
    "device_assert",
    "device_print",
    "div_rn",
    "dot",
    "dtype",
    "erf",
    "exp",
    "exp2",
    "expand_dims",
    "extra",
    "fdiv",
    "flip",
    "float16",
    "float32",
    "float64",
    "float8e4b15",
    "float8e4nv",
    "float8e4b8",
    "float8e5",
    "float8e5b16",
    "floor",
    "fma",
    "full",
    "function_type",
    "histogram",
    "inline_asm_elementwise",
    "interleave",
    "int1",
    "int16",
    "int32",
    "int64",
    "int8",
    "ir",
    "join",
    "load",
    "log",
    "log2",
    "make_block_ptr",
    "math",
    "max",
    "max_constancy",
    "max_contiguous",
    "maximum",
    "min",
    "minimum",
    "multiple_of",
    "num_programs",
    "pair_uniform_to_normal",
    "permute",
    "philox",
    "philox_impl",
    "pi32_t",
    "pointer_type",
    "program_id",
    "rand",
    "rand4x",
    "randint",
    "randint4x",
    "randn",
    "randn4x",
    "range",
    "ravel",
    "reduce",
    "reshape",
    "rsqrt",
    "sigmoid",
    "sin",
    "softmax",
    "sort",
    "split",
    "sqrt",
    "sqrt_rn",
    "static_assert",
    "static_print",
    "static_range",
    "store",
    "sum",
    "swizzle2d",
    "tensor",
    "trans",
    "triton",
    "uint16",
    "uint32",
    "uint64",
    "uint8",
    "uint_to_uniform_float",
    "umulhi",
    "view",
    "void",
    "where",
    "xor_sum",
    "zeros",
    "zeros_like",
]
```

Triton的核心代码都实现在python和include中

![Triton Top Level Source Code.png](../.images/Triton%20Top%20Level%20Source%20Code.png)

python目录内分为两个部分，一部分是triton，一部分是src

- triton中是上面看到的所有module的python代码
- src部分是外层include的源代码，主要就是编译器部分了

![Triton python source code.png](../.images/Triton%20python%20source%20code.png)

其实核心代码就在include、python/triton、python/src中

## Triton 前端

Triton前端代码将用户用Python编写的kernel转换为对应的Triton IR (Triton Dialect)。

### JIT decortor

这一步的入口函数，就是jit decorator。

```python
# tirton/runtime/jit.py
def jit(
    fn: Optional[T] = None,
    *,
    version=None,
    repr: Optional[Callable] = None,
    launch_metadata: Optional[Callable] = None,
    do_not_specialize: Optional[Iterable[int]] = None,
    debug: Optional[bool] = None,
    noinline: Optional[bool] = None,
) -> Union[JITFunction[T], Callable[[T], JITFunction[T]]]:
    """
    Decorator for JIT-compiling a function using the Triton compiler.

    :note: When a jit'd function is called, arguments are
        implicitly converted to pointers if they have a :code:`.data_ptr()` method
        and a `.dtype` attribute.

    :note: This function will be compiled and run on the GPU. It will only have access to:

           * python primitives,
           * builtins within the triton package,
           * arguments to this function,
           * other jit'd functions

    :param fn: the function to be jit-compiled
    :type fn: Callable
    """

    def decorator(fn: T) -> JITFunction[T]:
        assert callable(fn)
        if os.getenv("TRITON_INTERPRET", "0") == "1":
            from .interpreter import InterpretedFunction
            return InterpretedFunction(fn)
        else:
            return JITFunction(
                fn,
                version=version,
                do_not_specialize=do_not_specialize,
                debug=debug,
                noinline=noinline,
                repr=repr,
                launch_metadata=launch_metadata,
            )

    if fn is not None:
        return decorator(fn)

    else:
        return decorator
```

也就是说，我们在host代码中launch的kernel函数，实际上就是调用了JITFunction类的方法。

以vectorAdd为例子，triton中使用了和cuda类似的launch kernel方式

```python
add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
```

即，`add_kernel[grid]`调用了JITFunction的`__getitem__`方法。

因为JITFunction继承自KernelInterface类，并且JITFunction类内部没有重写`__getitem__`方法

```python
class JITFunction(KernelInterface[T]):
    ...
```

所以，`add_kernel[grid]`实际上调用了KernelInterface类的`__getitem__`方法

```python
class KernelInterface(Generic[T]):
    run: T

    def __getitem__(self, grid) -> T:
        """
        A JIT function is launched with: fn[grid](*args, **kwargs).
        Hence JITFunction.__getitem__ returns a callable proxy that
        memorizes the grid.
        """
        return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
        # return cast(T, functools.partial(cast(Callable, self.run), grid=grid))
```

返回了一个lambda函数，所以`(x, y, output, n_elements, BLOCK_SIZE=1024)`是lambda函数的调用

这里的使用方式，是子类使用父类方法，所以self是子类对象，即是子类JITFunction的run方法的调用，参数是`(x, y, output, n_elements, BLOCK_SIZE=1024)`

run方法里面最核心的三步操作：

- 获取AST：`src = self.ASTSource(self, signature, constants, configs[0])`
- 编译：`kernel = self.compile(src, target=target,options=options.__dict__,)`
- 执行：`kernel.run(grid_0, grid_1, grid_2, stream, ...)`

下面逐行分析下run方法

```python
class JITFunction(KernelInterface[T]):

    def run(self, *args, grid, warmup, **kwargs):
        # parse options
        # 获取当前设备
        device = driver.active.get_current_device()
        # 获取当前设备的stream
        stream = driver.active.get_current_stream(device)
        kwargs["debug"] = self.debug

        # Execute pre run hooks with args and kwargs
        # 执行hook，用户可以通过add_kernel.add_pre_run_hook(my_hook)方式添加pre_run_hook
        for hook in self.pre_run_hooks:
            hook(*args, **kwargs)
        
        # todo：不清楚binder的作用
        if self.binder is None:
            self.create_binder()

        bound_args, sig_and_spec, constexpr_vals, non_constexpr_vals, excess_kwargs = self.binder(*args, **kwargs)

        # compute cache key
        # 类似："*fp16*fp16*fp16*fp16i32i32DDDDDD((32, 128), {'num_ctas': 1, 'debug': None})"的string作为key
        key = ''.join(sig_and_spec) + str((constexpr_vals, excess_kwargs))
        # self.cache用于存储编译后的kernel，如果是第一次执行，cache就是一个空的defaultdict
        kernel = self.cache[device].get(key, None)

        # 第一次执行，没有kernel cache，因此要进入编译流程
        if kernel is None:
            # Kernel is not cached; we have to compile.
            # 和llvm中的target类似，就是平台相关的信息，Triton中是GPUTarget类对应了hip和cuda，同时还存储了capability和warp_size
            # GPUTarget(backend='cuda', arch=80, warp_size=32)
            target = driver.active.get_current_target()
            # <nvi.CUDABackend object at 0x7fc52dcc66e0>
            backend = self.make_backend(target)
            # {'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128, 'num_ctas': 1, 'debug': None}
            options = backend.parse_options(kwargs)

            # deprecated arguments
            assert "device_type" not in kwargs, "device_type option is deprecated; current target will be used"
            assert "device" not in kwargs, "device option is deprecated; current device will be used"
            assert "stream" not in kwargs, "stream option is deprecated; current stream will be used"
            # {'num_ctas': 1, 'debug': None}
            for k in excess_kwargs:
                if k not in options.__dict__:
                    raise KeyError("Keyword argument %s was specified but unrecognised" % k)

            # bound_args是kernel的所有输入参数，包括常量参数
            """
            {'DW': tensor([[-0.4097, -0.1726, -0.0188,  ..., -0.0649,  0.0228,  0.1083],
                [ 0.3774,  0.0352, -0.2380,  ..., -1.1426, -0.4902,  0.0113],
                [ 0.3823, -0.3728,  0.5493,  ..., -0.2502,  0.0594, -0.2191],
                ...,
                [-0.5967, -0.4812, -0.4919,  ...,  0.1768, -0.1425,  0.1530],
                [ 0.3452, -0.4663,  0.8062,  ...,  0.2399, -0.5469, -0.6846],
                [ 0.3804,  0.1188,  0.1857,  ..., -0.7290,  0.3096, -0.3596]],
                device='cuda:0', dtype=torch.float16), 'DB': tensor([[ 0.4019,  0.1949,  0.3682,  ...,  0.6538,  0.4087,  0.7461],
                [-0.0181,  0.4734, -0.1302,  ..., -0.0966,  0.4004, -0.0760],
                [ 0.4890, -0.2385, -0.3589,  ..., -0.5532,  0.2712,  0.1644],
                ...,
                [ 0.3857,  0.0612, -0.2368,  ..., -0.0600, -0.3054,  0.1996],
                [ 0.6079,  0.2634, -0.2778,  ...,  0.3577,  0.2805,  0.1104],
                [-0.0302, -0.3418, -0.0416,  ...,  0.6602, -0.4807,  0.1779]],
                device='cuda:0', dtype=torch.float16), 'FINAL_DW': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', dtype=torch.float16), 'FINAL_DB': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', dtype=torch.float16), 'M': 96, 'N': 8192, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}
            """
            bound_vals = tuple(bound_args.values())

            # `None` is nullptr. Implicitly convert to *i8. This needs to be
            # done here rather than when we build the signature as otherwise
            # the kernel cache key could not distinguish between byte pointers
            # and None arguments, resulting in a downstream mismatch:
            # 非常量的参数名
            sigkeys = [self.params[i].name for i in self.non_constexpr_indices]
            # 非常量的参数的类型
            sigvals = sig_and_spec[:len(sigkeys)]
            # 拼接成 sigkeys:sigvals 的map
            signature = {k: ('*i8' if (v == 'none') else v) for (k, v) in zip(sigkeys, sigvals)}

            configs = (self._get_config(*bound_vals), )
            constants = {
                p.name: v
                for (v, p) in zip(bound_vals, self.params)
                if p.is_constexpr or p.num in configs[0].equal_to_1 or v is None
            }
            for i, arg in constants.items():
                if callable(arg):
                    raise TypeError(f"Callable constexpr at index {i} is not supported")

            if self._call_hook(key, signature, device, constants, options, configs):
                return None
            # compile the kernel
            # 转换为AST <triton.compiler.compiler.ASTSource object at 0x7fc52dcc50f0>
            src = self.ASTSource(self, signature, constants, configs[0])
            # 转换为CompiledKernel对象 <triton.compiler.compiler.CompiledKernel object at 0x7fc52dcea650>
            kernel = self.compile(
                src,
                target=target,
                options=options.__dict__,
            )
            # 将编译完的kernel存入cache中，方便之后再次执行
            self.cache[device][key] = kernel

        # Check that used global values have not changed.
        not_present = object()
        for (name, globals_dict_id), (val, globals_dict) in self.used_global_vals.items():
            if (newVal := globals_dict.get(name, not_present)) != val:
                raise RuntimeError(
                    f"Global variable {name} has changed since we compiled this kernel, from {val} to {newVal}")

        if not warmup:
            # canonicalize grid
            assert grid is not None
            if callable(grid):
                # Arguments are passed as a dict to `grid`, by contract.
                # TODO(jlebar): In the new launch API, pass the compiler flags as a
                # second parameter to `grid`.
                grid = grid(bound_args)
            grid_size = len(grid)
            grid_0 = grid[0]
            grid_1 = grid[1] if grid_size > 1 else 1
            grid_2 = grid[2] if grid_size > 2 else 1

            # launch kernel
            # todo：先不管metadata是什么
            launch_metadata = kernel.launch_metadata(grid, stream, *non_constexpr_vals)
            kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,
                       self.CompiledKernel.launch_enter_hook, self.CompiledKernel.launch_exit_hook, *non_constexpr_vals)
        return kernel
```

涉及到前端的部分，有两处：

- 生成AST：`src = self.ASTSource(self, signature, constants, configs[0])`
- 编译Kernel中的一部分：`kernel = self.compile(src,target=target,options=options.__dict__,)`

### 生成AST

### 生成Triton IR

## Triton 中端

## Triton 后端


## References

- [OpenAI/Triton MLIR 迁移工作简介 - by Chunwei Yan from Nivida Tirton开发人员](https://superjomn.github.io/posts/triton-mlir-publish/)



# Triton 源码

- [Triton 源码](#triton-源码)
  - [Overview](#overview)
  - [Triton 源码结构](#triton-源码结构)
  - [Triton 前端](#triton-前端)
    - [JIT decortor](#jit-decortor)
    - [生成AST](#生成ast)
    - [生成Triton IR](#生成triton-ir)
  - [Triton 中端](#triton-中端)
  - [Triton 后端](#triton-后端)
  - [References](#references)

## Overview

Triton 3.0 代码为例：release/3.0.x:72b422d4906de2ae0209543faa6b8cc17ea4b11f

Triton整体依然可以认为是三段式的：前端 + 中端 + 后端

![Triton compiler arch.png](../.images/Triton%20compiler%20arch.png)

- 前端：Python源码生成AST，再由AST生成Triton IR
- 中端：执行各种优化pass，Triton IR生成TritonGPU IR
- 后端：TritonGPU IR生成LLVM IR，LLVM IR由LLVM后端生成PTX，使用ptxas最终编译称为cubin

目前Triton是基于MLIR开发的，所以从这个视角看，Triton中有两个Dialect：

- Triton Dialect：硬件无关
- TritonGPU Dialect：硬件相关

## Triton 源码结构

先熟悉下triton中所有的module

```python
# triton/__init__.py
"""isort:skip_file"""
__version__ = '3.0.0'

# ---------------------------------------
# Note: import order is significant here.

# submodules
from .runtime import (
    autotune,
    Config,
    heuristics,
    JITFunction,
    KernelInterface,
    reinterpret,
    TensorWrapper,
    OutOfResources,
    InterpreterError,
    MockTensor,
)
from .runtime.jit import jit
from .compiler import compile, CompilationError
from .errors import TritonError

from . import language
from . import testing
from . import tools

__all__ = [
    "autotune",
    "cdiv",
    "CompilationError",
    "compile",
    "Config",
    "heuristics",
    "impl",
    "InterpreterError",
    "jit",
    "JITFunction",
    "KernelInterface",
    "language",
    "MockTensor",
    "next_power_of_2",
    "ops",
    "OutOfResources",
    "reinterpret",
    "runtime",
    "TensorWrapper",
    "TritonError",
    "testing",
    "tools",
]

# -------------------------------------
# misc. utilities that  don't fit well
# into any specific module
# -------------------------------------


def cdiv(x: int, y: int):
    return (x + y - 1) // y


def next_power_of_2(n: int):
    """Return the smallest power of 2 greater than or equal to n"""
    n -= 1
    n |= n >> 1
    n |= n >> 2
    n |= n >> 4
    n |= n >> 8
    n |= n >> 16
    n |= n >> 32
    n += 1
    return n

```

其中比较重要的是：

- triton.language：triton kernel相关的，包括算子以及一些类型定义；
- triton.jit：装饰器，用于触发triton compiler；
- triton.autotune：装饰器，用于出发auto tune自动调参工具；
- triton.heuristics：指定某个超参数的计算过程；
- triton.Config：超参数的搜索空间；
- triton.testing：benchmark test相关的接口；

`language/__init__.py`中能够看到提供的所有操作。

```python
# triton/language/__init__.py
__all__ = [
    "PropagateNan",
    "TRITON_MAX_TENSOR_NUMEL",
    "_experimental_descriptor_load",
    "_experimental_descriptor_store",
    "abs",
    "advance",
    "arange",
    "argmax",
    "argmin",
    "associative_scan",
    "atomic_add",
    "atomic_and",
    "atomic_cas",
    "atomic_max",
    "atomic_min",
    "atomic_or",
    "atomic_xchg",
    "atomic_xor",
    "bfloat16",
    "block_type",
    "broadcast",
    "broadcast_to",
    "builtin",
    "cat",
    "cast",
    "cdiv",
    "ceil",
    "clamp",
    "const",
    "const_pointer_type",
    "constexpr",
    "cos",
    "cumprod",
    "cumsum",
    "debug_barrier",
    "device_assert",
    "device_print",
    "div_rn",
    "dot",
    "dtype",
    "erf",
    "exp",
    "exp2",
    "expand_dims",
    "extra",
    "fdiv",
    "flip",
    "float16",
    "float32",
    "float64",
    "float8e4b15",
    "float8e4nv",
    "float8e4b8",
    "float8e5",
    "float8e5b16",
    "floor",
    "fma",
    "full",
    "function_type",
    "histogram",
    "inline_asm_elementwise",
    "interleave",
    "int1",
    "int16",
    "int32",
    "int64",
    "int8",
    "ir",
    "join",
    "load",
    "log",
    "log2",
    "make_block_ptr",
    "math",
    "max",
    "max_constancy",
    "max_contiguous",
    "maximum",
    "min",
    "minimum",
    "multiple_of",
    "num_programs",
    "pair_uniform_to_normal",
    "permute",
    "philox",
    "philox_impl",
    "pi32_t",
    "pointer_type",
    "program_id",
    "rand",
    "rand4x",
    "randint",
    "randint4x",
    "randn",
    "randn4x",
    "range",
    "ravel",
    "reduce",
    "reshape",
    "rsqrt",
    "sigmoid",
    "sin",
    "softmax",
    "sort",
    "split",
    "sqrt",
    "sqrt_rn",
    "static_assert",
    "static_print",
    "static_range",
    "store",
    "sum",
    "swizzle2d",
    "tensor",
    "trans",
    "triton",
    "uint16",
    "uint32",
    "uint64",
    "uint8",
    "uint_to_uniform_float",
    "umulhi",
    "view",
    "void",
    "where",
    "xor_sum",
    "zeros",
    "zeros_like",
]
```

Triton的核心代码都实现在python和include中

![Triton Top Level Source Code.png](../.images/Triton%20Top%20Level%20Source%20Code.png)

python目录内分为两个部分，一部分是triton，一部分是src

- triton中是上面看到的所有module的python代码
- src部分是外层include的源代码，主要就是编译器部分了

![Triton python source code.png](../.images/Triton%20python%20source%20code.png)

其实核心代码就在include、python/triton、python/src中

## Triton 前端

Triton前端代码将用户用Python编写的kernel转换为对应的Triton IR (Triton Dialect)。

### JIT decortor

这一步的入口函数，就是jit decorator。

```python
# tirton/runtime/jit.py
def jit(
    fn: Optional[T] = None,
    *,
    version=None,
    repr: Optional[Callable] = None,
    launch_metadata: Optional[Callable] = None,
    do_not_specialize: Optional[Iterable[int]] = None,
    debug: Optional[bool] = None,
    noinline: Optional[bool] = None,
) -> Union[JITFunction[T], Callable[[T], JITFunction[T]]]:
    """
    Decorator for JIT-compiling a function using the Triton compiler.

    :note: When a jit'd function is called, arguments are
        implicitly converted to pointers if they have a :code:`.data_ptr()` method
        and a `.dtype` attribute.

    :note: This function will be compiled and run on the GPU. It will only have access to:

           * python primitives,
           * builtins within the triton package,
           * arguments to this function,
           * other jit'd functions

    :param fn: the function to be jit-compiled
    :type fn: Callable
    """

    def decorator(fn: T) -> JITFunction[T]:
        assert callable(fn)
        if os.getenv("TRITON_INTERPRET", "0") == "1":
            from .interpreter import InterpretedFunction
            return InterpretedFunction(fn)
        else:
            return JITFunction(
                fn,
                version=version,
                do_not_specialize=do_not_specialize,
                debug=debug,
                noinline=noinline,
                repr=repr,
                launch_metadata=launch_metadata,
            )

    if fn is not None:
        return decorator(fn)

    else:
        return decorator
```

也就是说，我们在host代码中launch的kernel函数，实际上就是调用了JITFunction类的方法。

以vectorAdd为例子，triton中使用了和cuda类似的launch kernel方式

```python
add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
```

即，`add_kernel[grid]`调用了JITFunction的`__getitem__`方法。

因为JITFunction继承自KernelInterface类，并且JITFunction类内部没有重写`__getitem__`方法

```python
class JITFunction(KernelInterface[T]):
    ...
```

所以，`add_kernel[grid]`实际上调用了KernelInterface类的`__getitem__`方法

```python
class KernelInterface(Generic[T]):
    run: T

    def __getitem__(self, grid) -> T:
        """
        A JIT function is launched with: fn[grid](*args, **kwargs).
        Hence JITFunction.__getitem__ returns a callable proxy that
        memorizes the grid.
        """
        return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
        # return cast(T, functools.partial(cast(Callable, self.run), grid=grid))
```

返回了一个lambda函数，所以`(x, y, output, n_elements, BLOCK_SIZE=1024)`是lambda函数的调用

这里的使用方式，是子类使用父类方法，所以self是子类对象，即是子类JITFunction的run方法的调用，参数是`(x, y, output, n_elements, BLOCK_SIZE=1024)`

run方法里面最核心的三步操作：

- 获取AST：`src = self.ASTSource(self, signature, constants, configs[0])`
- 编译：`kernel = self.compile(src, target=target,options=options.__dict__,)`
- 执行：`kernel.run(grid_0, grid_1, grid_2, stream, ...)`

下面逐行分析下run方法

```python
class JITFunction(KernelInterface[T]):

    def run(self, *args, grid, warmup, **kwargs):
        # parse options
        # 获取当前设备
        device = driver.active.get_current_device()
        # 获取当前设备的stream
        stream = driver.active.get_current_stream(device)
        kwargs["debug"] = self.debug

        # Execute pre run hooks with args and kwargs
        # 执行hook，用户可以通过add_kernel.add_pre_run_hook(my_hook)方式添加pre_run_hook
        for hook in self.pre_run_hooks:
            hook(*args, **kwargs)
        
        # todo：不清楚binder的作用
        if self.binder is None:
            self.create_binder()

        bound_args, sig_and_spec, constexpr_vals, non_constexpr_vals, excess_kwargs = self.binder(*args, **kwargs)

        # compute cache key
        # 类似："*fp16*fp16*fp16*fp16i32i32DDDDDD((32, 128), {'num_ctas': 1, 'debug': None})"的string作为key
        key = ''.join(sig_and_spec) + str((constexpr_vals, excess_kwargs))
        # self.cache用于存储编译后的kernel，如果是第一次执行，cache就是一个空的defaultdict
        kernel = self.cache[device].get(key, None)

        # 第一次执行，没有kernel cache，因此要进入编译流程
        if kernel is None:
            # Kernel is not cached; we have to compile.
            # 和llvm中的target类似，就是平台相关的信息，Triton中是GPUTarget类对应了hip和cuda，同时还存储了capability和warp_size
            # GPUTarget(backend='cuda', arch=80, warp_size=32)
            target = driver.active.get_current_target()
            # <nvi.CUDABackend object at 0x7fc52dcc66e0>
            backend = self.make_backend(target)
            # {'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128, 'num_ctas': 1, 'debug': None}
            options = backend.parse_options(kwargs)

            # deprecated arguments
            assert "device_type" not in kwargs, "device_type option is deprecated; current target will be used"
            assert "device" not in kwargs, "device option is deprecated; current device will be used"
            assert "stream" not in kwargs, "stream option is deprecated; current stream will be used"
            # {'num_ctas': 1, 'debug': None}
            for k in excess_kwargs:
                if k not in options.__dict__:
                    raise KeyError("Keyword argument %s was specified but unrecognised" % k)

            # bound_args是kernel的所有输入参数，包括常量参数
            """
            {'DW': tensor([[-0.4097, -0.1726, -0.0188,  ..., -0.0649,  0.0228,  0.1083],
                [ 0.3774,  0.0352, -0.2380,  ..., -1.1426, -0.4902,  0.0113],
                [ 0.3823, -0.3728,  0.5493,  ..., -0.2502,  0.0594, -0.2191],
                ...,
                [-0.5967, -0.4812, -0.4919,  ...,  0.1768, -0.1425,  0.1530],
                [ 0.3452, -0.4663,  0.8062,  ...,  0.2399, -0.5469, -0.6846],
                [ 0.3804,  0.1188,  0.1857,  ..., -0.7290,  0.3096, -0.3596]],
                device='cuda:0', dtype=torch.float16), 'DB': tensor([[ 0.4019,  0.1949,  0.3682,  ...,  0.6538,  0.4087,  0.7461],
                [-0.0181,  0.4734, -0.1302,  ..., -0.0966,  0.4004, -0.0760],
                [ 0.4890, -0.2385, -0.3589,  ..., -0.5532,  0.2712,  0.1644],
                ...,
                [ 0.3857,  0.0612, -0.2368,  ..., -0.0600, -0.3054,  0.1996],
                [ 0.6079,  0.2634, -0.2778,  ...,  0.3577,  0.2805,  0.1104],
                [-0.0302, -0.3418, -0.0416,  ...,  0.6602, -0.4807,  0.1779]],
                device='cuda:0', dtype=torch.float16), 'FINAL_DW': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', dtype=torch.float16), 'FINAL_DB': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', dtype=torch.float16), 'M': 96, 'N': 8192, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}
            """
            bound_vals = tuple(bound_args.values())

            # `None` is nullptr. Implicitly convert to *i8. This needs to be
            # done here rather than when we build the signature as otherwise
            # the kernel cache key could not distinguish between byte pointers
            # and None arguments, resulting in a downstream mismatch:
            # 非常量的参数名
            sigkeys = [self.params[i].name for i in self.non_constexpr_indices]
            # 非常量的参数的类型
            sigvals = sig_and_spec[:len(sigkeys)]
            # 拼接成 sigkeys:sigvals 的map
            signature = {k: ('*i8' if (v == 'none') else v) for (k, v) in zip(sigkeys, sigvals)}

            configs = (self._get_config(*bound_vals), )
            constants = {
                p.name: v
                for (v, p) in zip(bound_vals, self.params)
                if p.is_constexpr or p.num in configs[0].equal_to_1 or v is None
            }
            for i, arg in constants.items():
                if callable(arg):
                    raise TypeError(f"Callable constexpr at index {i} is not supported")

            if self._call_hook(key, signature, device, constants, options, configs):
                return None
            # compile the kernel
            # 转换为AST <triton.compiler.compiler.ASTSource object at 0x7fc52dcc50f0>
            src = self.ASTSource(self, signature, constants, configs[0])
            # 转换为CompiledKernel对象 <triton.compiler.compiler.CompiledKernel object at 0x7fc52dcea650>
            kernel = self.compile(
                src,
                target=target,
                options=options.__dict__,
            )
            # 将编译完的kernel存入cache中，方便之后再次执行
            self.cache[device][key] = kernel

        # Check that used global values have not changed.
        not_present = object()
        for (name, globals_dict_id), (val, globals_dict) in self.used_global_vals.items():
            if (newVal := globals_dict.get(name, not_present)) != val:
                raise RuntimeError(
                    f"Global variable {name} has changed since we compiled this kernel, from {val} to {newVal}")

        if not warmup:
            # canonicalize grid
            assert grid is not None
            if callable(grid):
                # Arguments are passed as a dict to `grid`, by contract.
                # TODO(jlebar): In the new launch API, pass the compiler flags as a
                # second parameter to `grid`.
                grid = grid(bound_args)
            grid_size = len(grid)
            grid_0 = grid[0]
            grid_1 = grid[1] if grid_size > 1 else 1
            grid_2 = grid[2] if grid_size > 2 else 1

            # launch kernel
            # todo：先不管metadata是什么
            launch_metadata = kernel.launch_metadata(grid, stream, *non_constexpr_vals)
            kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,
                       self.CompiledKernel.launch_enter_hook, self.CompiledKernel.launch_exit_hook, *non_constexpr_vals)
        return kernel
```

涉及到前端的部分，有两处：

- 生成AST：`src = self.ASTSource(self, signature, constants, configs[0])`
- 编译Kernel中的一部分：`kernel = self.compile(src,target=target,options=options.__dict__,)`

### 生成AST

```python
src = self.ASTSource(self, signature, constants, configs[0])
```

代码的返回值是 `<triton.compiler.compiler.ASTSource object at 0x7fc52dcc50f0>`，就是下面的ASTSource的对象

```python
class ASTSource:

    def __init__(self, fn, signature, constants=None, attrs=None) -> None:
        self.fn = fn
        self.ext = "ttir"
        self.name = fn.__name__
        self.signature = signature
        self.constants = constants
        self.attrs = attrs
        if isinstance(self.signature, str):
            self.signature = {k: v.strip() for k, v in enumerate(self.signature.split(","))}
        if self.constants is None:
            self.constants = dict()
        if self.attrs is None:
            self.attrs = AttrsDescriptor()

    def hash(self):
        sorted_sig = [v for k, v in sorted(self.signature.items())]
        # Note - we stringify the keys here to allow sorting to work for cases
        # where constants have mixed int/str keys.
        sorted_constants = sorted((str(k), v) for k, v in self.constants.items())
        key = f"{self.fn.cache_key}-{self.attrs.hash()}-{sorted_sig}-{sorted_constants}"
        return hashlib.sha256(key.encode("utf-8")).hexdigest()

    def make_ir(self, options, codegen_fns, context):
        return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)

    def parse_options(self):
        return dict()
```

TODO：暂时留一个疑问，为什么不是真正的AST，而只是一个所谓的ASTSource对象呢？

### 生成Triton IR

`self.compile(...)`实际上调用的是Triton中comiler module下的compile函数

- stages：`{'ttir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6710>, 'ttgir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6d40>, 'llir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6dd0>, 'ptx': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6e60>, 'cubin': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6ef0>}`

```python
# triton/compiler/compiler.py:226
def compile(src, target=None, options=None):
    if target is None:
        target = driver.active.get_current_target()
    assert isinstance(target, GPUTarget), "target must be of GPUTarget type"
    backend = make_backend(target)
    # 第一次编译的话，src是ASTSource，ir_source是false
    ir_source = not isinstance(src, ASTSource)
    # create backend
    if ir_source:
        assert isinstance(src, str), "source must be either AST or a filepath"
        src = IRSource(src)
    extra_options = src.parse_options()
    options = backend.parse_options(dict(options or dict(), **extra_options))
    # create cache manager
    env_vars = get_cache_invalidating_env_vars()
    key = f"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}"
    hash = hashlib.sha256(key.encode("utf-8")).hexdigest()
    fn_cache_manager = get_cache_manager(hash)
    # For dumping/overriding only hash the source as we want it to be independent of triton
    # core changes to make it easier to track kernels by hash.
    enable_override = os.environ.get("TRITON_KERNEL_OVERRIDE", "0") == "1"
    enable_ir_dump = os.environ.get("TRITON_KERNEL_DUMP", "0") == "1"
    fn_override_manager = get_override_manager(src.hash()) if enable_override else None
    fn_dump_manager = get_dump_manager(src.hash()) if enable_ir_dump else None
    metadata_filename = f"{src.name}.json"
    metadata_group = fn_cache_manager.get_group(metadata_filename) or {}
    metadata_path = metadata_group.get(metadata_filename)
    always_compile = os.environ.get("TRITON_ALWAYS_COMPILE", "0") == "1"
    if not always_compile and metadata_path is not None:
        # cache hit!
        metadata = json.loads(Path(metadata_path).read_text())
        return CompiledKernel(src, metadata_group, hash)
    # initialize metadata
    metadata = {
        "hash": hash,
        "target": target,
        **options.__dict__,
        **env_vars,
    }
    # run compilation pipeline  and populate metadata
    stages = dict()
    backend.add_stages(stages, options)
    # src.ext = "ttir"
    first_stage = list(stages.keys()).index(src.ext)
    # when the source is an IR file, don't apply the passes related to this stage. This makes it easier to write IR level tests.
    if ir_source:
        first_stage += 1
    # ir来自于triton._C模块，已经不是python代码了，<module 'triton._C.libtriton.ir'>
    # context: <triton._C.libtriton.ir.context object at 0x7f28fd677b70>
    # todo: context内有什么
    context = ir.context()
    ir.load_dialects(context)
    backend.load_dialects(context)
    codegen_fns = backend.get_codegen_implementation()
    try:
        # 调用ASTSource的make_ir方法获取到 <triton._C.libtriton.ir.module object at 0x7f28b5fe0ef0>
        module = src.make_ir(options, codegen_fns, context)
    except Exception as e:
        filter_traceback(e)
        raise
    use_ttgir_loc = os.environ.get("USE_TTGIR_LOC", "0") == "1"
    # 按照stage从AST开始一步步生成，同时进行优化，每一步生成的结果会单独存储到metadata_group[ir_filename]中
    for ext, compile_ir in list(stages.items())[first_stage:]:
        next_module = compile_ir(module, metadata)
        ir_filename = f"{src.name}.{ext}"
        metadata_group[ir_filename] = fn_cache_manager.put(next_module, ir_filename)
        if fn_dump_manager is not None:
            fn_dump_manager.put(next_module, ir_filename)
        if (fn_override_manager is not None and fn_override_manager.has_file(ir_filename)):
            print(f"\nOverriding kernel with file {ir_filename}")
            full_name = fn_override_manager.get_file(ir_filename)
            next_module = parse(full_name, ext, context)
        # use an env variable to parse ttgir from file
        if use_ttgir_loc and ext == "ttgir":
            ttgir_full_name = fn_cache_manager.get_file(ir_filename)
            next_module.create_location_snapshot(ttgir_full_name)
            print(f"Create new locations for {ttgir_full_name}")
        module = next_module
    # write-back metadata
    metadata_group[metadata_filename] = fn_cache_manager.put(json.dumps(metadata, default=vars), metadata_filename,binary=False)
    fn_cache_manager.put_group(metadata_filename, metadata_group)
    # return handle to compiled kernel
    return CompiledKernel(src, metadata_group, hash)
```

也就是说，真正的代码生成存在于`module = src.make_ir(options, codegen_fns, context)`和之后的for loop内

先看第一步的make_ir究竟做了什么

回到ASTSource

```python
class ASTSource:
    def make_ir(self, options, codegen_fns, context):
        return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)
```

调用了code_generator中的ast_to_ttir函数

```python
# python/triton/compiler/code_generator.py
def ast_to_ttir(fn, specialization, context, options, codegen_fns):
    # fn还是JITFunction
    attrs = specialization.attrs
    # create kernel prototype
    cst_key = lambda i: fn.arg_names.index(i) if isinstance(i, str) else i
    constants = {cst_key(key): value for key, value in specialization.constants.items()}
    # visit kernel AST
    gscope = fn.__globals__.copy()
    function_name = fn.repr(specialization)
    tys = list(specialization.signature.values())
    new_constants = {k: True if k in tys and tys[k] == "i1" else 1 for k in attrs.equal_to_1}
    new_attrs = {k: [("tt.divisibility", 16)] for k in attrs.divisible_by_16}

    all_constants = constants.copy()
    all_constants.update(new_constants)
    arg_types = [str_to_ty(v) for k, v in specialization.signature.items() if k not in specialization.constants]
    file_name, begin_line = _get_fn_file_line(fn)

    prototype = language.function_type([], arg_types)
    generator = CodeGenerator(context, prototype, gscope=gscope, constants=all_constants, function_name=function_name,
                              jit_fn=fn, attributes=new_attrs, is_kernel=True, file_name=file_name,
                              begin_line=begin_line, options=options, codegen_fns=codegen_fns)
    # fn.parse(): <ast.Module object at 0x7f78ef4ad2a0>
    # 输入是AST
    generator.visit(fn.parse())
    # 输出已经是ir module了：<triton._C.libtriton.ir.module object at 0x7f78ef477e20>
    ret = generator.module
    # module takes ownership of the context
    ret.context = context
    return ret
```

也就是说，这里做了两步操作，一个是`fn.parse`获取Python AST，然后是`generator.visit`生成TritonIR

回到JITFunction，可以看到，实际上就是调用的python官方的ast方法获取的AST，self.src则是源码的字符串

```python
import ast
class JITFunction(KernelInterface[T]):
    def parse(self):
        tree = ast.parse(self.src)
        assert isinstance(tree, ast.Module)
        assert len(tree.body) == 1
        assert isinstance(tree.body[0], ast.FunctionDef)
        return tree
```

self.src示例：

```shell
(Pdb) p self.src
"def add_kernel(\n    x_ptr,  # *Pointer* to first input vector.\n    y_ptr,  # *Pointer* to second input vector.\n    output_ptr,  # *Pointer* to output vector.\n    n_elements,  # Size of the vector.\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n    # NOTE: `constexpr` so it can be used as a shape value.\n):\n    # There are multiple 'programs' processing different data. We identify which program\n    # we are here:\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n    # This program will process inputs that are offset from the initial data.\n    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n    # Note that offsets is a list of pointers:\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    # Create a mask to guard memory operations against out-of-bounds accesses.\n    mask = offsets < n_elements\n    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n    # multiple of the block size.\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    # Write x + y back to DRAM.\n    tl.store(output_ptr + offsets, output, mask=mask)\n"
```

返回的tree则是`<ast.Module object at 0x7f061493f9a0>`，这是一个树。

这里没有直接遍历它，而是查看了下它的root node是不是ast.FunctionDef（之后生成tirton ir的时候才回去遍历它），那就在自己写一个方法来遍历下这个树，把它print出来看看它长什么样子

```python
def print_ast(node, level=0):
    # 打印当前节点的类型和一些信息
    print('  ' * level + type(node).__name__)
    
    # 遍历所有属性，如果属性值是AST节点，递归打印
    for field, value in ast.iter_fields(node):
        if isinstance(value, list):
            for item in value:
                if isinstance(item, ast.AST):
                    print_ast(item, level + 1)
        elif isinstance(value, ast.AST):
            print_ast(value, level + 1)
```

获得输出如下，拿这个AST和add_kernel函数对比是能完全对应的，毕竟只是翻译。具体怎么翻译过来的先不深究。

```python
Module
  FunctionDef
    arguments
      arg
      arg
      arg
      arg
      arg
        Attribute
          Name
            Load
          Load
    Assign
      Name
        Store
      Call
        Attribute
          Name
            Load
          Load
        keyword
          Constant
    Assign
      Name
        Store
      BinOp
        Name
          Load
        Mult
        Name
          Load
    Assign
      Name
        Store
      BinOp
        Name
          Load
        Add
        Call
          Attribute
            Name
              Load
            Load
          Constant
          Name
            Load
    Assign
      Name
        Store
      Compare
        Name
          Load
        Lt
        Name
          Load
    Assign
      Name
        Store
      Call
        Attribute
          Name
            Load
          Load
        BinOp
          Name
            Load
          Add
          Name
            Load
        keyword
          Name
            Load
    Assign
      Name
        Store
      Call
        Attribute
          Name
            Load
          Load
        BinOp
          Name
            Load
          Add
          Name
            Load
        keyword
          Name
            Load
    Assign
      Name
        Store
      BinOp
        Name
          Load
        Add
        Name
          Load
    Expr
      Call
        Attribute
          Name
            Load
          Load
        BinOp
          Name
            Load
          Add
          Name
            Load
        Name
          Load
        keyword
          Name
            Load
```

完成`fn.parse()`获取到AST之后，进入visit方法`generator.visit(fn.parse())`

```python
class CodeGenerator(ast.NodeVisitor):
    def visit(self, node):
        if node is None:
            return
        with warnings.catch_warnings():
            # The ast library added visit_Constant and deprecated some other
            # methods but we can't move to that without breaking Python 3.6 and 3.7.
            warnings.simplefilter("ignore", DeprecationWarning)  # python 3.9
            warnings.simplefilter("ignore", PendingDeprecationWarning)  # python 3.8

            # 下面两行为了保留现场
            # cur_node初始化为None
            last_node = self.cur_node
            # self.builder也来自于triton._C
            last_loc = self.builder.get_loc()
            # node为AST的root node，也就是FunctionDef
            self.cur_node = node
            if hasattr(node, 'lineno') and hasattr(node, 'col_offset'):
                self.builder.set_loc(self.file_name, self.begin_line + node.lineno, node.col_offset)
                last_loc = self.builder.get_loc()
            try:
                # 子类调用父类的visit方法
                ret = super().visit(node)
            except CompilationError:
                raise
            except Exception as e:
                # Wrap the error in a CompilationError which contains the source
                # of the @jit function.
                raise CompilationError(self.jit_fn.src, self.cur_node, repr(e)) from None

            # Reset the location to the last one before the visit
            if last_loc:
                self.cur_node = last_node
                self.builder.set_loc(last_loc)
            return ret
```

进一步深入到父类`ast.NodeVisitor`的visit方法，它还是python内置的ast module。

```python
import ast
class CodeGenerator(ast.NodeVisitor):
    ...
```

如果深入进去看的话，实际上就是上面实现的深度优先遍历 + 子类的visit方法，只是没有print，不知道它长什么样子。

```python
class NodeVisitor(object):
    def visit(self, node):
        """Visit a node."""
        # 这里的self是子类
        method = 'visit_' + node.__class__.__name__
        # 获取子类中的当前节点visit的方法，否则就是generic_visit
        visitor = getattr(self, method, self.generic_visit)
        return visitor(node)

    def generic_visit(self, node):
        """Called if no explicit visitor function exists for a node."""
        for field, value in iter_fields(node):
            if isinstance(value, list):
                for item in value:
                    if isinstance(item, AST):
                        self.visit(item)
            elif isinstance(value, AST):
                self.visit(value)
```

为了能够完成转换，这里最关键的一步是`visitor = getattr(self, method, self.generic_visit)`，vistor实际上是子类CodeGenerator中的`'visit_' + node.__class__.__name__`组合起来的方法。

例如：

```python
class CodeGenerator(ast.NodeVisitor):
    def visit_Module:
        ...
    def visit_FunctionDef:
        ...
    def visit_Load:
        ...
    ...
```

以visit_FunctionDef为例子，看下它里面做了什么，重点是怎么从AST称为Triton IR的

```python
class CodeGenerator(ast.NodeVisitor):
    def visit_FunctionDef(self, node):
        # visit我们实现的kernel函数的入参
        # arg_names: ['x_ptr', 'y_ptr', 'output_ptr', 'n_elements', 'BLOCK_SIZE']
        # kwarg_names: None
        arg_names, kwarg_names = self.visit(node.args)
        if self.fn:
            raise self._unsupported(node, "nested function definition is not supported.")
        # initialize defaults
        # vector add这个例子里没有default args
        for i, default_value in enumerate(node.args.defaults):
            arg_node = node.args.args[-i - 1]
            annotation = arg_node.annotation
            name = arg_node.arg
            st_target = ast.Name(id=name, ctx=ast.Store())
            if annotation is None:
                init_node = ast.Assign(targets=[st_target], value=default_value)
            else:
                init_node = ast.AnnAssign(target=st_target, value=default_value, annotation=annotation)

            try:
                assert not self.visiting_arg_default_value
                self.visiting_arg_default_value = True
                self.visit(init_node)
            finally:
                self.visiting_arg_default_value = False

        # initialize function
        visibility = "public" if self.is_kernel else "private"
        # 通过builder获取到self.fn：<triton._C.libtriton.ir.function object at 0x7f6835d001f0>
        # self.module: <triton._C.libtriton.ir.module object at 0x7f682cc0aca0>
        self.fn = self.builder.get_or_insert_function(self.module, self.function_name,
                                                      self.prototype.to_ir(self.builder), visibility, self.noinline)
        self.module.push_back(self.fn)
        entry = self.fn.add_entry_block()
        arg_values = []
        idx = 0
        for i, arg_name in enumerate(arg_names):
            if i in self.constants:
                cst = self.constants[i]
                if not _is_constexpr(cst):
                    cst = constexpr(self.constants[i])
                arg_values.append(cst)
                continue
            else:
                if i in self.attributes:
                    for name, value in self.attributes[i]:
                        self.fn.set_arg_attr(idx, name, value)
                arg_values.append(tensor(self.fn.args(idx), self.prototype.param_types[idx]))
                idx += 1

        insert_pt = self.builder.get_insertion_block()
        for arg_name, arg_value in zip(arg_names, arg_values):
            self.set_value(arg_name, arg_value)
        self.builder.set_insertion_point_to_start(entry)
        # visit function body
        self.visit_compound_statement(node.body)
        # finalize function
        if self.ret_type is None or self.ret_type == language.void:
            self.ret_type = language.void
            self.builder.ret([])
        else:
            # update return type
            if isinstance(self.ret_type, tuple):
                self.prototype.ret_types = list(self.ret_type)
                self.fn.reset_type(self.prototype.to_ir(self.builder))
            else:
                self.prototype.ret_types = [self.ret_type]
                self.fn.reset_type(self.prototype.to_ir(self.builder))
        if insert_pt:
            self.builder.set_insertion_point_to_end(insert_pt)
        # Remove dead code
        self.fn.finalize()
```

实际的转换动作由builder对象负责，它在code_generator的`__init__`中被初始化。它是来自_C.libtriton中的module。

```python
from .._C.libtriton import ir

class CodeGenerator(ast.NodeVisitor):

    def __init__(self, context, prototype, gscope, attributes, constants, function_name, jit_fn: JITFunction, options,
                 codegen_fns, debug=None, module=None, is_kernel=False, function_types: Optional[Dict] = None,
                 noinline=False, file_name: Optional[str] = None, begin_line=0):
        self.context = context
        self.builder = ir.builder(context)
        ...
```

在本文已开始，我们看过了Triton的项目结构，c++相关的定义都在src中，来到src/main.cc可以知道这个module的定义

```python
# python/src/main.cc
PYBIND11_MODULE(libtriton, m) {
  m.doc() = "Python bindings to the C++ Triton API";
  init_triton_env_vars(m);
  init_triton_ir(m.def_submodule("ir"));
  init_triton_passes(m.def_submodule("passes"));
  init_triton_interpreter(m.def_submodule("interpreter"));
  init_triton_llvm(m.def_submodule("llvm"));
  FOR_EACH_P(INIT_BACKEND, TRITON_BACKENDS_TUPLE)
}
```

继续深入到`init_triton_ir`中找builder的binding，这个class binding足足有500+行，TritonOpBuilder就是它实际的c++类了。

```python
# python/src/ir.cc
py::class_<TritonOpBuilder>(m, "builder", py::module_local(),
                              py::dynamic_attr())
      .def(py::init<MLIRContext *>())
      // getters
      .def("create_module",
           [](TritonOpBuilder &self) -> ModuleOp {
             return self.create<ModuleOp>();
           })
      // insertion block/point
      .def("set_insertion_point_to_start",
           [](TritonOpBuilder &self, Block &block) -> void {
             self.setInsertionPointToStart(block);
           })
      .def("set_insertion_point_to_end",
           [](TritonOpBuilder &self, Block &block) {
             self.setInsertionPointToEnd(block);
           }
        ...
      );
```

以前面遇到的`get_or_insert_function`为例：

```python
self.fn = self.builder.get_or_insert_function(self.module, self.function_name,
                                              self.prototype.to_ir(self.builder), visibility, self.noinline)
```

```c++
// Ops
.def("get_or_insert_function",
      [](TritonOpBuilder &self, ModuleOp &module, std::string &funcName,
        Type &funcType, std::string &visibility,
        bool noinline) -> FuncOp {
        if (Operation *funcOperation = module.lookupSymbol(funcName))
          # 如果当前module中已经创建过这个function，则直接返回它
          return llvm::dyn_cast<FuncOp>(funcOperation);
        if (auto funcTy = dyn_cast<FunctionType>(funcType)) {
          # 如果没有，则按照它的函数名、函数定义、函数属性创建一个FuncOp出来
          llvm::SmallVector<NamedAttribute> attrs = {
              NamedAttribute(
                  self.getBuilder().getStringAttr("sym_visibility"),
                  self.getBuilder().getStringAttr(visibility)),
              NamedAttribute(self.getBuilder().getStringAttr("noinline"),
                            self.getBuilder().getBoolAttr(noinline))};
          return self.create<FuncOp>(funcName, funcTy, attrs);
        }
        throw std::invalid_argument("invalid function type");
      })
```

可以看到，实际执行TritonIR的类型创建的，是又TritonOpBuilder中的builder负责的

```c++
class TritonOpBuilder {
// ...
private:
  std::unique_ptr<OpBuilder> builder;
  std::unique_ptr<Location> lastLoc;
// ...
}
```

熟悉的OpBuilder，[mlir::OpBuilder](https://mlir.llvm.org/doxygen/classmlir_1_1OpBuilder.html)，MLIR提供的标准接口用于创建OP。

以及create出来的FuncOp类型，也是MLIR中定义的标准类型

```c++
class FuncOp : public ::mlir::Op<FuncOp, ::mlir::OpTrait::OneRegion, ::mlir::OpTrait::ZeroResults, ::mlir::OpTrait::ZeroSuccessors, ::mlir::OpTrait::ZeroOperands, ::mlir::OpTrait::OpInvariants, ::mlir::BytecodeOpInterface::Trait, ::mlir::OpTrait::AffineScope, ::mlir::OpTrait::AutomaticAllocationScope, ::mlir::CallableOpInterface::Trait, ::mlir::SymbolOpInterface::Trait, ::mlir::FunctionOpInterface::Trait, ::mlir::OpTrait::IsIsolatedFromAbove, ::mlir::OpAsmOpInterface::Trait> {
// ...
}
```

它在Triton也进行了python binding

```c++
  py::class_<FuncOp, OpState>(m, "function", py::module_local())
      // .def_property_readonly("attrs", &ir::function::attrs)
      // .def("add_attr", &ir::function::add_attr);
      .def("args",
           [](FuncOp &self, unsigned idx) -> BlockArgument {
             if (idx >= self.getNumArguments())
               throw pybind11::index_error(
                   "Function argument index out of range");
             return self.getArgument(idx);
           })
      .def(
          "add_entry_block",
          [](FuncOp &self) -> Block * { return self.addEntryBlock(); },
          ret::reference)
      .def(
          "set_arg_attr",
          [](FuncOp &self, int arg_no, const std::string &name, int val) {
            // set arg attributes "name" to value "val"
            auto attrTy = IntegerType::get(self.getContext(), 32);
            self.setArgAttr(arg_no, name, IntegerAttr::get(attrTy, val));
          },
          ret::reference)
      //  .def("has_attr", &::FuncOp::hasAttr)
      .def("finalize",
           [](FuncOp &self) -> void {
             // Remove dead code
             // 1. Unreachable code after return
             self.walk([&](Block *block) {
               Operation *retOp = nullptr;
               // It's better to not use walk here because we only want to
               // check operations in the current block
               for (auto &op : block->getOperations()) {
                 if (isa<ReturnOp>(op))
                   if (retOp == nullptr) {
                     retOp = &op;
                     break;
                   }
               }
               if (retOp && retOp != &block->back()) {
                 auto pos = retOp->getIterator();
                 pos++;
                 auto *newBlock = block->splitBlock(pos);
                 newBlock->erase();
               }
             });
             // 2. Check if the result of tl.advance is used
             self.walk([&](Operation *op) {
               if (isa<AdvanceOp>(op) && op->getResult(0).use_empty())
                 outputWarning(op->getLoc(), "The result of tl.advance is not "
                                             "being used. Note that tl.advance "
                                             "does not have any side effects. "
                                             "To move the block pointer, you "
                                             "need to assign the result of "
                                             "tl.advance to a variable.");
             });
           })
      .def_property_readonly("type", &FuncOp::getFunctionType)
      .def("reset_type", &FuncOp::setType);
```

所以，当get_or_insert_function返回FuncOp类型时，python侧查看到的返回类型是

```python
# self.module: <triton._C.libtriton.ir.module object at 0x7f682cc0aca0>
self.fn = self.builder.get_or_insert_function(self.module, self.function_name,
                                              self.prototype.to_ir(self.builder), visibility, self.noinline)
```

之后，fn在加入到module中

```python
self.module.push_back(self.fn)
```

对应到c++世界中如下：

```c++
py::class_<ModuleOp, OpState>(m, "module", py::module_local(),
                              py::dynamic_attr())
    // ...
    .def("push_back",
          [](ModuleOp &self, FuncOp &funcOp) -> void {
            self.push_back(funcOp);
          })
    // ...
```

经过code generator完成AST Tree的遍历后，即可获得最终的ModuleOp，它就是ast_to_ttir函数的返回值，也就是最终的TritonIR

```python
def ast_to_ttir(fn, specialization, context, options, codegen_fns):
    ...
    generator.visit(fn.parse())

    ret = generator.module
    # module takes ownership of the context
    ret.context = context
    return ret
```

通过设置环境变量TRITON_CACHE_DIR=./和TRITON_KERNEL_DUMP=1，我们可以将所有的IR保存到当前路径

vector add用例的TritonIR如下，和我们在python中写的add_kernel亦是能对应上的。

```c++
// add_kernel.ttir
#loc = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0)
module {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0)) attributes {noinline = false} {
    %c1024_i32 = arith.constant 1024 : i32 loc(#loc1)
    %0 = tt.get_program_id x : i32 loc(#loc2)
    %1 = arith.muli %0, %c1024_i32 : i32 loc(#loc3)
    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32> loc(#loc4)
    %3 = tt.splat %1 : i32 -> tensor<1024xi32> loc(#loc5)
    %4 = arith.addi %3, %2 : tensor<1024xi32> loc(#loc5)
    %5 = tt.splat %arg3 : i32 -> tensor<1024xi32> loc(#loc6)
    %6 = arith.cmpi slt, %4, %5 : tensor<1024xi32> loc(#loc6)
    %7 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc7)
    %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc7)
    %9 = tt.load %8, %6 : tensor<1024x!tt.ptr<f32>> loc(#loc8)
    %10 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc9)
    %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc9)
    %12 = tt.load %11, %6 : tensor<1024x!tt.ptr<f32>> loc(#loc10)
    %13 = arith.addf %9, %12 : tensor<1024xf32> loc(#loc11)
    %14 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc12)
    %15 = tt.addptr %14, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc12)
    tt.store %15, %13, %6 : tensor<1024x!tt.ptr<f32>> loc(#loc13)
    tt.return loc(#loc14)
  } loc(#loc)
} loc(#loc)
#loc1 = loc(unknown)
#loc2 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":27:24)
#loc3 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":32:24)
#loc4 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":33:41)
#loc5 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":33:28)
#loc6 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":35:21)
#loc7 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":38:24)
#loc8 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":38:16)
#loc9 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":39:24)
#loc10 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":39:16)
#loc11 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":40:17)
#loc12 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:26)
#loc13 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:35)
#loc14 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:4)
```

至此，Triton前端的使命已经完成！我们也顺利从Python世界进入到C++世界，再进入到了MLIR世界。

### Triton Dialect

此处还有一个重要问题，我们一开始说的Trition定义了两个MLIR Dialect，一个是Triton Dialect，另一个TritonGPU Dialect。

而Triton IR已经生成了，似乎除了OpBuilder，ModuleOp，FuncOp之外，还没有完整的看一下Triton Dialect的定义。

在Triton Doc中可以找到Triton Dialect的说明：[‘tt’ Dialect](https://triton-lang.org/main/dialects/TritonDialect.html)

代码中，Triton Dialect被实现在`python/triton/_C/include/triton/Dialect/Triton/`路径下。

## Triton 中端

## Triton 后端


## References

- [OpenAI/Triton MLIR 迁移工作简介 - by Chunwei Yan from Nivida Tirton开发人员](https://superjomn.github.io/posts/triton-mlir-publish/)
- [Triton Document Dialect](https://triton-lang.org/main/dialects/)



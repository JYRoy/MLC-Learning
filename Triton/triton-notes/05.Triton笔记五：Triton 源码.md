# Triton 源码

- [Triton 源码](#triton-源码)
  - [Overview](#overview)
  - [Triton 源码结构](#triton-源码结构)
  - [Triton 前端](#triton-前端)
    - [JIT decortor](#jit-decortor)
    - [生成AST](#生成ast)
    - [生成Triton IR](#生成triton-ir)
    - [Triton Dialect](#triton-dialect)
  - [Triton 中端](#triton-中端)
  - [Triton 后端](#triton-后端)
  - [References](#references)

## Overview

Triton 3.0 代码为例：release/3.0.x:72b422d4906de2ae0209543faa6b8cc17ea4b11f

Triton整体依然可以认为是三段式的：前端 + 中端 + 后端

![Triton compiler arch.png](../.images/Triton%20compiler%20arch.png)

- 前端：Python源码生成AST，再由AST生成Triton IR
- 中端：执行各种优化pass，Triton IR生成TritonGPU IR
- 后端：TritonGPU IR生成LLVM IR，LLVM IR由LLVM后端生成PTX，使用ptxas最终编译称为cubin

目前Triton是基于MLIR开发的，所以从这个视角看，Triton中有两个Dialect：

- Triton Dialect：硬件无关
- TritonGPU Dialect：硬件相关

## Triton 源码结构

先熟悉下triton中所有的module

```python
# triton/__init__.py
"""isort:skip_file"""
__version__ = '3.0.0'

# ---------------------------------------
# Note: import order is significant here.

# submodules
from .runtime import (
    autotune,
    Config,
    heuristics,
    JITFunction,
    KernelInterface,
    reinterpret,
    TensorWrapper,
    OutOfResources,
    InterpreterError,
    MockTensor,
)
from .runtime.jit import jit
from .compiler import compile, CompilationError
from .errors import TritonError

from . import language
from . import testing
from . import tools

__all__ = [
    "autotune",
    "cdiv",
    "CompilationError",
    "compile",
    "Config",
    "heuristics",
    "impl",
    "InterpreterError",
    "jit",
    "JITFunction",
    "KernelInterface",
    "language",
    "MockTensor",
    "next_power_of_2",
    "ops",
    "OutOfResources",
    "reinterpret",
    "runtime",
    "TensorWrapper",
    "TritonError",
    "testing",
    "tools",
]

# -------------------------------------
# misc. utilities that  don't fit well
# into any specific module
# -------------------------------------


def cdiv(x: int, y: int):
    return (x + y - 1) // y


def next_power_of_2(n: int):
    """Return the smallest power of 2 greater than or equal to n"""
    n -= 1
    n |= n >> 1
    n |= n >> 2
    n |= n >> 4
    n |= n >> 8
    n |= n >> 16
    n |= n >> 32
    n += 1
    return n

```

其中比较重要的是：

- triton.language：triton kernel相关的，包括算子以及一些类型定义；
- triton.jit：装饰器，用于触发triton compiler；
- triton.autotune：装饰器，用于出发auto tune自动调参工具；
- triton.heuristics：指定某个超参数的计算过程；
- triton.Config：超参数的搜索空间；
- triton.testing：benchmark test相关的接口；

`language/__init__.py`中能够看到提供的所有操作。

```python
# triton/language/__init__.py
__all__ = [
    "PropagateNan",
    "TRITON_MAX_TENSOR_NUMEL",
    "_experimental_descriptor_load",
    "_experimental_descriptor_store",
    "abs",
    "advance",
    "arange",
    "argmax",
    "argmin",
    "associative_scan",
    "atomic_add",
    "atomic_and",
    "atomic_cas",
    "atomic_max",
    "atomic_min",
    "atomic_or",
    "atomic_xchg",
    "atomic_xor",
    "bfloat16",
    "block_type",
    "broadcast",
    "broadcast_to",
    "builtin",
    "cat",
    "cast",
    "cdiv",
    "ceil",
    "clamp",
    "const",
    "const_pointer_type",
    "constexpr",
    "cos",
    "cumprod",
    "cumsum",
    "debug_barrier",
    "device_assert",
    "device_print",
    "div_rn",
    "dot",
    "dtype",
    "erf",
    "exp",
    "exp2",
    "expand_dims",
    "extra",
    "fdiv",
    "flip",
    "float16",
    "float32",
    "float64",
    "float8e4b15",
    "float8e4nv",
    "float8e4b8",
    "float8e5",
    "float8e5b16",
    "floor",
    "fma",
    "full",
    "function_type",
    "histogram",
    "inline_asm_elementwise",
    "interleave",
    "int1",
    "int16",
    "int32",
    "int64",
    "int8",
    "ir",
    "join",
    "load",
    "log",
    "log2",
    "make_block_ptr",
    "math",
    "max",
    "max_constancy",
    "max_contiguous",
    "maximum",
    "min",
    "minimum",
    "multiple_of",
    "num_programs",
    "pair_uniform_to_normal",
    "permute",
    "philox",
    "philox_impl",
    "pi32_t",
    "pointer_type",
    "program_id",
    "rand",
    "rand4x",
    "randint",
    "randint4x",
    "randn",
    "randn4x",
    "range",
    "ravel",
    "reduce",
    "reshape",
    "rsqrt",
    "sigmoid",
    "sin",
    "softmax",
    "sort",
    "split",
    "sqrt",
    "sqrt_rn",
    "static_assert",
    "static_print",
    "static_range",
    "store",
    "sum",
    "swizzle2d",
    "tensor",
    "trans",
    "triton",
    "uint16",
    "uint32",
    "uint64",
    "uint8",
    "uint_to_uniform_float",
    "umulhi",
    "view",
    "void",
    "where",
    "xor_sum",
    "zeros",
    "zeros_like",
]
```

Triton的核心代码都实现在python和include中

![Triton Top Level Source Code.png](../.images/Triton%20Top%20Level%20Source%20Code.png)

python目录内分为两个部分，一部分是triton，一部分是src

- triton中是上面看到的所有module的python代码
- src部分是外层include的源代码，主要就是编译器部分了

![Triton python source code.png](../.images/Triton%20python%20source%20code.png)

其实核心代码就在include、python/triton、python/src中

## Triton 前端

Triton前端代码将用户用Python编写的kernel转换为对应的Triton IR (Triton Dialect)。

### JIT decortor

这一步的入口函数，就是jit decorator。

```python
# tirton/runtime/jit.py
def jit(
    fn: Optional[T] = None,
    *,
    version=None,
    repr: Optional[Callable] = None,
    launch_metadata: Optional[Callable] = None,
    do_not_specialize: Optional[Iterable[int]] = None,
    debug: Optional[bool] = None,
    noinline: Optional[bool] = None,
) -> Union[JITFunction[T], Callable[[T], JITFunction[T]]]:
    """
    Decorator for JIT-compiling a function using the Triton compiler.

    :note: When a jit'd function is called, arguments are
        implicitly converted to pointers if they have a :code:`.data_ptr()` method
        and a `.dtype` attribute.

    :note: This function will be compiled and run on the GPU. It will only have access to:

           * python primitives,
           * builtins within the triton package,
           * arguments to this function,
           * other jit'd functions

    :param fn: the function to be jit-compiled
    :type fn: Callable
    """

    def decorator(fn: T) -> JITFunction[T]:
        assert callable(fn)
        if os.getenv("TRITON_INTERPRET", "0") == "1":
            from .interpreter import InterpretedFunction
            return InterpretedFunction(fn)
        else:
            return JITFunction(
                fn,
                version=version,
                do_not_specialize=do_not_specialize,
                debug=debug,
                noinline=noinline,
                repr=repr,
                launch_metadata=launch_metadata,
            )

    if fn is not None:
        return decorator(fn)

    else:
        return decorator
```

也就是说，我们在host代码中launch的kernel函数，实际上就是调用了JITFunction类的方法。

以vectorAdd为例子，triton中使用了和cuda类似的launch kernel方式

```python
add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
```

即，`add_kernel[grid]`调用了JITFunction的`__getitem__`方法。

因为JITFunction继承自KernelInterface类，并且JITFunction类内部没有重写`__getitem__`方法

```python
class JITFunction(KernelInterface[T]):
    ...
```

所以，`add_kernel[grid]`实际上调用了KernelInterface类的`__getitem__`方法

```python
class KernelInterface(Generic[T]):
    run: T

    def __getitem__(self, grid) -> T:
        """
        A JIT function is launched with: fn[grid](*args, **kwargs).
        Hence JITFunction.__getitem__ returns a callable proxy that
        memorizes the grid.
        """
        return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
        # return cast(T, functools.partial(cast(Callable, self.run), grid=grid))
```

返回了一个lambda函数，所以`(x, y, output, n_elements, BLOCK_SIZE=1024)`是lambda函数的调用

这里的使用方式，是子类使用父类方法，所以self是子类对象，即是子类JITFunction的run方法的调用，参数是`(x, y, output, n_elements, BLOCK_SIZE=1024)`

run方法里面最核心的三步操作：

- 获取AST：`src = self.ASTSource(self, signature, constants, configs[0])`
- 编译：`kernel = self.compile(src, target=target,options=options.__dict__,)`
- 执行：`kernel.run(grid_0, grid_1, grid_2, stream, ...)`

下面逐行分析下run方法

```python
class JITFunction(KernelInterface[T]):

    def run(self, *args, grid, warmup, **kwargs):
        # parse options
        # 获取当前设备
        device = driver.active.get_current_device()
        # 获取当前设备的stream
        stream = driver.active.get_current_stream(device)
        kwargs["debug"] = self.debug

        # Execute pre run hooks with args and kwargs
        # 执行hook，用户可以通过add_kernel.add_pre_run_hook(my_hook)方式添加pre_run_hook
        for hook in self.pre_run_hooks:
            hook(*args, **kwargs)
        
        # todo：不清楚binder的作用
        if self.binder is None:
            self.create_binder()

        bound_args, sig_and_spec, constexpr_vals, non_constexpr_vals, excess_kwargs = self.binder(*args, **kwargs)

        # compute cache key
        # 类似："*fp16*fp16*fp16*fp16i32i32DDDDDD((32, 128), {'num_ctas': 1, 'debug': None})"的string作为key
        key = ''.join(sig_and_spec) + str((constexpr_vals, excess_kwargs))
        # self.cache用于存储编译后的kernel，如果是第一次执行，cache就是一个空的defaultdict
        kernel = self.cache[device].get(key, None)

        # 第一次执行，没有kernel cache，因此要进入编译流程
        if kernel is None:
            # Kernel is not cached; we have to compile.
            # 和llvm中的target类似，就是平台相关的信息，Triton中是GPUTarget类对应了hip和cuda，同时还存储了capability和warp_size
            # GPUTarget(backend='cuda', arch=80, warp_size=32)
            target = driver.active.get_current_target()
            # <nvi.CUDABackend object at 0x7fc52dcc66e0>
            backend = self.make_backend(target)
            # {'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128, 'num_ctas': 1, 'debug': None}
            options = backend.parse_options(kwargs)

            # deprecated arguments
            assert "device_type" not in kwargs, "device_type option is deprecated; current target will be used"
            assert "device" not in kwargs, "device option is deprecated; current device will be used"
            assert "stream" not in kwargs, "stream option is deprecated; current stream will be used"
            # {'num_ctas': 1, 'debug': None}
            for k in excess_kwargs:
                if k not in options.__dict__:
                    raise KeyError("Keyword argument %s was specified but unrecognised" % k)

            # bound_args是kernel的所有输入参数，包括常量参数
            """
            {'DW': tensor([[-0.4097, -0.1726, -0.0188,  ..., -0.0649,  0.0228,  0.1083],
                [ 0.3774,  0.0352, -0.2380,  ..., -1.1426, -0.4902,  0.0113],
                [ 0.3823, -0.3728,  0.5493,  ..., -0.2502,  0.0594, -0.2191],
                ...,
                [-0.5967, -0.4812, -0.4919,  ...,  0.1768, -0.1425,  0.1530],
                [ 0.3452, -0.4663,  0.8062,  ...,  0.2399, -0.5469, -0.6846],
                [ 0.3804,  0.1188,  0.1857,  ..., -0.7290,  0.3096, -0.3596]],
                device='cuda:0', dtype=torch.float16), 'DB': tensor([[ 0.4019,  0.1949,  0.3682,  ...,  0.6538,  0.4087,  0.7461],
                [-0.0181,  0.4734, -0.1302,  ..., -0.0966,  0.4004, -0.0760],
                [ 0.4890, -0.2385, -0.3589,  ..., -0.5532,  0.2712,  0.1644],
                ...,
                [ 0.3857,  0.0612, -0.2368,  ..., -0.0600, -0.3054,  0.1996],
                [ 0.6079,  0.2634, -0.2778,  ...,  0.3577,  0.2805,  0.1104],
                [-0.0302, -0.3418, -0.0416,  ...,  0.6602, -0.4807,  0.1779]],
                device='cuda:0', dtype=torch.float16), 'FINAL_DW': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', dtype=torch.float16), 'FINAL_DB': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', dtype=torch.float16), 'M': 96, 'N': 8192, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}
            """
            bound_vals = tuple(bound_args.values())

            # `None` is nullptr. Implicitly convert to *i8. This needs to be
            # done here rather than when we build the signature as otherwise
            # the kernel cache key could not distinguish between byte pointers
            # and None arguments, resulting in a downstream mismatch:
            # 非常量的参数名
            sigkeys = [self.params[i].name for i in self.non_constexpr_indices]
            # 非常量的参数的类型
            sigvals = sig_and_spec[:len(sigkeys)]
            # 拼接成 sigkeys:sigvals 的map
            signature = {k: ('*i8' if (v == 'none') else v) for (k, v) in zip(sigkeys, sigvals)}

            configs = (self._get_config(*bound_vals), )
            constants = {
                p.name: v
                for (v, p) in zip(bound_vals, self.params)
                if p.is_constexpr or p.num in configs[0].equal_to_1 or v is None
            }
            for i, arg in constants.items():
                if callable(arg):
                    raise TypeError(f"Callable constexpr at index {i} is not supported")

            if self._call_hook(key, signature, device, constants, options, configs):
                return None
            # compile the kernel
            # 转换为AST <triton.compiler.compiler.ASTSource object at 0x7fc52dcc50f0>
            src = self.ASTSource(self, signature, constants, configs[0])
            # 转换为CompiledKernel对象 <triton.compiler.compiler.CompiledKernel object at 0x7fc52dcea650>
            kernel = self.compile(
                src,
                target=target,
                options=options.__dict__,
            )
            # 将编译完的kernel存入cache中，方便之后再次执行
            self.cache[device][key] = kernel

        # Check that used global values have not changed.
        not_present = object()
        for (name, globals_dict_id), (val, globals_dict) in self.used_global_vals.items():
            if (newVal := globals_dict.get(name, not_present)) != val:
                raise RuntimeError(
                    f"Global variable {name} has changed since we compiled this kernel, from {val} to {newVal}")

        if not warmup:
            # canonicalize grid
            assert grid is not None
            if callable(grid):
                # Arguments are passed as a dict to `grid`, by contract.
                # TODO(jlebar): In the new launch API, pass the compiler flags as a
                # second parameter to `grid`.
                grid = grid(bound_args)
            grid_size = len(grid)
            grid_0 = grid[0]
            grid_1 = grid[1] if grid_size > 1 else 1
            grid_2 = grid[2] if grid_size > 2 else 1

            # launch kernel
            # todo：先不管metadata是什么
            launch_metadata = kernel.launch_metadata(grid, stream, *non_constexpr_vals)
            kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,
                       self.CompiledKernel.launch_enter_hook, self.CompiledKernel.launch_exit_hook, *non_constexpr_vals)
        return kernel
```

涉及到前端的部分，有两处：

- 生成AST：`src = self.ASTSource(self, signature, constants, configs[0])`
- 编译Kernel中的一部分：`kernel = self.compile(src,target=target,options=options.__dict__,)`

### 生成AST

```python
src = self.ASTSource(self, signature, constants, configs[0])
```

代码的返回值是 `<triton.compiler.compiler.ASTSource object at 0x7fc52dcc50f0>`，就是下面的ASTSource的对象，这里并没有做真正的AST生成，支持提供了一个用于AST生成的接口，真正的AST生成在ast_to_ttir中执行

```python
class ASTSource:

    def __init__(self, fn, signature, constants=None, attrs=None) -> None:
        self.fn = fn
        self.ext = "ttir"
        self.name = fn.__name__
        self.signature = signature
        self.constants = constants
        self.attrs = attrs
        if isinstance(self.signature, str):
            self.signature = {k: v.strip() for k, v in enumerate(self.signature.split(","))}
        if self.constants is None:
            self.constants = dict()
        if self.attrs is None:
            self.attrs = AttrsDescriptor()

    def hash(self):
        sorted_sig = [v for k, v in sorted(self.signature.items())]
        # Note - we stringify the keys here to allow sorting to work for cases
        # where constants have mixed int/str keys.
        sorted_constants = sorted((str(k), v) for k, v in self.constants.items())
        key = f"{self.fn.cache_key}-{self.attrs.hash()}-{sorted_sig}-{sorted_constants}"
        return hashlib.sha256(key.encode("utf-8")).hexdigest()

    def make_ir(self, options, codegen_fns, context):
        return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)

    def parse_options(self):
        return dict()
```

### 生成Triton IR

`self.compile(...)`实际上调用的是Triton中comiler module下的compile函数

- stages：`{'ttir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6710>, 'ttgir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6d40>, 'llir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6dd0>, 'ptx': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6e60>, 'cubin': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6ef0>}`

先看一下简化的compiler函数流程

```python
def compile(src, target=None, options=None):
    ...
    # 根据指定的target获取一个backend，这里会返回CUDABackend
    backend = make_backend(target) 
    ...
    # 添加编译stage
    backend.add_stages(stages, options) 
    ...
    # 加载所需dialects
    ir.load_dialects(context)  
    backend.load_dialects(context)
    # 创建Triton IR
    module = src.make_ir(options, context) 
    ...
    for ext, compile_ir in list(stages.items())[first_stage:]:
        # 编译、优化各个阶段IR
        next_module = compile_ir(module, metadata) 
        ...
        module = next_module
    ...
    return CompiledKernel(src, metadata_group, hash)
```

下面看下完整的代码

```python
# triton/compiler/compiler.py:226
def compile(src, target=None, options=None):
    if target is None:
        target = driver.active.get_current_target()
    assert isinstance(target, GPUTarget), "target must be of GPUTarget type"
    backend = make_backend(target)
    # 第一次编译的话，src是ASTSource，ir_source是false
    ir_source = not isinstance(src, ASTSource)
    # create backend
    if ir_source:
        assert isinstance(src, str), "source must be either AST or a filepath"
        src = IRSource(src)
    extra_options = src.parse_options()
    options = backend.parse_options(dict(options or dict(), **extra_options))
    # create cache manager
    env_vars = get_cache_invalidating_env_vars()
    key = f"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}"
    hash = hashlib.sha256(key.encode("utf-8")).hexdigest()
    fn_cache_manager = get_cache_manager(hash)
    # For dumping/overriding only hash the source as we want it to be independent of triton
    # core changes to make it easier to track kernels by hash.
    enable_override = os.environ.get("TRITON_KERNEL_OVERRIDE", "0") == "1"
    enable_ir_dump = os.environ.get("TRITON_KERNEL_DUMP", "0") == "1"
    fn_override_manager = get_override_manager(src.hash()) if enable_override else None
    fn_dump_manager = get_dump_manager(src.hash()) if enable_ir_dump else None
    metadata_filename = f"{src.name}.json"
    metadata_group = fn_cache_manager.get_group(metadata_filename) or {}
    metadata_path = metadata_group.get(metadata_filename)
    always_compile = os.environ.get("TRITON_ALWAYS_COMPILE", "0") == "1"
    if not always_compile and metadata_path is not None:
        # cache hit!
        metadata = json.loads(Path(metadata_path).read_text())
        return CompiledKernel(src, metadata_group, hash)
    # initialize metadata
    metadata = {
        "hash": hash,
        "target": target,
        **options.__dict__,
        **env_vars,
    }
    # run compilation pipeline  and populate metadata
    stages = dict()
    backend.add_stages(stages, options)
    # src.ext = "ttir"
    first_stage = list(stages.keys()).index(src.ext)
    # when the source is an IR file, don't apply the passes related to this stage. This makes it easier to write IR level tests.
    if ir_source:
        first_stage += 1
    # ir来自于triton._C模块，已经不是python代码了，<module 'triton._C.libtriton.ir'>
    # context: <triton._C.libtriton.ir.context object at 0x7f28fd677b70>
    context = ir.context()
    # 加载dialect
    ir.load_dialects(context)
    backend.load_dialects(context)
    codegen_fns = backend.get_codegen_implementation()
    try:
        # 调用ASTSource的make_ir方法获取到 <triton._C.libtriton.ir.module object at 0x7f28b5fe0ef0>
        module = src.make_ir(options, codegen_fns, context)
    except Exception as e:
        filter_traceback(e)
        raise
    use_ttgir_loc = os.environ.get("USE_TTGIR_LOC", "0") == "1"
    # 按照stage从AST开始一步步生成，同时进行优化，每一步生成的结果会单独存储到metadata_group[ir_filename]中
    for ext, compile_ir in list(stages.items())[first_stage:]:
        next_module = compile_ir(module, metadata)
        ir_filename = f"{src.name}.{ext}"
        metadata_group[ir_filename] = fn_cache_manager.put(next_module, ir_filename)
        if fn_dump_manager is not None:
            fn_dump_manager.put(next_module, ir_filename)
        if (fn_override_manager is not None and fn_override_manager.has_file(ir_filename)):
            print(f"\nOverriding kernel with file {ir_filename}")
            full_name = fn_override_manager.get_file(ir_filename)
            next_module = parse(full_name, ext, context)
        # use an env variable to parse ttgir from file
        if use_ttgir_loc and ext == "ttgir":
            ttgir_full_name = fn_cache_manager.get_file(ir_filename)
            next_module.create_location_snapshot(ttgir_full_name)
            print(f"Create new locations for {ttgir_full_name}")
        module = next_module
    # write-back metadata
    metadata_group[metadata_filename] = fn_cache_manager.put(json.dumps(metadata, default=vars), metadata_filename,binary=False)
    fn_cache_manager.put_group(metadata_filename, metadata_group)
    # return handle to compiled kernel
    return CompiledKernel(src, metadata_group, hash)
```

从上到下疏离关键逻辑，首先是dialect加载，是由ir的方法来负责的

在本文已开始，我们看过了Triton的项目结构，c++相关的定义都在src中，来到src/main.cc可以知道ir这个module的定义

```python
# python/src/main.cc
PYBIND11_MODULE(libtriton, m) {
  m.doc() = "Python bindings to the C++ Triton API";
  init_triton_env_vars(m);
  init_triton_ir(m.def_submodule("ir"));
  init_triton_passes(m.def_submodule("passes"));
  init_triton_interpreter(m.def_submodule("interpreter"));
  init_triton_llvm(m.def_submodule("llvm"));
  FOR_EACH_P(INIT_BACKEND, TRITON_BACKENDS_TUPLE)
}
```

```c++
  m.def("load_dialects", [](MLIRContext &context) {
    DialectRegistry registry;
    registry.insert<TritonDialect, ::mlir::triton::gpu::TritonGPUDialect,
                    math::MathDialect, arith::ArithDialect, index::IndexDialect,
                    scf::SCFDialect, ::mlir::gpu::GPUDialect,
                    cf::ControlFlowDialect, LLVM::LLVMDialect>();
    registerBuiltinDialectTranslation(registry);
    registerLLVMDialectTranslation(registry);
    context.appendDialectRegistry(registry);
    context.loadAllAvailableDialects();
  });
```

对于TritonIR生成，需要的是TritonDialect，它是由TableGen根据td文件生成的类型。这里先不看TritonDialect具体定义，下面的Triton Dialect章节中专门分析。

继续向后走，真正的代码生成存在于`module = src.make_ir(options, codegen_fns, context)`和之后的for loop内

先看第一步的make_ir究竟做了什么

回到ASTSource

```python
class ASTSource:
    def make_ir(self, options, codegen_fns, context):
        return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)
```

调用了code_generator中的ast_to_ttir函数

```python
# python/triton/compiler/code_generator.py
def ast_to_ttir(fn, specialization, context, options, codegen_fns):
    # fn还是JITFunction
    attrs = specialization.attrs
    # create kernel prototype
    cst_key = lambda i: fn.arg_names.index(i) if isinstance(i, str) else i
    constants = {cst_key(key): value for key, value in specialization.constants.items()}
    # visit kernel AST
    gscope = fn.__globals__.copy()
    function_name = fn.repr(specialization)
    tys = list(specialization.signature.values())
    new_constants = {k: True if k in tys and tys[k] == "i1" else 1 for k in attrs.equal_to_1}
    new_attrs = {k: [("tt.divisibility", 16)] for k in attrs.divisible_by_16}

    all_constants = constants.copy()
    all_constants.update(new_constants)
    arg_types = [str_to_ty(v) for k, v in specialization.signature.items() if k not in specialization.constants]
    file_name, begin_line = _get_fn_file_line(fn)

    prototype = language.function_type([], arg_types)
    generator = CodeGenerator(context, prototype, gscope=gscope, constants=all_constants, function_name=function_name,
                              jit_fn=fn, attributes=new_attrs, is_kernel=True, file_name=file_name,
                              begin_line=begin_line, options=options, codegen_fns=codegen_fns)
    # fn.parse(): <ast.Module object at 0x7f78ef4ad2a0>
    # 输入是AST
    generator.visit(fn.parse())
    # 输出已经是ir module了：<triton._C.libtriton.ir.module object at 0x7f78ef477e20>
    ret = generator.module
    # module takes ownership of the context
    ret.context = context
    return ret
```

也就是说，这里做了两步操作，一个是`fn.parse`获取Python AST，然后是`generator.visit`生成TritonIR

回到JITFunction，可以看到，实际上就是调用的python官方的ast方法获取的AST，self.src则是源码的字符串

```python
import ast
class JITFunction(KernelInterface[T]):
    def parse(self):
        tree = ast.parse(self.src)
        assert isinstance(tree, ast.Module)
        assert len(tree.body) == 1
        assert isinstance(tree.body[0], ast.FunctionDef)
        return tree
```

self.src示例：

```shell
(Pdb) p self.src
"def add_kernel(\n    x_ptr,  # *Pointer* to first input vector.\n    y_ptr,  # *Pointer* to second input vector.\n    output_ptr,  # *Pointer* to output vector.\n    n_elements,  # Size of the vector.\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n    # NOTE: `constexpr` so it can be used as a shape value.\n):\n    # There are multiple 'programs' processing different data. We identify which program\n    # we are here:\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n    # This program will process inputs that are offset from the initial data.\n    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n    # Note that offsets is a list of pointers:\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    # Create a mask to guard memory operations against out-of-bounds accesses.\n    mask = offsets < n_elements\n    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n    # multiple of the block size.\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    # Write x + y back to DRAM.\n    tl.store(output_ptr + offsets, output, mask=mask)\n"
```

返回的tree则是`<ast.Module object at 0x7f061493f9a0>`，这是一个树。

这里没有直接遍历它，而是查看了下它的root node是不是ast.FunctionDef（之后生成tirton ir的时候才回去遍历它），那就在自己写一个方法来遍历下这个树，把它print出来看看它长什么样子

```python
def print_ast(node, level=0):
    # 打印当前节点的类型和一些信息
    print('  ' * level + type(node).__name__)
    
    # 遍历所有属性，如果属性值是AST节点，递归打印
    for field, value in ast.iter_fields(node):
        if isinstance(value, list):
            for item in value:
                if isinstance(item, ast.AST):
                    print_ast(item, level + 1)
        elif isinstance(value, ast.AST):
            print_ast(value, level + 1)
```

获得输出如下，拿这个AST和add_kernel函数对比是能完全对应的，毕竟只是翻译。具体怎么翻译过来的先不深究。

```python
Module
  FunctionDef
    arguments
      arg
      arg
      arg
      arg
      arg
        Attribute
          Name
            Load
          Load
    Assign
      Name
        Store
      Call
        Attribute
          Name
            Load
          Load
        keyword
          Constant
    Assign
      Name
        Store
      BinOp
        Name
          Load
        Mult
        Name
          Load
    Assign
      Name
        Store
      BinOp
        Name
          Load
        Add
        Call
          Attribute
            Name
              Load
            Load
          Constant
          Name
            Load
    Assign
      Name
        Store
      Compare
        Name
          Load
        Lt
        Name
          Load
    Assign
      Name
        Store
      Call
        Attribute
          Name
            Load
          Load
        BinOp
          Name
            Load
          Add
          Name
            Load
        keyword
          Name
            Load
    Assign
      Name
        Store
      Call
        Attribute
          Name
            Load
          Load
        BinOp
          Name
            Load
          Add
          Name
            Load
        keyword
          Name
            Load
    Assign
      Name
        Store
      BinOp
        Name
          Load
        Add
        Name
          Load
    Expr
      Call
        Attribute
          Name
            Load
          Load
        BinOp
          Name
            Load
          Add
          Name
            Load
        Name
          Load
        keyword
          Name
            Load
```

完成`fn.parse()`获取到AST之后，进入visit方法`generator.visit(fn.parse())`

```python
class CodeGenerator(ast.NodeVisitor):
    def visit(self, node):
        if node is None:
            return
        with warnings.catch_warnings():
            # The ast library added visit_Constant and deprecated some other
            # methods but we can't move to that without breaking Python 3.6 and 3.7.
            warnings.simplefilter("ignore", DeprecationWarning)  # python 3.9
            warnings.simplefilter("ignore", PendingDeprecationWarning)  # python 3.8

            # 下面两行为了保留现场
            # cur_node初始化为None
            last_node = self.cur_node
            # self.builder也来自于triton._C
            last_loc = self.builder.get_loc()
            # node为AST的root node，也就是FunctionDef
            self.cur_node = node
            if hasattr(node, 'lineno') and hasattr(node, 'col_offset'):
                self.builder.set_loc(self.file_name, self.begin_line + node.lineno, node.col_offset)
                last_loc = self.builder.get_loc()
            try:
                # 子类调用父类的visit方法
                ret = super().visit(node)
            except CompilationError:
                raise
            except Exception as e:
                # Wrap the error in a CompilationError which contains the source
                # of the @jit function.
                raise CompilationError(self.jit_fn.src, self.cur_node, repr(e)) from None

            # Reset the location to the last one before the visit
            if last_loc:
                self.cur_node = last_node
                self.builder.set_loc(last_loc)
            return ret
```

进一步深入到父类`ast.NodeVisitor`的visit方法，它还是python内置的ast module。

```python
import ast
class CodeGenerator(ast.NodeVisitor):
    ...
```

如果深入进去看的话，实际上就是上面实现的深度优先遍历 + 子类的visit方法，只是没有print，不知道它长什么样子。

```python
class NodeVisitor(object):
    def visit(self, node):
        """Visit a node."""
        # 这里的self是子类
        method = 'visit_' + node.__class__.__name__
        # 获取子类中的当前节点visit的方法，否则就是generic_visit
        visitor = getattr(self, method, self.generic_visit)
        return visitor(node)

    def generic_visit(self, node):
        """Called if no explicit visitor function exists for a node."""
        for field, value in iter_fields(node):
            if isinstance(value, list):
                for item in value:
                    if isinstance(item, AST):
                        self.visit(item)
            elif isinstance(value, AST):
                self.visit(value)
```

为了能够完成转换，这里最关键的一步是`visitor = getattr(self, method, self.generic_visit)`，vistor实际上是子类CodeGenerator中的`'visit_' + node.__class__.__name__`组合起来的方法。

例如：

```python
class CodeGenerator(ast.NodeVisitor):
    def visit_Module:
        ...
    def visit_FunctionDef:
        ...
    def visit_Load:
        ...
    ...
```

以visit_FunctionDef为例子，看下它里面做了什么，重点是怎么从AST称为Triton IR的

```python
class CodeGenerator(ast.NodeVisitor):
    def visit_FunctionDef(self, node):
        # visit我们实现的kernel函数的入参
        # arg_names: ['x_ptr', 'y_ptr', 'output_ptr', 'n_elements', 'BLOCK_SIZE']
        # kwarg_names: None
        arg_names, kwarg_names = self.visit(node.args)
        if self.fn:
            raise self._unsupported(node, "nested function definition is not supported.")
        # initialize defaults
        # vector add这个例子里没有default args
        for i, default_value in enumerate(node.args.defaults):
            arg_node = node.args.args[-i - 1]
            annotation = arg_node.annotation
            name = arg_node.arg
            st_target = ast.Name(id=name, ctx=ast.Store())
            if annotation is None:
                init_node = ast.Assign(targets=[st_target], value=default_value)
            else:
                init_node = ast.AnnAssign(target=st_target, value=default_value, annotation=annotation)

            try:
                assert not self.visiting_arg_default_value
                self.visiting_arg_default_value = True
                self.visit(init_node)
            finally:
                self.visiting_arg_default_value = False

        # initialize function
        visibility = "public" if self.is_kernel else "private"
        # 通过builder获取到self.fn：<triton._C.libtriton.ir.function object at 0x7f6835d001f0>
        # self.module: <triton._C.libtriton.ir.module object at 0x7f682cc0aca0>
        self.fn = self.builder.get_or_insert_function(self.module, self.function_name,
                                                      self.prototype.to_ir(self.builder), visibility, self.noinline)
        self.module.push_back(self.fn)
        entry = self.fn.add_entry_block()
        arg_values = []
        idx = 0
        for i, arg_name in enumerate(arg_names):
            if i in self.constants:
                cst = self.constants[i]
                if not _is_constexpr(cst):
                    cst = constexpr(self.constants[i])
                arg_values.append(cst)
                continue
            else:
                if i in self.attributes:
                    for name, value in self.attributes[i]:
                        self.fn.set_arg_attr(idx, name, value)
                arg_values.append(tensor(self.fn.args(idx), self.prototype.param_types[idx]))
                idx += 1

        insert_pt = self.builder.get_insertion_block()
        for arg_name, arg_value in zip(arg_names, arg_values):
            self.set_value(arg_name, arg_value)
        self.builder.set_insertion_point_to_start(entry)
        # visit function body
        self.visit_compound_statement(node.body)
        # finalize function
        if self.ret_type is None or self.ret_type == language.void:
            self.ret_type = language.void
            self.builder.ret([])
        else:
            # update return type
            if isinstance(self.ret_type, tuple):
                self.prototype.ret_types = list(self.ret_type)
                self.fn.reset_type(self.prototype.to_ir(self.builder))
            else:
                self.prototype.ret_types = [self.ret_type]
                self.fn.reset_type(self.prototype.to_ir(self.builder))
        if insert_pt:
            self.builder.set_insertion_point_to_end(insert_pt)
        # Remove dead code
        self.fn.finalize()
```

实际的转换动作由builder对象负责，它在code_generator的`__init__`中被初始化。它是来自_C.libtriton中的module。

```python
from .._C.libtriton import ir

class CodeGenerator(ast.NodeVisitor):

    def __init__(self, context, prototype, gscope, attributes, constants, function_name, jit_fn: JITFunction, options,
                 codegen_fns, debug=None, module=None, is_kernel=False, function_types: Optional[Dict] = None,
                 noinline=False, file_name: Optional[str] = None, begin_line=0):
        self.context = context
        self.builder = ir.builder(context)
        ...
```

继续深入到`init_triton_ir`中找builder的binding，这个class binding足足有500+行，TritonOpBuilder就是它实际的c++类了。

```python
# python/src/ir.cc
py::class_<TritonOpBuilder>(m, "builder", py::module_local(),
                              py::dynamic_attr())
      .def(py::init<MLIRContext *>())
      // getters
      .def("create_module",
           [](TritonOpBuilder &self) -> ModuleOp {
             return self.create<ModuleOp>();
           })
      // insertion block/point
      .def("set_insertion_point_to_start",
           [](TritonOpBuilder &self, Block &block) -> void {
             self.setInsertionPointToStart(block);
           })
      .def("set_insertion_point_to_end",
           [](TritonOpBuilder &self, Block &block) {
             self.setInsertionPointToEnd(block);
           }
        ...
      );
```

以前面遇到的`get_or_insert_function`为例：

```python
self.fn = self.builder.get_or_insert_function(self.module, self.function_name,
                                              self.prototype.to_ir(self.builder), visibility, self.noinline)
```

```c++
// Ops
.def("get_or_insert_function",
      [](TritonOpBuilder &self, ModuleOp &module, std::string &funcName,
        Type &funcType, std::string &visibility,
        bool noinline) -> FuncOp {
        if (Operation *funcOperation = module.lookupSymbol(funcName))
          # 如果当前module中已经创建过这个function，则直接返回它
          return llvm::dyn_cast<FuncOp>(funcOperation);
        if (auto funcTy = dyn_cast<FunctionType>(funcType)) {
          # 如果没有，则按照它的函数名、函数定义、函数属性创建一个FuncOp出来
          llvm::SmallVector<NamedAttribute> attrs = {
              NamedAttribute(
                  self.getBuilder().getStringAttr("sym_visibility"),
                  self.getBuilder().getStringAttr(visibility)),
              NamedAttribute(self.getBuilder().getStringAttr("noinline"),
                            self.getBuilder().getBoolAttr(noinline))};
          return self.create<FuncOp>(funcName, funcTy, attrs);
        }
        throw std::invalid_argument("invalid function type");
      })
```

可以看到，实际执行TritonIR的类型创建的，是又TritonOpBuilder中的builder负责的

```c++
class TritonOpBuilder {
// ...
private:
  std::unique_ptr<OpBuilder> builder;
  std::unique_ptr<Location> lastLoc;
// ...
}
```

OpBuilder，[mlir::OpBuilder](https://mlir.llvm.org/doxygen/classmlir_1_1OpBuilder.html)，MLIR提供的标准接口用于创建OP。

create出来的FuncOp类型，它是Triton Dialect，它的实现是由TableGen来生成`build/XXXX/include/triton/Dialect/Triton/IR/Ops.h.inc`

```c++
class FuncOp : public ::mlir::Op<FuncOp, ::mlir::OpTrait::OneRegion, ::mlir::OpTrait::ZeroResults, ::mlir::OpTrait::ZeroSuccessors, ::mlir::OpTrait::ZeroOperands, ::mlir::OpTrait::OpInvariants, ::mlir::BytecodeOpInterface::Trait, ::mlir::OpTrait::AffineScope, ::mlir::OpTrait::AutomaticAllocationScope, ::mlir::CallableOpInterface::Trait, ::mlir::SymbolOpInterface::Trait, ::mlir::FunctionOpInterface::Trait, ::mlir::OpTrait::IsIsolatedFromAbove, ::mlir::OpAsmOpInterface::Trait> {
// ...
  static constexpr ::llvm::StringLiteral getOperationName() {
    return ::llvm::StringLiteral("tt.func");  // op名被定义为tt.func，我们在打印出来的tritonir中看到的就是这个名字
  }
}
```

它也进行了python binding

```c++
  py::class_<FuncOp, OpState>(m, "function", py::module_local())
      // .def_property_readonly("attrs", &ir::function::attrs)
      // .def("add_attr", &ir::function::add_attr);
      .def("args",
           [](FuncOp &self, unsigned idx) -> BlockArgument {
             if (idx >= self.getNumArguments())
               throw pybind11::index_error(
                   "Function argument index out of range");
             return self.getArgument(idx);
           })
      .def(
          "add_entry_block",
          [](FuncOp &self) -> Block * { return self.addEntryBlock(); },
          ret::reference)
      .def(
          "set_arg_attr",
          [](FuncOp &self, int arg_no, const std::string &name, int val) {
            // set arg attributes "name" to value "val"
            auto attrTy = IntegerType::get(self.getContext(), 32);
            self.setArgAttr(arg_no, name, IntegerAttr::get(attrTy, val));
          },
          ret::reference)
      //  .def("has_attr", &::FuncOp::hasAttr)
      .def("finalize",
           [](FuncOp &self) -> void {
             // Remove dead code
             // 1. Unreachable code after return
             self.walk([&](Block *block) {
               Operation *retOp = nullptr;
               // It's better to not use walk here because we only want to
               // check operations in the current block
               for (auto &op : block->getOperations()) {
                 if (isa<ReturnOp>(op))
                   if (retOp == nullptr) {
                     retOp = &op;
                     break;
                   }
               }
               if (retOp && retOp != &block->back()) {
                 auto pos = retOp->getIterator();
                 pos++;
                 auto *newBlock = block->splitBlock(pos);
                 newBlock->erase();
               }
             });
             // 2. Check if the result of tl.advance is used
             self.walk([&](Operation *op) {
               if (isa<AdvanceOp>(op) && op->getResult(0).use_empty())
                 outputWarning(op->getLoc(), "The result of tl.advance is not "
                                             "being used. Note that tl.advance "
                                             "does not have any side effects. "
                                             "To move the block pointer, you "
                                             "need to assign the result of "
                                             "tl.advance to a variable.");
             });
           })
      .def_property_readonly("type", &FuncOp::getFunctionType)
      .def("reset_type", &FuncOp::setType);
```

之后，fn再加入到module中

```python
self.module.push_back(self.fn)
```

对应到c++世界中如下：

```c++
py::class_<ModuleOp, OpState>(m, "module", py::module_local(),
                              py::dynamic_attr())
    // ...
    .def("push_back",
          [](ModuleOp &self, FuncOp &funcOp) -> void {
            self.push_back(funcOp);
          })
    // ...
```

在来看一下对于具体的计算Op，是怎么生成TritonIR的，这里以BinOp二元运算为例子：

```python
def visit_BinOp(self, node):
    # 获取左右操作数
    lhs = self.visit(node.left)
    rhs = self.visit(node.right)
    # 对于vector add，获取到的method_name是add
    method_name = self._method_name_for_bin_op.get(type(node.op))
    if method_name is None:
        raise self._unsupported(node,
                                "AST binary operator '{}' is not (currently) implemented.".format(node.op.__name__))
    return self._apply_binary_method(method_name, lhs, rhs)
```

调用`_apply_binary_method`

```python
def _apply_binary_method(self, method_name, lhs, rhs):
    # TODO: raise something meaningful if getattr fails below, esp for reverse method
    # 对于做操作数是triton_tensor类型的，要调用triton_tensor的add方法
    if _is_triton_tensor(lhs):
        return getattr(lhs, method_name)(rhs, _builder=self.builder)
    if _is_triton_tensor(rhs):
        reverse_method_name = re.sub(r"__(.*)__", r"__r\1__", method_name)
        return getattr(rhs, reverse_method_name)(lhs, _builder=self.builder)
    return getattr(lhs, method_name)(rhs)
```

triton tensor类型被定义在了`python/triton/language/core.py`文件中

```python
from . import semantic
class tensor:
  """Represents an N-dimensional array of values or pointers.

    :code:`tensor` is the fundamental data structure in Triton programs.  Most
    functions in :py:mod:`triton.language` operate on and return tensors.
  """
  ...
  @builtin
  def __add__(self, other, _builder=None):
      other = _to_tensor(other, _builder)
      return semantic.add(self, other, _builder)
  ...
```

进入到semantic.p中的add函数

```python
def add(input: tl.tensor, other: tl.tensor, builder: ir.builder) -> tl.tensor:
    input, other = binary_op_type_checking_impl(input, other, builder, True, True)
    input_scalar_ty = input.type.scalar
    other_scalar_ty = other.type.scalar
    if input_scalar_ty.is_ptr() and other_scalar_ty.is_ptr():
        raise TypeError("cannot add pointers together")

    # offset + ptr
    # ptr + offset
    if other_scalar_ty.is_ptr() and not input_scalar_ty.is_ptr():
        input, other = other, input
        input_scalar_ty = input.type.scalar
        other_scalar_ty = other.type.scalar
    if input_scalar_ty.is_ptr():
        return tl.tensor(builder.create_addptr(input.handle, other.handle), input.type)
    # float + float
    elif input_scalar_ty.is_floating():
        return tl.tensor(builder.create_fadd(input.handle, other.handle), input.type)
    # int + int
    elif input_scalar_ty.is_int():
        return tl.tensor(builder.create_add(input.handle, other.handle), input.type)
    raise TypeError(f"unexpected type {input_scalar_ty}")
```

可以看到无论哪一个分支，都是进入到ir.builder当中，create_add是使用的预置的arith dialect中的AddIOp

```c++
.def("create_add",
      [](TritonOpBuilder &self, Value &lhs, Value &rhs) -> Value {
        return self.create<arith::AddIOp>(lhs, rhs);
      })
```

对于load和store函数，则binding到了triton定义的LoadOp和StoreOp

```c++
// Input/Output
.def("create_load",
      [](TritonOpBuilder &self, Value &ptrs, CacheModifier cacheModifier,
        EvictionPolicy evictionPolicy, bool isVolatile) -> Value {
        return self.create<LoadOp>(ptrs, cacheModifier, evictionPolicy,
                                  isVolatile);
      })
.def("create_store",
      [](TritonOpBuilder &self, Value &ptrs, Value &value,
        CacheModifier cacheModifier,
        EvictionPolicy evictionPolicy) -> void {
        self.create<StoreOp>(ptrs, value, cacheModifier, evictionPolicy);
      })
```

它们的实现也是由TableGen来生成的`build/XXXX/include/triton/Dialect/Triton/IR/Ops.h.inc`

```c++
namespace mlir {
namespace triton {
class LoadOp : public ::mlir::Op<...> {
  //...
}
}
}
```

经过code generator完成AST Tree的遍历后，即可获得最终的ModuleOp，它就是ast_to_ttir函数的返回值，也就是最终的TritonIR

```python
def ast_to_ttir(fn, specialization, context, options, codegen_fns):
    ...
    generator.visit(fn.parse())

    ret = generator.module
    # module takes ownership of the context
    ret.context = context
    return ret
```

通过设置环境变量TRITON_CACHE_DIR=./和TRITON_KERNEL_DUMP=1，我们可以将所有的IR保存到当前路径

vector add用例的TritonIR如下，和我们在python中写的add_kernel亦是能对应上的，比如前面看过的FuncOp的OpName "tt.func"

```c++
// add_kernel.ttir
#loc = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0)
module {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0)) attributes {noinline = false} {
    %c1024_i32 = arith.constant 1024 : i32 loc(#loc1)
    %0 = tt.get_program_id x : i32 loc(#loc2)
    %1 = arith.muli %0, %c1024_i32 : i32 loc(#loc3)
    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32> loc(#loc4)
    %3 = tt.splat %1 : i32 -> tensor<1024xi32> loc(#loc5)
    %4 = arith.addi %3, %2 : tensor<1024xi32> loc(#loc5)
    %5 = tt.splat %arg3 : i32 -> tensor<1024xi32> loc(#loc6)
    %6 = arith.cmpi slt, %4, %5 : tensor<1024xi32> loc(#loc6)
    %7 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc7)
    %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc7)
    %9 = tt.load %8, %6 : tensor<1024x!tt.ptr<f32>> loc(#loc8)
    %10 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc9)
    %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc9)
    %12 = tt.load %11, %6 : tensor<1024x!tt.ptr<f32>> loc(#loc10)
    %13 = arith.addf %9, %12 : tensor<1024xf32> loc(#loc11)
    %14 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc12)
    %15 = tt.addptr %14, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc12)
    tt.store %15, %13, %6 : tensor<1024x!tt.ptr<f32>> loc(#loc13)
    tt.return loc(#loc14)
  } loc(#loc)
} loc(#loc)
#loc1 = loc(unknown)
#loc2 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":27:24)
#loc3 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":32:24)
#loc4 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":33:41)
#loc5 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":33:28)
#loc6 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":35:21)
#loc7 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":38:24)
#loc8 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":38:16)
#loc9 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":39:24)
#loc10 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":39:16)
#loc11 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":40:17)
#loc12 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:26)
#loc13 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:35)
#loc14 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:4)
```

至此，Triton前端的使命已经完成！我们也顺利从Python世界进入到C++世界，再进入到了MLIR世界。

### Triton Dialect

此处还有一个重要问题，我们一开始说的Trition定义了两个MLIR Dialect，一个是Triton Dialect，另一个TritonGPU Dialect。

而Triton IR已经生成了，似乎除了OpBuilder，ModuleOp，FuncOp，LoadOp之外，还没有完整的看一下Triton Dialect的定义。

在Triton Doc中可以找到Triton Dialect的说明：[‘tt’ Dialect](https://triton-lang.org/main/dialects/TritonDialect.html)

代码中，Triton Dialect被实现在`python/triton/_C/include/triton/Dialect/Triton/`路径下。


| 文件                                                 | 说明                            |
| ---------------------------------------------------- | ------------------------------- |
| include/triton/Dialect/Triton/IR/TritonDialect.td    | ttir的定义 Dialect TableGen文件 |
| include/triton/Dialect/Triton/IR/TritonOps.td        | ttir内部的op相关的定义          |
| include/triton/Dialect/Triton/IR/TritonAttrDefs.td   | ops的属性信息定义               |
| include/triton/Dialect/Triton/IR/TritonTypes.td      | 内部类型定义                    |
| include/triton/Dialect/Triton/IR/TritonInterfaces.td | interfaces定义                  |


首先来看下`include/triton/Dialect/Triton/IR/TritonDialect.td`中Triton_Dialect的定义

```
def Triton_Dialect : Dialect {
  // 定义Triton Dialect的名字
  let name = "tt";
  // 定义Triton Dialect的c++命名空间
  let cppNamespace = "::mlir::triton";

  // 描述说明
  let summary = "The Triton IR in MLIR";

  let description = [{
    Triton Dialect.

    Dependent Dialects:
      * Arith:
        * addf, addi, andi, cmpf, cmpi, divf, fptosi, ...
      * Math:
        * exp, sin, cos, log, ...
      * StructuredControlFlow:
        * for, if, while, yield, condition
      * ControlFlow:
        * br, cond_br
  }];

  // 依赖的MLIR的预置Dialects
  let dependentDialects = [
    "arith::ArithDialect",     // 处理加减乘除等运算
    "math::MathDialect",       // 处理log、exp等复杂数学运算
    "scf::SCFDialect",         // 结构化控制流，保留for、if等语句
    "cf::ControlFlowDialect"   // 无结构控制流，只有条件跳转命令
  ];

  // 声明要添加的自定义函数
  let extraClassDeclaration = [{
    void registerTypes();
  }];
  
  // MLIR Dialect提供的一些属性配置
  let hasConstantMaterializer = 1;      // Triton Dialect是否支持常量的解析和生成
  let useDefaultTypePrinterParser = 1;  // Triton Dialect是否使用默认的类型打印和解析器
  let usePropertiesForAttributes = 1;   // Triton Dialect是否使用属性的属性来表示属性
}
```

根据这个td文件，TableGen可以生成出如下的类型

```c++
/*===- TableGen'erated file -------------------------------------*- C++ -*-===*\
|*                                                                            *|
|* Dialect Declarations                                                       *|
|*                                                                            *|
|* Automatically generated file, do not edit!                                 *|
|* From: TritonDialect.td                                                     *|
|*                                                                            *|
\*===----------------------------------------------------------------------===*/

namespace mlir {
namespace triton {

class TritonDialect : public ::mlir::Dialect {
  explicit TritonDialect(::mlir::MLIRContext *context);

  void initialize();
  friend class ::mlir::MLIRContext;
public:
  ~TritonDialect() override;
  static constexpr ::llvm::StringLiteral getDialectNamespace() {
    return ::llvm::StringLiteral("tt");
  }

  /// Parse a type registered to this dialect.
  ::mlir::Type parseType(::mlir::DialectAsmParser &parser) const override;

  /// Print a type registered to this dialect.
  void printType(::mlir::Type type,
                 ::mlir::DialectAsmPrinter &os) const override;

  /// Materialize a single constant operation from a given attribute value with
  /// the desired resultant type.
  ::mlir::Operation *materializeConstant(::mlir::OpBuilder &builder,
                                         ::mlir::Attribute value,
                                         ::mlir::Type type,
                                         ::mlir::Location loc) override;

    void registerTypes();
  };
} // namespace triton
} // namespace mlir
MLIR_DECLARE_EXPLICIT_TYPE_ID(::mlir::triton::TritonDialect)
```


接着是`include/triton/Dialect/Triton/IR/TritonOps.td`中定义的各种算子，算子比较多，总共有38个，还是选择一个典型的来看一下

```
//
// Op Base
//
// 所有Op的父类，继承自Op
class TT_Op<string mnemonic, list<Trait> traits = []> :
    Op<Triton_Dialect, mnemonic,
       !listconcat(traits, [TensorSizeTrait, VerifyTensorLayoutsTrait])> {
}

//
// Dot Op
//
// 继承自TT_Op
def TT_DotOp : TT_Op<"dot", [Pure,
                             DeclareOpInterfaceMethods<InferTypeOpInterface>,
                             TypesMatchWith<"result's type matches accumulator's type",
                                            "d", "c", "$_self">]> {
    // 描述说明
    let summary = "dot";

    let description = [{
        $d = matrix_multiply($a, $b) + $c. $inputPrecision describes how to exercise the TC
        when the inputs are f32. It can be one of: tf32, tf32x3, ieee.
        tf32: use TC with tf32 ops.
        tf32x3: implement the 3xTF32 trick. For more info see the pass in F32DotTC.cpp
        ieee: don't use TC, implement dot in software.
        If the GPU does not have Tensor cores or the inputs are not f32, this flag is ignored.
    }];
    
    // 参数 
    let arguments = (
      ins
      TT_TensorOrMemDesc:$a,  // 输入a
      TT_TensorOrMemDesc:$b,  // 输入b
      TT_FpIntTensor:$c,      // 输入c
      DefaultValuedAttr<TT_InputPrecisionAttr, "::mlir::triton::InputPrecision::IEEE">:$inputPrecision,
      DefaultValuedAttr<I32Attr, "0">:$maxNumImpreciseAcc
    );

    let results = (outs TT_FpIntTensor:$d);  // 输出d

    // attr-dict prints enums as integers.  To get inputPrecision printed as a
    // string, we need to specify it explicitly.
    let assemblyFormat = [{
      $a`,` $b`,` $c (`,` `inputPrecision` `=` $inputPrecision^)? attr-dict `:`
      type($a) `*` type($b) `->` type($d)
    }];
    let hasVerifier = 1;
}
```

再是属性定义`include/triton/Dialect/Triton/IR/TritonAttrDefs.td`

```c++
// Attributes for LoadOp and StoreOp
// 用于Load和Store的缓存修改器的属性
def TT_CacheModifierAttr : I32EnumAttr<
    "CacheModifier", "",
    [
        I32EnumAttrCase<"NONE", 1, "none">,  // 没有特定的缓存策略
        I32EnumAttrCase<"CA", 2, "ca">,      // 内存访问合并
        I32EnumAttrCase<"CG", 3, "cg">,      // 粗力度并行激素啊
        I32EnumAttrCase<"WB", 4, "wb">,      // 写回
        I32EnumAttrCase<"CS", 5, "cs">,      // 计算着色器
        I32EnumAttrCase<"WT", 6, "wt">,      // 写透缓存
    ]> {
    let cppNamespace = "::mlir::triton";
}
```

Types中则是定义的用于Triton IR中的类型`include/triton/Dialect/Triton/IR/TritonTypes.td`，例如浮点、布尔、整数、张量等

```c++
//
// Types
//
class TritonTypeDef<string name, string _mnemonic, list<Trait> traits = []>
    : TypeDef<Triton_Dialect, name, traits> {
    // Used by printer/parser
    let mnemonic = _mnemonic;
}

// Floating-point Type
def TT_Float : AnyTypeOf<[F8E4M3FNUZ, F8E5M2, F8E5M2FNUZ, F16, BF16, F32, F64], "floating-point">;
def TT_FloatTensor : RankedTensorOf<[TT_Float]>;
def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;

// Boolean Type
// TT_Bool -> I1
def TT_BoolTensor : RankedTensorOf<[I1]>;
def TT_BoolLike : AnyTypeOf<[I1, TT_BoolTensor]>;

// Integer Type
def TT_Int : AnyTypeOf<[I1, I8, I16, I32, I64], "integer">;
def TT_IntTensor : RankedTensorOf<[TT_Int]>;
def TT_IntLike : AnyTypeOf<[TT_Int, TT_IntTensor]>;

...
```


最后是Interface `include/triton/Dialect/Triton/IR/TritonInterfaces.td`

```c++
#ifndef TRITON_INTERFACES
#define TRITON_INTERFACES

include "mlir/IR/OpBase.td"

def TensorSizeTrait : NativeOpTrait<"TensorSizeTrait">;  // 用于指定草错具有明确定义的张量大小，它确保操作的操作数和结果具有兼容的张量大小。
def VerifyTensorLayoutsTrait : NativeOpTrait<"VerifyTensorLayoutsTrait">;  // 用于指定操作需要验证张量布局。它确保操作的操作数和结果具有兼容的张量布局。
def SameOperandsEncoding : NativeOpTrait<"SameOperandsEncoding">;  // 用于指定操作要求操作数具有相同的编码。它确保操作的操作数具有相同的编码。
def SameOperandsAndResultEncoding : NativeOpTrait<"SameOperandsAndResultEncoding">;  // 用于指定操作要求操作数和结果具有相同的编码。它确保操作的操作数和结果具有相同的编码。
def SameLoadStoreOperandsShape : NativeOpTrait<"SameLoadStoreOperandsShape">;  // 用于指定操作要求加载和存储操作数具有相同的形状。它确保操作的加载和存储操作数具有相同的形状。
def SameLoadStoreOperandsAndResultShape : NativeOpTrait<"SameLoadStoreOperandsAndResultShape">;  // 用于指定操作要求加载、存储和结果操作数具有相同的形状。它确保操作的加载、存储和结果操作数具有相同的形状。
def SameLoadStoreOperandsEncoding : NativeOpTrait<"SameLoadStoreOperandsEncoding">;  // 用于指定操作要求加载和存储操作数具有相同的编码。它确保操作的加载和存储操作数具有相同的编码。
def SameLoadStoreOperandsAndResultEncoding : NativeOpTrait<"SameLoadStoreOperandsAndResultEncoding">;  // 用于指定操作要求加载、存储和结果操作数具有相同的编码。它确保操作的加载、存储和结果操作数具有相同的编码。

#endif // TRITON_INTERFACES
```

## Triton 中端

经过前端转换之后，python已经被转换为了TritonIR。前端的工作已经结束。

开始进入中端的工作流程，对TritonIR进行优化，将TritonIR转成TritonGPUIR，再对TritonGPU进行优化。

代码逻辑上，入口函数已经在compiler.py中的compiler函数：

```python
def compile(src, target=None, options=None):
    ...
    # 根据指定的target获取一个backend，这里会返回CUDABackend
    backend = make_backend(target) 
    ...
    # 添加编译stage
    backend.add_stages(stages, options) 
    ...
    # 加载所需dialects
    ir.load_dialects(context)  
    backend.load_dialects(context)
    # 创建Triton IR
    module = src.make_ir(options, context) 
    ...
    for ext, compile_ir in list(stages.items())[first_stage:]:
        # 编译、优化各个阶段IR
        next_module = compile_ir(module, metadata) 
        ...
        module = next_module
    ...
    return CompiledKernel(src, metadata_group, hash)
```

即下面for loop的部分，它会遍历stage的内容，获取对应stage的compile函数

- stages：`{'ttir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6710>, 'ttgir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6d40>, 'llir': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6dd0>, 'ptx': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6e60>, 'cubin': <function CUDABackend.add_stages.<locals>.<lambda> at 0x7f28b57e6ef0>}`

这个stage compile的添加是由backend.add_stage执行的，对于不同的backend，实现不同：

- CUDABackend：
  ```python
  def add_stages(self, stages, options):
        stages["ttir"] = lambda src, metadata: self.make_ttir(src, metadata, options)
        stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, self.capability)
        stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options, self.capability)
        stages["ptx"] = lambda src, metadata: self.make_ptx(src, metadata, options, self.capability)
        stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.capability)
    ```
- HIPBackend：
  ```python
    def add_stages(self, stages, options):
        stages["ttir"] = lambda src, metadata: self.make_ttir(src, metadata, options)
        stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options)
        stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
        stages["amdgcn"] = lambda src, metadata: self.make_amdgcn(src, metadata, options)
        stages["hsaco"] = lambda src, metadata: self.make_hsaco(src, metadata, options)
    ```

以CUDABackend为例子，第一个stage对应的compile函数为CUDABackend的成员方法make_ttir，其中passes.common是对于所有ir都有用的，passes.ttir则是triton ir特有的

```python
  @staticmethod
  def make_ttir(mod, metadata, options):
      # 和llvm类似，依赖一个pass manager驱动pass的添加、执行
      pm = ir.pass_manager(mod.context)
      pm.enable_debug()

      # 添加需要的pass
      passes.common.add_inliner(pm)  # 尽可能的inline
      passes.ttir.add_rewrite_tensor_pointer(pm)  # 使用tensor指针重写load/store
      passes.ttir.add_combine(pm)  # 将ops融合
      passes.common.add_canonicalizer(pm)  # 常量折叠，转换等常规优化
      passes.ttir.add_reorder_broadcast(pm)  # 将elementwise的操作移动到broadcast之前
      passes.common.add_cse(pm)  # 公共子表达式消除
      passes.common.add_licm(pm)  # 将循环中不变的执行调整到循环外边
      passes.common.add_symbol_dce(pm)  # 消除死变量
      pm.run(mod)  # 执行pass
      return mod
```

首先来看pass_manager，它在ir.cc中被binding到了PassManager类，它并不是triton设计的，而是mlir中设计的

```c++
  py::class_<PassManager>(m, "pass_manager", py::module_local())
      .def(py::init<MLIRContext *>())
```

下面就是向pass manager中添加pass。

首先在我们看到的python添加之前，还需要做init操作，这一步在libtriton库init时就已经执行了，即PYBIND11_MODULE中的init_triton_pass

```c++
// python/src/main.cc
PYBIND11_MODULE(libtriton, m) {
  m.doc() = "Python bindings to the C++ Triton API";
  init_triton_env_vars(m);
  init_triton_ir(m.def_submodule("ir"));
  init_triton_passes(m.def_submodule("passes"));
  init_triton_interpreter(m.def_submodule("interpreter"));
  init_triton_llvm(m.def_submodule("llvm"));
  FOR_EACH_P(INIT_BACKEND, TRITON_BACKENDS_TUPLE)
}
```

init_triton_pass中会初始化所有的pass

```c++
void init_triton_passes(py::module &&m) {
  init_triton_analysis(m.def_submodule("analysis"));
  init_triton_passes_common(m.def_submodule("common"));
  init_triton_passes_convert(m.def_submodule("convert"));
  init_triton_passes_ttir(m.def_submodule("ttir"));
  init_triton_passes_ttgpuir(m.def_submodule("ttgpuir"));
  init_triton_passes_llvmir(m.def_submodule("llvmir"));
}
```

进入具体的common和ttir的函数

```c++
void init_triton_passes_common(py::module &&m) {
  using namespace mlir;
  ADD_PASS_WRAPPER_0("add_sccp", createSCCPPass);
  ADD_PASS_WRAPPER_0("add_symbol_dce", createSymbolDCEPass);
  ADD_PASS_WRAPPER_0("add_inliner", createInlinerPass);
  ADD_PASS_WRAPPER_0("add_canonicalizer", createCanonicalizerPass);
  ADD_PASS_WRAPPER_0("add_cse", createCSEPass);
  ADD_PASS_WRAPPER_0("add_licm", createLoopInvariantCodeMotionPass);
}

void init_triton_passes_ttir(py::module &&m) {
  using namespace mlir::triton;
  ADD_PASS_WRAPPER_0("add_combine", createCombineOpsPass);
  ADD_PASS_WRAPPER_0("add_reorder_broadcast", createReorderBroadcastPass);
  ADD_PASS_WRAPPER_0("add_rewrite_tensor_pointer",
                     createRewriteTensorPointerPass);
  ADD_PASS_WRAPPER_4("add_convert_to_ttgpuir",
                     createConvertTritonToTritonGPUPass, const std::string &,
                     int, int, int);
}
```

`ADD_PASS_WRAPPER_0`宏中做了具体的函数绑定，即继续回到python侧，发生`passes.ttir.add_combine(pm)`调用时pass通过addPass方法添加到pass manager中

```c++
#define ADD_PASS_WRAPPER_0(name, builder)                                      \
  m.def(name, [](mlir::PassManager &pm) { pm.addPass(builder()); })

```

builder()即为createCombineOpsPass()方法，在Passes.td中可以找到

```c++
def TritonCombineOps : Pass</*cli-arg*/"triton-combine", /*Op*/"mlir::ModuleOp"> {
  let summary = "combine ops";
  let description = [{
    dot(a, b, 0) + c => dot(a, b, c)

    addptr(addptr(ptr, idx0), idx1) => addptr(ptr, AddI(idx0, idx1))

    select(cond, load(ptrs, broadcast(cond), ???), other) =>
        load(ptrs, broadcast(cond), other)
  }];

  let constructor = "mlir::triton::createCombineOpsPass()";

  let dependentDialects = ["mlir::arith::ArithDialect"];
}
```

它的实现则是预先定义好的，不是TableGen生成的，位于lib/Dialect/Triton/Transforms/Combine.cpp中

```c++
std::unique_ptr<mlir::Pass> createCombineOpsPass() {
  return std::make_unique<CombineOpsPass>();
}
```

也就是返回CombineOpsPass对象的指针，它被添加到了Pass Manager中

```c++
class CombineOpsPass : public TritonCombineOpsBase<CombineOpsPass> {
public:
  void runOnOperation() override {
    MLIRContext *context = &getContext();
    RewritePatternSet patterns(context);
    ModuleOp m = getOperation();

    // Dot Add %{
    patterns.add<CombineDotAddIPattern>(context);
    patterns.add<CombineDotAddFPattern>(context);
    patterns.add<CombineDotAddIRevPattern>(context);
    patterns.add<CombineDotAddFRevPattern>(context);
    // %}
    patterns.add<CombineSelectMaskedLoadPattern>(context);
    patterns.add<CombineAddPtrPattern>(context);
    patterns.add<CombineBroadcastConstantPattern>(context);
    patterns.add<CombineBroadcastMulReducePattern>(context);

    if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())
      signalPassFailure();
  }
};
```

其他pass也是相同的添加方式，全部做完后，调用pm.run方法

```c++
py::class_<PassManager>(m, "pass_manager", py::module_local())
      .def(py::init<MLIRContext *>())
      .def("run", [](PassManager &self, ModuleOp &mod) {
        // pass manager this 指针，ttir module
        // TODO: maybe dump module to file and print error for better
        // diagnostics
        
        // *暂时不考虑下列环境变量设置
        auto reproducerPath =
            triton::tools::getStrEnv("TRITON_REPRODUCER_PATH");
        if (!reproducerPath.empty()) {
          auto anchorName = self.getOpAnchorName();
          auto passes = self.getPasses();
          Operation *op = mod.getOperation();
          makeReproducer(anchorName, passes, op, reproducerPath);
        }

        if (triton::tools::getBoolEnv("TRITON_ENABLE_LLVM_DEBUG")) {
          ::llvm::DebugFlag = true;
        }

        if (auto debugOnly = triton::tools::getStrEnv("TRITON_LLVM_DEBUG_ONLY");
            !debugOnly.empty()) {
          llvm::SmallVector<StringRef, 3> split;
          llvm::SmallVector<std::string, 3> storage;
          llvm::SmallVector<const char *, 3> debugTypes;

          StringRef(debugOnly.c_str()).split(split, ',');
          llvm::transform(split, std::back_inserter(debugTypes),
                          [&storage](StringRef str) {
                            // StringRefs are not always null-terminated.
                            // The purpose for this storage pattern is to
                            // produce a collection of C-strings that are.
                            storage.push_back(str.str());
                            return storage.back().c_str();
                          });

          ::llvm::DebugFlag = true;
          ::llvm::setCurrentDebugTypes(debugTypes.data(), debugTypes.size());
        }

        bool haveTiming = ::triton::tools::getBoolEnv("MLIR_ENABLE_TIMING");
        if (haveTiming) {
          self.enableTiming();
        }

        // 实际执行mlir提供的pass manager的run方法，执行刚刚添加过的所有pass
        if (failed(self.run(mod.getOperation())))
          throw std::runtime_error("PassManager::run failed");
      });
```

获取到的是一个优化后的ttir module，接下来，这个module将会作为输入，给到make_ttgir用于ttgir的生成和优化

```python
  @staticmethod
  def make_ttgir(mod, metadata, opt, capability):
      cluster_info = nvidia.ClusterInfo()
      if opt.cluster_dims is not None:
          cluster_info.clusterDimX = opt.cluster_dims[0]
          cluster_info.clusterDimY = opt.cluster_dims[1]
          cluster_info.clusterDimZ = opt.cluster_dims[2]
      # TTIR -> TTGIR
      pm = ir.pass_manager(mod.context)
      pm.enable_debug()
      passes.ttir.add_convert_to_ttgpuir(pm, f"cuda:{capability}", opt.num_warps, 32, opt.num_ctas)
      # optimize TTGIR
      passes.ttgpuir.add_coalesce(pm)
      if capability // 10 >= 8:
          passes.ttgpuir.add_f32_dot_tc(pm)
      # TODO(Qingyi): Move PlanCTAPass to the front of CoalescePass
      nvidia.passes.ttnvgpuir.add_plan_cta(pm, cluster_info)
      passes.ttgpuir.add_remove_layout_conversions(pm)
      passes.ttgpuir.add_optimize_thread_locality(pm)
      passes.ttgpuir.add_accelerate_matmul(pm)
      passes.ttgpuir.add_remove_layout_conversions(pm)
      passes.ttgpuir.add_optimize_dot_operands(pm, capability >= 80)
      passes.common.add_cse(pm)
      if capability // 10 >= 8:
          passes.ttgpuir.add_combine_tensor_select_and_if(pm)
          passes.ttgpuir.add_pipeline(pm, opt.num_stages)
      passes.ttgpuir.add_prefetch(pm)
      passes.ttgpuir.add_optimize_dot_operands(pm, capability >= 80)
      passes.ttgpuir.add_remove_layout_conversions(pm)
      passes.ttgpuir.add_reduce_data_duplication(pm)
      passes.ttgpuir.add_reorder_instructions(pm)
      passes.common.add_cse(pm)
      passes.common.add_symbol_dce(pm)
      if capability // 10 >= 9:
          nvidia.passes.ttnvgpuir.add_fence_insertion(pm)
          nvidia.passes.ttnvgpuir.add_tma_lowering(pm)
      passes.common.add_canonicalizer(pm)
      pm.run(mod)
      metadata["cluster_dims"] = (cluster_info.clusterDimX, cluster_info.clusterDimY, cluster_info.clusterDimZ)
      return mod
```

因为我们已经对这里的pass添加流程很熟悉了，所以直接跳到它的实现中

```c++

class ConvertTritonToTritonGPU
    : public ConvertTritonToTritonGPUBase<ConvertTritonToTritonGPU> {
public:
  ConvertTritonToTritonGPU() = default;
  // constructor with some parameters set explicitly.
  ConvertTritonToTritonGPU(const std::string &target, int numWarps,
                           int threadsPerWarp, int numCTAs) {
    this->numWarps = numWarps;
    this->threadsPerWarp = threadsPerWarp;
    this->numCTAs = numCTAs;
    this->target = target;
  }

  void runOnOperation() override {
    MLIRContext *context = &getContext();
    ModuleOp mod = getOperation();
    // type converter
    TritonGPUTypeConverter typeConverter(context, numWarps, threadsPerWarp,
                                         numCTAs);
    TritonGPUConversionTarget target(*context, typeConverter);
    // rewrite patterns
    RewritePatternSet patterns(context);  // 用于收集转换过程中的RewritePattern
    // add rules
    populateArithPatternsAndLegality(typeConverter, patterns, target);  // 添加arith dialect op的RewritePattern到patternset
    // 会调用patterns.add<GenericOpPattern<arith::AddIOp>表明使用GenericOpPattern处理addi
    populateMathPatternsAndLegality(typeConverter, patterns, target);  // 添加math dialect op的RewritePattern到patternset
    populateTritonPatterns(typeConverter, patterns, numCTAs);  // 添加triton dialect op的RewritePattern到patternset
    // 会调用patterns.insert<GenericOpPattern<triton::LoadOp>, GenericOpPattern<triton::StoreOp> 表明使用GenericOpPattern处理ld st
    populateSCFPatterns(typeConverter, patterns);  // 添加SCF dialect op的RewritePattern到patternset
    populateCFPatterns(typeConverter, patterns);  // 添加CFP dialect op的RewritePattern到patternset

    auto inti = llvm::APSInt(32, false);
    auto i32_ty = IntegerType::get(mod->getContext(), 32);

    mod->setAttr(
        AttrNumWarpsName,
        IntegerAttr::get(i32_ty, llvm::APInt(32, numWarps.getValue())));
    mod->setAttr(
        AttrNumThreadsPerWarp,
        IntegerAttr::get(i32_ty, llvm::APInt(32, threadsPerWarp.getValue())));

    mod->setAttr(AttrNumCTAsName,
                 IntegerAttr::get(i32_ty, llvm::APInt(32, numCTAs.getValue())));

    if (this->target.getValue().empty()) {
      mod.emitError("expected target specification to attach to the module op");
      return signalPassFailure();
    }
    mod->setAttr(AttrTargetName,
                 StringAttr::get(context, this->target.getValue()));

    // 执行op的转换
    if (failed(applyPartialConversion(mod, target, std::move(patterns))))
      return signalPassFailure();

    // update layouts
    //  broadcast src => multicast, dst => broadcasted
    // if (failed(target.refineLayouts(mod, numWarps)))
    //   return signalPassFailure();
  }
};
```

populateTritonPatterns会注册很多用于转换的pattern

```c++
void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,
                            RewritePatternSet &patterns, unsigned numCTAs) {
  MLIRContext *context = patterns.getContext();
  patterns.insert< // TODO: view should have custom pattern that views the
                   // layout
      GenericOpPattern<triton::AdvanceOp>,
      GenericOpPattern<triton::MakeTensorPtrOp>,
      GenericOpPattern<triton::ReshapeOp>, GenericOpPattern<triton::BitcastOp>,
      GenericOpPattern<triton::FpToFpOp>, GenericOpPattern<triton::IntToPtrOp>,
      GenericOpPattern<triton::PtrToIntOp>, GenericOpPattern<triton::SplatOp>,
      TritonBroadcastPattern, GenericOpPattern<triton::AddPtrOp>,
      TritonCatPattern, TritonJoinOpPattern, TritonSplitOpPattern,
      GenericOpPattern<triton::ClampFOp>,
      GenericOpPattern<triton::PreciseSqrtOp>,
      GenericOpPattern<triton::PreciseDivFOp>,
      GenericOpPattern<triton::MulhiUIOp>,
      GenericOpPattern<triton::ElementwiseInlineAsmOp>, TritonReducePattern,
      GenericOpPattern<triton::ReduceReturnOp>, TritonScanPattern,
      GenericOpPattern<triton::ScanReturnOp>,
      GenericOpPattern<triton::MakeRangeOp>, TritonExpandDimsPattern,
      TritonTransPattern, TritonDotPattern, GenericOpPattern<triton::LoadOp>,
      GenericOpPattern<triton::StoreOp>, GenericOpPattern<triton::HistogramOp>,
      GenericOpPattern<triton::ExternElementwiseOp>,
      GenericOpPattern<triton::PrintOp>, GenericOpPattern<triton::AssertOp>,
      GenericOpPattern<triton::AtomicCASOp>,
      GenericOpPattern<triton::AtomicRMWOp>, GenericOpPattern<ReturnOp>,
      GenericOpPattern<triton::ExperimentalDescriptorLoadOp>,
      GenericOpPattern<triton::ExperimentalDescriptorStoreOp>,
      GenericOpPattern<triton::CallOp>, TritonFuncOpPattern>(typeConverter,
                                                             context);
}
```

这个过程被称为conversion（或者rewrite）过程，TritonGPUTypeConverter用于类型的转换，TritonGPUConversionTarget用于op的转换。然后通过populate函数讲一些dialect的节点对应的转换pattern记录在patternset内，用来提供节点的转换方法。最后applyPartialConversion方法执行op的转换。

applyPartialConversion这是mlir中提供的标准接口，位于`mlir/lib/Transforms/Utils/DialectConversion.cpp`中

```c++
LogicalResult
mlir::applyPartialConversion(ArrayRef<Operation *> ops,
                             const ConversionTarget &target,
                             const FrozenRewritePatternSet &patterns,
                             DenseSet<Operation *> *unconvertedOps) {
  OperationConverter opConverter(target, patterns, OpConversionMode::Partial,
                                 unconvertedOps);
  return opConverter.convertOperations(ops);
}
```

进一步调用同文件中的convertOperations方法

```c++
LogicalResult OperationConverter::convertOperations(
    ArrayRef<Operation *> ops,
    function_ref<void(Diagnostic &)> notifyCallback) {
  if (ops.empty())
    return success();
  const ConversionTarget &target = opLegalizer.getTarget();

  // Compute the set of operations and blocks to convert.
  SmallVector<Operation *> toConvert;
  for (auto *op : ops) {
    op->walk<WalkOrder::PreOrder, ForwardDominanceIterator<>>(
        [&](Operation *op) {
          toConvert.push_back(op);
          auto legalityInfo = target.isLegal(op); // 判断当前op是否合法，即在ttgir中依然可以使用的op表示
          if (legalityInfo && legalityInfo->isRecursivelyLegal)
            return WalkResult::skip();
          return WalkResult::advance();
        });
  }

  // Convert each operation and discard rewrites on failure.
  ConversionPatternRewriter rewriter(ops.front()->getContext());
  ConversionPatternRewriterImpl &rewriterImpl = rewriter.getImpl();
  for (auto *op : toConvert)
    if (failed(convert(rewriter, op))) // 转换op
      return rewriterImpl.discardRewrites(), failure();
  ......
  return success();
}
```

判断op是否合法是由ConversionTarget的isLegal方法负责的，它来自于opLegalizer的getTarget()方法，而opLegalizer的实例化则是上一步中的opConverter实例化时被同步实例化的。它返回的target实际上就是applyPartialConversion中传入的ConversionTarget，即TritonGPUConversionTarget。

对于哪些op合法，哪些op不合法，在TritonGPUConversionTarget中会进行指定

```c++
TritonGPUConversionTarget::TritonGPUConversionTarget(
    MLIRContext &context, TritonGPUTypeConverter &typeConverter)
    : ConversionTarget(context) {
  // TODO: we should also verify ops of TritonGPUDialect
  // TritonGPUDialect中的所有OP都合法
  addLegalDialect<triton::gpu::TritonGPUDialect>();  

  // Some ops from SCF are illegal
  // 以下四个scf中的OP都不合法
  addIllegalOp<scf::ExecuteRegionOp, scf::ParallelOp, scf::ReduceOp,
               scf::ReduceReturnOp>();
  
  // 以下四个OP在某些情况下合法
  addDynamicallyLegalDialect<arith::ArithDialect, math::MathDialect,
                             triton::TritonDialect, cf::ControlFlowDialect,
                             scf::SCFDialect>([&](Operation *op) {
    bool hasLegalRegions = true;
    for (auto &region : op->getRegions()) {
      // 由typeConverter来做类型判断是否合法
      hasLegalRegions = hasLegalRegions && typeConverter.isLegal(&region);
    }
    if (hasLegalRegions && typeConverter.isLegal(op)) {
      return true;
    }
    return false;
  });

  // We have requirements for the data layouts

  addDynamicallyLegalOp<triton::DotOp>([](triton::DotOp dotOp) -> bool {
    Attribute aEncoding =
        cast<RankedTensorType>(dotOp.getA().getType()).getEncoding();
    Attribute bEncoding =
        cast<RankedTensorType>(dotOp.getB().getType()).getEncoding();
    if (aEncoding && isa<triton::gpu::DotOperandEncodingAttr>(aEncoding) &&
        bEncoding && isa<triton::gpu::DotOperandEncodingAttr>(bEncoding))
      return true;
    return false;
  });
}
```

这里用到的是TritonGPUTypeConverter，在它的构造函数中声明了RankedTensorType和triton::PointerType两个类型需要转换，

```c++

TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,
                                               int numWarps, int threadsPerWarp,
                                               int numCTAs)
    : context(context), numWarps(numWarps), threadsPerWarp(threadsPerWarp),
      numCTAs(numCTAs) {

  // 当前类型合法直接返回
  addConversion([](Type type) { return type; });

  // Add encoding for tensor
  addConversion([this](RankedTensorType tensorType) -> RankedTensorType {
    // types with encoding are already in the right format
    // TODO: check for layout encodings more specifically
    if (tensorType.getEncoding())
      return tensorType;
    ArrayRef<int64_t> shape = tensorType.getShape();
    triton::gpu::BlockedEncodingAttr encoding =
        getDefaultBlockedEncoding(this->context, shape, this->numWarps,
                                  this->threadsPerWarp, this->numCTAs);
    return RankedTensorType::get(shape, tensorType.getElementType(), encoding);
  });

  // Add encoding for tensor pointer
  addConversion([this](triton::PointerType ptrType) -> triton::PointerType {
    // Check whether tensor pointer `tt.ptr<tensor<>>`
    auto pointeeTensorType =
        dyn_cast<RankedTensorType>(ptrType.getPointeeType());
    if (pointeeTensorType == nullptr)
      return ptrType;

    // Add layout into the tensor
    auto convertedTensorType = convertType(pointeeTensorType);
    return triton::PointerType::get(convertedTensorType,
                                    ptrType.getAddressSpace());
  });
  ...
}
```

经过合法性判断后，来到convert(rewriter, op)这一步

```c++
LogicalResult OperationConverter::convert(ConversionPatternRewriter &rewriter,
                                          Operation *op) {
  // Legalize the given operation.
  if (failed(opLegalizer.legalize(op, rewriter))) {
    if (mode == OpConversionMode::Full)
      return op->emitError()
             << "failed to legalize operation '" << op->getName() << "'";
    if (mode == OpConversionMode::Partial) {
      if (opLegalizer.isIllegal(op))
        return op->emitError()
               << "failed to legalize operation '" << op->getName()
               << "' that was explicitly marked illegal";
      if (trackedOps)
        trackedOps->insert(op);
    }
  } else if (mode == OpConversionMode::Analysis) {
    trackedOps->insert(op);
  }
  return success();
}
```

而它内部主要发挥作用的是legalize方法

```c++
LogicalResult
OperationLegalizer::legalize(Operation *op,
                             ConversionPatternRewriter &rewriter) {
  ...
  // If the operation isn't legal, try to fold it in-place.
  // 常量折叠
  if (succeeded(legalizeWithFold(op, rewriter))) {
    LLVM_DEBUG({
      logSuccess(logger, "operation was folded");
      logger.startLine() << logLineComment;
    });
    return success();
  }

  // Otherwise, we need to apply a legalization pattern to this operation.
  // 尝试具体的pattern
  if (succeeded(legalizeWithPattern(op, rewriter))) {
    LLVM_DEBUG({
      logSuccess(logger, "");
      logger.startLine() << logLineComment;
    });
    return success();
  }
  ...
  return failure();
}
```

它的实现中

```c++
LogicalResult
OperationLegalizer::legalizeWithPattern(Operation *op,
                                        ConversionPatternRewriter &rewriter) {
  ...

  // Try to match and rewrite a pattern on this operation.
  return applicator.matchAndRewrite(op, rewriter, canApply, onFailure,
                                    onSuccess);
}
```

`applicator.matchAndRewrite`调用的是PatternApplicator中的方法

```c++
LogicalResult PatternApplicator::matchAndRewrite (...) {
  const Pattern *bestPattern = nullptr;
  // Find the next pattern with the highest benefit.
  ...
  const auto *pattern = static_cast<const RewritePattern *>(bestPattern);
  result = pattern->matchAndRewrite(op, rewriter);
  ...
}
```

根据当前op的name找到所有的候选pattern，然后会经过一个cost model计算得到当前pattern的benefits指标作为选择的依据，选出最收益最高的Pattern调用它的matchAndRewrite方法

在TritonGPUDialect中的用于TritonIR到TritonGPU IR转换的pattern，在TritonToTritonGPUPasses.cpp中，以populateTritonPatterns为例，它里面用到了大量的GenericOpPattern，比如我们的vector add例子中用到的laod、store：

```c++
GenericOpPattern<triton::LoadOp>, GenericOpPattern<triton::StoreOp>
```

在同文件中，找到GenericOpPattern的实现，一目了然，它实际上直接用了传入的类型，没有做任何的转换，就是原本的op输出了。所以对于GenericOpPattern，我们的IR虽然从TritonIR变为了TritGPUIR，但是，应该不会发生很大的变化。

```c++
// TritonToTritonGPUPass.cpp
template <class Op> struct GenericOpPattern : public OpConversionPattern<Op> {
  using OpConversionPattern<Op>::OpConversionPattern;

  LogicalResult
  matchAndRewrite(Op op, typename Op::Adaptor adaptor,
                  ConversionPatternRewriter &rewriter) const override {
    SmallVector<Type> retTypes;
    if (failed(this->getTypeConverter()->convertTypes(op->getResultTypes(),
                                                      retTypes)))
      return failure();
    rewriter.replaceOpWithNewOp<Op>(op, retTypes, adaptor.getOperands(),
                                    op->getAttrs());

    return success();
  }
};
```

到这里add_convert_to_ttgpuir的工作就全部结束了，它实际上就是两步：

1. 判断op是否合法；
2. 如果合法不处理，如果不合法则通过RewritePatternSet中添加的RewritePattern的matchAndRewrite接口进行转换；

跳过make_ttgir中的胜于pass，执行完pm.run(mod)后，获取到的vector add的ir如下所示，一方面我们这个例子过于简单，另一个方面我们看到大量使用GeneraicOpPattern的情况，ir实际上没有发生大的变换，只是增加了平台相关的module attribute

```c++
#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#loc = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0)
module attributes {"triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.target = "cuda:80", "triton_gpu.threads-per-warp" = 32 : i32} {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":17:0)) attributes {noinline = false} {
    %c1024_i32 = arith.constant 1024 : i32 loc(#loc1)
    %0 = tt.get_program_id x : i32 loc(#loc2)
    %1 = arith.muli %0, %c1024_i32 : i32 loc(#loc3)
    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #blocked> loc(#loc4)
    %3 = tt.splat %1 : i32 -> tensor<1024xi32, #blocked> loc(#loc5)
    %4 = arith.addi %3, %2 : tensor<1024xi32, #blocked> loc(#loc5)
    %5 = tt.splat %arg3 : i32 -> tensor<1024xi32, #blocked> loc(#loc6)
    %6 = arith.cmpi slt, %4, %5 : tensor<1024xi32, #blocked> loc(#loc6)
    %7 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc7)
    %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32>, #blocked>, tensor<1024xi32, #blocked> loc(#loc7)
    %9 = tt.load %8, %6 : tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc8)
    %10 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc9)
    %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr<f32>, #blocked>, tensor<1024xi32, #blocked> loc(#loc9)
    %12 = tt.load %11, %6 : tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc10)
    %13 = arith.addf %9, %12 : tensor<1024xf32, #blocked> loc(#loc11)
    %14 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc12)
    %15 = tt.addptr %14, %4 : tensor<1024x!tt.ptr<f32>, #blocked>, tensor<1024xi32, #blocked> loc(#loc12)
    tt.store %15, %13, %6 : tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc13)
    tt.return loc(#loc14)
  } loc(#loc)
} loc(#loc)
#loc1 = loc(unknown)
#loc2 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":27:24)
#loc3 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":32:24)
#loc4 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":33:41)
#loc5 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":33:28)
#loc6 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":35:21)
#loc7 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":38:24)
#loc8 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":38:16)
#loc9 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":39:24)
#loc10 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":39:16)
#loc11 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":40:17)
#loc12 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:26)
#loc13 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:35)
#loc14 = loc("/data_ssd1/zjy_home/my_code/MLC-Learning/Triton/triton-examples/00.vector-add/00.vector-add.py":42:4)
```


## Triton 后端


## References

- [OpenAI/Triton MLIR 迁移工作简介 - by Chunwei Yan from Nivida Tirton开发人员](https://superjomn.github.io/posts/triton-mlir-publish/) 高质量
- [机器学习编译器代码生成相关 MLIR Dialect - by Lei MLIR开发人员](https://www.lei.chat/zh/posts/mlir-codegen-dialects-for-machine-learning-compilers/) 高质量
- [MLIR Tutorial by 周可行 北京大学](https://github.com/KEKE046/mlir-tutorial) 高质量
- [MLIR Operation Definition Specification (ODS)](https://mlir.llvm.org/docs/DefiningDialects/Operations/)
- [MLIR Defining Dialects](https://mlir.llvm.org/docs/DefiningDialects/)
- [Triton Document Dialect](https://triton-lang.org/main/dialects/)
- [窥探Triton的lower(一)](https://zhuanlan.zhihu.com/p/695171704)
- [窥探Triton的lower(二)](https://zhuanlan.zhihu.com/p/695255185)
- [Triton Dialect](https://zhuanlan.zhihu.com/p/685209384)
- [2023 EuroLLVM - MLIR Dialect Design and Composition for Front-End Compilers](https://www.youtube.com/watch?v=hIt6J1_E21c)


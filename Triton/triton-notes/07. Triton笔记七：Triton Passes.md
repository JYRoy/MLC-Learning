# Triton Passes

- [Triton Passes](#triton-passes)
  - [Overview](#overview)
  - [IR Convert Pass](#ir-convert-pass)
    - [ConvertTritonToTritonGPU](#converttritontotritongpu)
    - [ConvertTritonGPUToLLVM](#converttritongputollvm)
  - [References](#references)

## Overview

Triton中的所有passes都在python/src/passes.cc中被注册到libtriton.passes的module中

- passes：TritonIR、TritonGPUIR和LLVMIR依赖的所有passes
  - analysis
    - ModuleAllocation
    - ModuleMembarAnalysis
  - common
    - SCCP
    - SymbolDCE
    - Inliner
    - Canonicalizer
    - CSE
    - LoopInvariantCodeMotion
  - convert
    - ConvertSCFToCF
    - ConvertControlFlowToLLVM
    - CovertIndexToLLVM
    - ArithToLLVMConversion
  - ttir
    - CombineOps
    - ReorderBroadcast
    - RewriteTensorPointer
    - ConvertTensorPointer
    - ConvertTritonToTritonGPU：给每个tensor分配layout
  - ttgpuir
    - TritonGPUCoalesce：针对load和store修改layout，合并全局访存
    - TritonGPUOptimizeThreadLocality
    - TritonGPUPipeline
    - TritonGPUAccelerateMatMul：修改layout以使用tensor core，软流水，back conflict去除
    - TritonGPUReorderInstructions
    - TritonGPUF32DotTC
    - TritonGPUOptimizerDotOperands
    - TritonGPURemoveLaytoutConversions：沿着数据流传播layout，消除不必要的layout转换
    - TritonGPUReduceDataDuplication
    - AllocateSharedMemory
    - TritonGPUCombineTensorSelectAndIf
  - ttgpuir
    - ConvertTritonGPUToLLVM：生成SIMT的LLVMIR。添加shared memroy和barrier。
    - DecomposeUnsupportedConversions
  - ttnvgpuir
    - TritonNvidiaGPUPlanCTA
    - TritonNvidiaGPUFenceInsertion
    - TritonNvidiaGPUTMALowering
  - llvmir
    - LLVMDIScope

从功能上划分，有些pass是优化IR的，有些pass是转换IR的。

有些是Triton中实现的，有些是MLIR中原本就存在的。

## IR Convert Pass

根据之前梳理的编译过程，首先关注下IR lowering相关的pass

IR过程如下：

- Python Src转换为Python AST
- Python AST转换为Triton IR
- Triton IR转换为TritonGPU IR
- TritonGPU IR转换为LLVM IR(MLIR)
- LLVM IR(MLIR)转换为LLVM IR
- LLVM IR转换为PTX
- 调用ptxas编译ptx为cubin

其中，涉及到Triton实现的pass部分的为：

- Triton IR转换为TritonGPU IR：ConvertTritonToTritonGPU
- TritonGPU IR转换为LLVM IR(MLIR)：ConvertTritonGPUToLLVM

### ConvertTritonToTritonGPU

该Pass在init_triton_passes_ttir中注册

```c++
void init_triton_passes_ttir(py::module &&m) {
  using namespace mlir::triton;
  ...
  ADD_PASS_WRAPPER_4("add_convert_to_ttgpuir",
                     createConvertTritonToTritonGPUPass, const std::string &,
                     int, int, int);
}
```

其中ADD_PASS_WRAPPER_4这个宏为进行pybind，完成向pass manager中添加pass的操作，添加的对象为createConvertTritonToTritonGPUPass函数的返回值

```c++
#define ADD_PASS_WRAPPER_4(name, builder, ty0, ty1, ty2, ty3)                  \
  m.def(name, [](mlir::PassManager &pm, ty0 val0, ty1 val1, ty2 val2,          \
                 ty3 val3) { pm.addPass(builder(val0, val1, val2, val3)); })
```


在这个pass的td文件中可以看到它定义，`let constructor = "mlir::triton::createConvertTritonToTritonGPUPass()";`声明createConvertTritonToTritonGPUPass为这个pass的构造函数。

```c
#ifndef TRITON_CONVERSION_PASSES
#define TRITON_CONVERSION_PASSES

include "mlir/Pass/PassBase.td"

def ConvertTritonToTritonGPU: Pass<"convert-triton-to-tritongpu", "mlir::ModuleOp"> {
    let summary = "Convert Triton to TritonGPU";
    let description = [{

    }];
    let constructor = "mlir::triton::createConvertTritonToTritonGPUPass()";

    let dependentDialects = ["mlir::arith::ArithDialect",
                             "mlir::math::MathDialect",
                             // TODO: Does this pass depend on SCF?
                             "mlir::scf::SCFDialect",
                             "mlir::triton::TritonDialect",
                             "mlir::triton::gpu::TritonGPUDialect"];

   let options = [
       Option<"numWarps", "num-warps",
              "int32_t", /*default*/"4",
              "number of warps">,

       Option<"threadsPerWarp", "threads-per-warp",
              "int32_t", /*default*/"32",
              "number of threads per warp">,
        Option<"numCTAs", "num-ctas",
              "int32_t", /*default*/"1",
              "number of ctas in a cga">,
        Option<"target", "target",
              "std::string", /*default*/"\"\"",
              "the GPU target, e.g., cuda:80, hip:gfx942">
   ];
}

#endif
```

createConvertTritonToTritonGPUPass这个pass的实现在lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp中，创建了ConvertTritonToTritonGPU类的对象

```c++
std::unique_ptr<OperationPass<ModuleOp>>
mlir::triton::createConvertTritonToTritonGPUPass(const std::string &target,
                                                 int numWarps,
                                                 int threadsPerWarp,
                                                 int numCTAs) {
  return std::make_unique<::ConvertTritonToTritonGPU>(target, numWarps,
                                                      threadsPerWarp, numCTAs);
}

std::unique_ptr<OperationPass<ModuleOp>>
mlir::triton::createConvertTritonToTritonGPUPass() {
  return std::make_unique<::ConvertTritonToTritonGPU>();
}
```

ConvertTritonToTritonGPU类在同文件中定义，构造函数只是给几个成员变量进行了赋值，pass的实际执行动作被定义在runOnOperation函数中，这是mlir的pass规定写法，因为pass manager执行pass时会去调用这个函数，runOnOperation的具体解释以注释的方式说明

从整体流程上看，可以分为三步：

1. 定义targets：
   1. targets主要用于明确在转换过程中哪些算子和 Dialect 是合法的，算子和 Dialect 可以被标记为合法、动态与非法三种 actions，其中动态是指某些算子只有在部分实例中是合法的。
   2. 当存在 type 转换时，则需要 Type Converter 来定义 type 在与 pattern 交互时的转换方式，重映射后的操作数类型需要与 type converter 规定的一致，如果没有提供 type converter
2. 添加rewrite patterns：
   1. 需要合法化 patterns 来将非法算子转为合法算子，所以 rewrite pattern 是用于实现非法算子转换为合法算子的转换逻辑。
   2. Dialect Conversion 框架会自动根据所提供的的 patterns 生成一个转换图用于合法化，从而简化整个改写的流程，例如我们的 patterns 中只提到 Dialect A 的 op0 可以合法化为 B 中的 op0，B 中的 op0 可以合法化为 C 中的 op0，Conversion 框架就会自动检测 DialectA 的 op0 可以合法化为 DialectC 中的 op0，而不用经过中间的算子转换。
3. 使用转换接口applyPartialConversion

```c++

class ConvertTritonToTritonGPU
    : public ConvertTritonToTritonGPUBase<ConvertTritonToTritonGPU> {
public:
  ConvertTritonToTritonGPU() = default;
  // constructor with some parameters set explicitly.
  ConvertTritonToTritonGPU(const std::string &target, int numWarps,
                           int threadsPerWarp, int numCTAs) {
    this->numWarps = numWarps;
    this->threadsPerWarp = threadsPerWarp;
    this->numCTAs = numCTAs;
    this->target = target;
  }

  void runOnOperation() override {
    MLIRContext *context = &getContext();
    ModuleOp mod = getOperation();
    // type converter
    TritonGPUTypeConverter typeConverter(context, numWarps, threadsPerWarp,
                                         numCTAs);
    // 1. 创建 target
    TritonGPUConversionTarget target(*context, typeConverter);
    // rewrite patterns
    // 2. 添加rewrite patterns
    RewritePatternSet patterns(context);
    // add rules
    // 这些方法展开后是一系列的patterns.add<...>(...)
    populateArithPatternsAndLegality(typeConverter, patterns, target);
    populateMathPatternsAndLegality(typeConverter, patterns, target);
    populateTritonPatterns(typeConverter, patterns, numCTAs);
    populateSCFPatterns(typeConverter, patterns);
    populateCFPatterns(typeConverter, patterns);

    auto inti = llvm::APSInt(32, false);
    auto i32_ty = IntegerType::get(mod->getContext(), 32);

    mod->setAttr(
        AttrNumWarpsName,
        IntegerAttr::get(i32_ty, llvm::APInt(32, numWarps.getValue())));
    mod->setAttr(
        AttrNumThreadsPerWarp,
        IntegerAttr::get(i32_ty, llvm::APInt(32, threadsPerWarp.getValue())));

    mod->setAttr(AttrNumCTAsName,
                 IntegerAttr::get(i32_ty, llvm::APInt(32, numCTAs.getValue())));

    if (this->target.getValue().empty()) {
      mod.emitError("expected target specification to attach to the module op");
      return signalPassFailure();
    }
    mod->setAttr(AttrTargetName,
                 StringAttr::get(context, this->target.getValue()));

    if (failed(applyPartialConversion(mod, target, std::move(patterns))))
      return signalPassFailure();

    // update layouts
    //  broadcast src => multicast, dst => broadcasted
    // if (failed(target.refineLayouts(mod, numWarps)))
    //   return signalPassFailure();
  }
};
```

关于pattern，我们以populateTritonPatterns为例子

```c++
void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,
                            RewritePatternSet &patterns, unsigned numCTAs) {
  MLIRContext *context = patterns.getContext();
  patterns.insert<
      GenericOpPattern<triton::AdvanceOp>,
      GenericOpPattern<triton::MakeTensorPtrOp>,
      GenericOpPattern<triton::ReshapeOp>, GenericOpPattern<triton::BitcastOp>,
      GenericOpPattern<triton::FpToFpOp>, GenericOpPattern<triton::IntToPtrOp>,
      GenericOpPattern<triton::PtrToIntOp>, GenericOpPattern<triton::SplatOp>,
      TritonBroadcastPattern, GenericOpPattern<triton::AddPtrOp>,
      TritonCatPattern, TritonJoinOpPattern, TritonSplitOpPattern,
      GenericOpPattern<triton::ClampFOp>,
      GenericOpPattern<triton::PreciseSqrtOp>,
      GenericOpPattern<triton::PreciseDivFOp>,
      GenericOpPattern<triton::MulhiUIOp>,
      GenericOpPattern<triton::ElementwiseInlineAsmOp>, TritonReducePattern,
      GenericOpPattern<triton::ReduceReturnOp>, TritonScanPattern,
      GenericOpPattern<triton::ScanReturnOp>,
      GenericOpPattern<triton::MakeRangeOp>, TritonExpandDimsPattern,
      TritonTransPattern, TritonDotPattern, GenericOpPattern<triton::LoadOp>,
      GenericOpPattern<triton::StoreOp>, GenericOpPattern<triton::HistogramOp>,
      GenericOpPattern<triton::ExternElementwiseOp>,
      GenericOpPattern<triton::PrintOp>, GenericOpPattern<triton::AssertOp>,
      GenericOpPattern<triton::AtomicCASOp>,
      GenericOpPattern<triton::AtomicRMWOp>, GenericOpPattern<ReturnOp>,
      GenericOpPattern<triton::ExperimentalDescriptorLoadOp>,
      GenericOpPattern<triton::ExperimentalDescriptorStoreOp>,
      GenericOpPattern<triton::CallOp>, TritonFuncOpPattern>(typeConverter, context);
}
```

对于GenericOpPattern，虽然用了replaceOpWithNewOp，但实际上没有什么改变。

```c++
// <class triton::AdvanceOp>
template <class Op> struct GenericOpPattern : public OpConversionPattern<Op> {
  using OpConversionPattern<Op>::OpConversionPattern;

  LogicalResult
  // triton::AdvanceOp
  matchAndRewrite(Op op, typename Op::Adaptor adaptor,
                  ConversionPatternRewriter &rewriter) const override {
    SmallVector<Type> retTypes;
    if (failed(this->getTypeConverter()->convertTypes(op->getResultTypes(),
                                                      retTypes)))
      return failure();
    // rewriter.replaceOpWithNewOp<triton::AdvanceOp>(op, retTypes, adaptor.getOperands(),
    //                              op->getAttrs());
    rewriter.replaceOpWithNewOp<Op>(op, retTypes, adaptor.getOperands(),
                                    op->getAttrs());

    return success();
  }
};
```

对于TritonXXXPattern这些patterns

```c++
struct TritonBroadcastPattern
    : public OpConversionPattern<triton::BroadcastOp> {
  using OpConversionPattern::OpConversionPattern;

  // This creates a tensor with the new shape but the argument's layout
  LogicalResult
  matchAndRewrite(BroadcastOp op, OpAdaptor adaptor,
                  ConversionPatternRewriter &rewriter) const override {
    auto srcType = cast<RankedTensorType>(adaptor.getSrc().getType());
    auto srcEncoding = srcType.getEncoding();
    if (!srcEncoding)
      return failure();
    Type retType = RankedTensorType::get(
        op.getType().getShape(), op.getType().getElementType(), srcEncoding);
    // Type retType = this->getTypeConverter()->convertType(op.getType());
    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::BroadcastOp>(
                      op, retType, adaptor.getOperands()),
                  adaptor.getAttributes());
    return success();
  }
};
```

### ConvertTritonGPUToLLVM

和ConvertTritonToTritonGPU类似

```c++
std::unique_ptr<OperationPass<ModuleOp>> createConvertTritonGPUToLLVMPass() {
  return std::make_unique<ConvertTritonGPUToLLVM>();
}
std::unique_ptr<OperationPass<ModuleOp>>
createConvertTritonGPUToLLVMPass(int32_t computeCapability) {
  return std::make_unique<ConvertTritonGPUToLLVM>(computeCapability);
}
```

ConvertTritonGPUToLLVM也有对应的runOnOperation函数，其中包含大量的poplulateXXXXToLLVMPatterns，以populateDotOpToLLVMPatterns为例子，pattern中添加了三个Conversion

```c++
void mlir::triton::NVIDIA::populateDotOpToLLVMPatterns(
    LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,
    PatternBenefit benefit) {
  patterns.add<DotOpConversion>(typeConverter, benefit);
  patterns.add<DotAsyncOpConversion>(typeConverter, benefit);
  patterns.add<DotWaitOpConversion>(typeConverter, benefit);
}
```

## References

- [MLIR 之 Dialect Conversion](https://tpumlir.org/zh-cn/2022/11/18/mlir-zhi-dialect-conversion.html)

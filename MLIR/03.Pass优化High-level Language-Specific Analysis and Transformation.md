# Pass优化High-level Language-Specific Analysis and Transformation

- [Pass优化High-level Language-Specific Analysis and Transformation](#pass优化high-level-language-specific-analysis-and-transformation)
  - [Optimize Transpose using C++ style pattern-match and rewrite](#optimize-transpose-using-c-style-pattern-match-and-rewrite)
  - [Optimize Reshapes using DRR](#optimize-reshapes-using-drr)

有两个方法可以实现transformations：

1. C++模板匹配和重写
2. Declarative Rewrite Relus(DRR)模板匹配和重写

## Optimize Transpose using C++ style pattern-match and rewrite 

首先的例子是使用简单的模板匹配来消除两个`transpose`嵌套使用`transpose(transpose(X)) -> X`，假设函数定义如下：

```python
def transpose_transpose(x) {
  return transpose(transpose(x));
}

def main() {
  var a<2, 3> = [[1, 2, 3], [4, 5, 6]];
  var b = transpose_transpose(a);
  print(b);
}
```

使用下面的命令

```shell
./toyc-ch3 ../../mlir/test/Examples/Toy/Ch3/transpose_transpose.toy -emit=mlir
```

对应的toy IR如下：

```python
module {
  toy.func @transpose_transpose(%arg0: tensor<*xf64>) -> tensor<*xf64> {
    %0 = toy.transpose(%arg0 : tensor<*xf64>) to tensor<*xf64>
    %1 = toy.transpose(%0 : tensor<*xf64>) to tensor<*xf64>
    toy.return %1 : tensor<*xf64>
  }
  toy.func @main() {
    %0 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>
    %1 = toy.reshape(%0 : tensor<2x3xf64>) to tensor<2x3xf64>
    %2 = toy.generic_call @transpose_transpose(%1) : (tensor<2x3xf64>) -> tensor<*xf64>
    toy.print %2 : tensor<*xf64>
    toy.return
  }
}
```

可以看到函数中实际执行了两次transpose，也就是说输入的x和输出的x是一模一样的，在优化中，我们就可以把这两次transpose给优化掉。

我们先使用 C++ 方式来写出表达式匹配和重写的代码（在mlir/examples/toy/Ch3/mlir/ToyCombine.cpp中）：

- 继承自`OpRewritePattern<TransposeOp>`类
- 重写`matchAndRewrite`方法
  - 获取操作数（输入）
  - 获取操作数对应的操作是否为transpose
  - 如果不是，返回匹配失败，表达式不需要优化
  - 如果是，重写当前表达式，返回匹配成功，将op重写为内层转置操作的操作数

```c++
/// This is an example of a c++ rewrite pattern for the TransposeOp. It
/// optimizes the following scenario: transpose(transpose(x)) -> x
struct SimplifyRedundantTranspose : public mlir::OpRewritePattern<TransposeOp> {
  /// We register this pattern to match every toy.transpose in the IR.
  /// The "benefit" is used by the framework to order the patterns and process
  /// them in order of profitability.
  SimplifyRedundantTranspose(mlir::MLIRContext *context)
      : OpRewritePattern<TransposeOp>(context, /*benefit=*/1) {}

  /// This method attempts to match a pattern and rewrite it. The rewriter
  /// argument is the orchestrator of the sequence of rewrites. The pattern is
  /// expected to interact with it to perform any changes to the IR from here.
  llvm::LogicalResult
  matchAndRewrite(TransposeOp op,
                  mlir::PatternRewriter &rewriter) const override {
    // Look through the input of the current transpose.
    mlir::Value transposeInput = op.getOperand();
    TransposeOp transposeInputOp = transposeInput.getDefiningOp<TransposeOp>();

    // Input defined by another transpose? If not, no match.
    if (!transposeInputOp)
      return failure();

    // Otherwise, we have a redundant transpose. Use the rewriter.
    rewriter.replaceOp(op, {transposeInputOp.getOperand()});
    return success();
  }
};
```

接下来，需要在canonicalization framework（规范化框架）中注册pattern，要开启注册，需要在transpose的tablegen中设置`hasCanonicalizer = 1`

```c++
//===----------------------------------------------------------------------===//
// TransposeOp
//===----------------------------------------------------------------------===//

def TransposeOp : Toy_Op<"transpose", [Pure]> {
  let summary = "transpose operation";

  let arguments = (ins F64Tensor:$input);
  let results = (outs F64Tensor);

  let assemblyFormat = [{
    `(` $input `:` type($input) `)` attr-dict `to` type(results)
  }];

  // Enable registering canonicalization patterns with this operation.
  let hasCanonicalizer = 1;

  // Allow building a TransposeOp with from the input operand.
  let builders = [
    OpBuilder<(ins "Value":$input)>
  ];

  // Indicate that additional verification for this operation is necessary.
  let hasVerifier = 1;
}
```

只要设置好，我们就可以在c++中进行注册

```c++
// Register our patterns for rewrite by the Canonicalization framework.
void TransposeOp::getCanonicalizationPatterns(
    RewritePatternSet &results, MLIRContext *context) {
  results.add<SimplifyRedundantTranspose>(context);
}
```

对应的，我们还需要在main文件（toyc.cpp）中将基于规范化框架的优化添加到运行流程中。

```c++
mlir::PassManager pm(module->getName());
pm.addNestedPass<mlir::toy::FuncOp>(mlir::createCanonicalizerPass());
```

完成后，使用下面的命令来执行这个优化

```shell
./toyc-ch3 ../../mlir/test/Examples/Toy/Ch3/transpose_transpose.toy -emit=mlir -opt
```

在输出中，可以看到`transpose_transpose`函数定义中已经没有任何transpose命令

```python
module {
  toy.func @transpose_transpose(%arg0: tensor<*xf64>) -> tensor<*xf64> {
    toy.return %arg0 : tensor<*xf64>
  }
  toy.func @main() {
    %0 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>
    %1 = toy.generic_call @transpose_transpose(%0) : (tensor<2x3xf64>) -> tensor<*xf64>
    toy.print %1 : tensor<*xf64>
    toy.return
  }
}
```

## Optimize Reshapes using DRR

另外一种表达式重写方法是基于DRR规则的方式来自动生成表达式匹配和重写函数。

DRR（Declarative Rule-based Pattern-match and Rewrite）：声明式的基于规则的模式匹配和重写。是一种基于DAG的声明性rewriter，提供table-based的模式匹配和重写规则的句法。

使用一个reshape优化的例子，来学习基于DRR的方式

```python
def main() {
  var a<2,1> = [1, 2];
  var b<2,1> = a;
  var c<2,1> = b;
  print(c);
}
```

执行下面的命令

```shell
./toyc-ch3 ../../mlir/test/Examples/Toy/Ch3/trivial_reshape.toy -emit=mlir
```

生成的toy IR如下，因为a、b和c的shape是一模一样的，所以reshape操作都是多余的。

```python
module {
  toy.func @main() {
    %0 = toy.constant dense<[1.000000e+00, 2.000000e+00]> : tensor<2xf64>
    %1 = toy.reshape(%0 : tensor<2xf64>) to tensor<2x1xf64>
    %2 = toy.reshape(%1 : tensor<2x1xf64>) to tensor<2x1xf64>
    %3 = toy.reshape(%2 : tensor<2x1xf64>) to tensor<2x1xf64>
    toy.print %3 : tensor<2x1xf64>
    toy.return
  }
}
```

和`SimplifyRedundantTranspose`类似，也需要进行pattern的定义，

- `ReshapeReshapeOptPattern`：`(ReshapeOp(ReshapeOp $arg))`替换为`(ReshapeOp $arg)`，即对多次相同的reshape只保留一次
- `RedundantReshapeOptPattern`：`(ReshapeOp:$res $arg)`替换为`reshape`的参数`(replaceWithValue $arg)`，这个还有个前提的约束条件是`$0.getType() == $1.getType()`，即类型相同

```c++
// Reshape(Reshape(x)) = Reshape(x)
def ReshapeReshapeOptPattern : Pat<(ReshapeOp(ReshapeOp $arg)),
                                    (ReshapeOp $arg)>;
def TypesAreIdentical : Constraint<CPred<"$0.getType() == $1.getType()">>;
def RedundantReshapeOptPattern : Pat<
                                    (ReshapeOp:$res $arg), (replaceWithValue $arg),
                                    [(TypesAreIdentical $res, $arg)]>;
```

除了上述的两个pattern，还用了`NativeCodeCall`来帮助进行constant value的reshape优化。在上述源代码中`var a<2,1> = [1, 2];`这一行生成了两行toy ir

```python
# generated from var a<2,1> = [1, 2];
%0 = toy.constant dense<[1.000000e+00, 2.000000e+00]> : tensor<2xf64>
%1 = toy.reshape(%0 : tensor<2xf64>) to tensor<2x1xf64>
```

`[1, 2]`是对应的constant，它的shape是`tensor<2xf64>`，和被赋值的a的shape不同，因此这里只靠上面两个reshape的pattern是无法优化掉的。

因此，在这里引入了一个`FoldConstantReshapeOptPattern`，将constant value的reshape后赋值变为inplace的操作，生成的toy ir中的constant的shape直接就是目标shape。

```c++
def ReshapeConstant : NativeCodeCall<"$0.reshape(($1.getType()).cast<ShapedType>())">;
def FoldConstantReshapeOptPattern : Pat<
  (ReshapeOp:$res (ConstantOp $arg)),
  (ConstantOp (ReshapeConstant $arg, $res))>;
```

接下来使用ODS框架和td文件来生成代码文件

```python
./mlir-tblgen --gen-rewriters /data_ssd1/zjy_home/llvm/llvm-project/mlir/examples/toy/Ch3/mlir/ToyCombine.td -I /data_ssd1/zjy_home/llvm/llvm-project/mlir/include/ -I ../../mlir/examples/toy/Ch3/include/
```

或者在CMakeList.txt中配置

```txt
set(LLVM_TARGET_DEFINITIONS mlir/ToyCombine.td)
mlir_tablegen(ToyCombine.inc -gen-rewriters)
add_public_tablegen_target(ToyCh3CombineIncGen)
```

运行官方教程给的编译好的二进制

```shell
./toyc-ch3 ../../mlir/test/Examples/Toy/Ch3/trivial_reshape.toy -emit=mlir -opt
```

生成出优化掉多余reshape的toy ir

```python
module {
  toy.func @main() {
    %0 = toy.constant dense<[[1.000000e+00], [2.000000e+00]]> : tensor<2x1xf64>
    toy.print %0 : tensor<2x1xf64>
    toy.return
  }
}
```
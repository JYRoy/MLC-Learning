# Enabling Generic Transformation with Interfaces

- [Enabling Generic Transformation with Interfaces](#enabling-generic-transformation-with-interfaces)
  - [Shape Inference：Preparing for Code Generation](#shape-inferencepreparing-for-code-generation)
    - [inline](#inline)

这里Generic Transformation指的泛化转换是说我们在03中讨论的一些优化pass，都是针对toy dialect的，但是对于其他的dialect，可能也会进行相同的优化。

如果针对每个dialect都实现一遍，会导致大量代码重复。所以我们期望能够对toy dialect能够在其他的dialect被重用。也就是从特化到泛化。

## Shape Inference：Preparing for Code Generation

对于之前写的一些例子，比如`multiply_transpose`，如果没有之前的优化，除了实例化constant tensor时，其他时候我们是不知道任何shape信息的。

```python
def multiply_transpose(a, b) {
  return transpose(a) * transpose(b);
}

def main() {
  var a<2, 3> = [[1, 2, 3], [4, 5, 6]];
  var b<2, 3> = [1, 2, 3, 4, 5, 6];
  var c = multiply_transpose(a, b);
  var d = multiply_transpose(b, a);
  print(d);
}
```

对应的未优化的toy ir如下，对于generic_call的返回tensor，我们是不知道返回的tensor的shape的。可能会导致前文03中的pass无法充分优化。

```python
toy.func @multiply_transpose(%arg0: tensor<*xf64>, %arg1: tensor<*xf64>) -> tensor<*xf64> {
  %0 = toy.transpose(%arg0 : tensor<*xf64>) to tensor<*xf64>
  %1 = toy.transpose(%arg1 : tensor<*xf64>) to tensor<*xf64>
  %2 = toy.mul %0, %1 : tensor<*xf64>
  toy.return %2 : tensor<*xf64>
}
toy.func @main() {
  %0 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>
  %1 = toy.reshape(%0 : tensor<2x3xf64>) to tensor<2x3xf64>
  %2 = toy.constant dense<[1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]> : tensor<6xf64>
  %3 = toy.reshape(%2 : tensor<6xf64>) to tensor<2x3xf64>
  %4 = toy.generic_call @multiply_transpose(%1, %3) : (tensor<2x3xf64>, tensor<2x3xf64>) -> tensor<*xf64>
  %5 = toy.generic_call @multiply_transpose(%3, %1) : (tensor<2x3xf64>, tensor<2x3xf64>) -> tensor<*xf64>
  toy.print %5 : tensor<*xf64>
  toy.return
}
```

要解决的主要问题就是如何处理用户定义的通用函数的调用，应为每个调用点可能推导出不同的shape。目前有两种方式来解决：

1. 基于参数类型进行符号推断，但是如果我们引入很多control flow会使得很难通用化
2. 函数特化，对于有新参数的函数的每个调用点都复制被调用的函数到调用点（todo：指的是inline？）

### inline

对于`multiply_transpose`这种小函数，它多次被调用，但是实际的计算开销很小，因此inline是一个有效的手段来优化调用开销。

**第一步**

通过`DialectInlinerInterface`类，定义toy dialect中的inline操作的约束。`DialectInlinerInterface`中包含很多虚函数的hook，我们定义的子类需要重写这些函数来实现自定义inline的操作和指定表达式的变形规则。

```c++
/// This class defines the interface for handling inlining with Toy operations.
/// We simplify inherit from the base interface class and override
/// the necessary methods.
struct ToyInlinerInterface : public DialectInlinerInterface {
  using DialectInlinerInterface::DialectInlinerInterface;

  /// This hook checks to see if the given callable operation is legal to inline
  /// into the given call. For Toy this hook can simply return true, as the Toy
  /// Call operation is always inlinable.
  bool isLegalToInline(Operation *call, Operation *callable,
                       bool wouldBeCloned) const final {
    return true;
  }

  /// This hook checks to see if the given operation is legal to inline into the
  /// given region. For Toy this hook can simply return true, as all Toy
  /// operations are inlinable.
  bool isLegalToInline(Operation *, Region *, bool,
                       IRMapping &) const final {
    return true;
  }

  /// This hook cheks if the given 'src' region can be inlined into the 'dest'
  /// region. The regions here are the bodies of the callable functions. For
  /// Toy, any function can be inlined, so we simply return true.
  bool isLegalToInline(Region *dest, Region *src, bool wouldBeCloned,
                       IRMapping &valueMapping) const final {
    return true;
  }

  /// This hook is called when a terminator operation has been inlined. The only
  /// terminator that we have in the Toy dialect is the return
  /// operation(toy.return). We handle the return by replacing the values
  /// previously returned by the call operation with the operands of the
  /// return.
  void handleTerminator(Operation *op,
                        MutableArrayRef<Value> valuesToRepl) const final {
    // Only "toy.return" needs to be handled here.
    auto returnOp = cast<ReturnOp>(op);

    // Replace the values directly with the return operands.
    assert(returnOp.getNumOperands() == valuesToRepl.size());
    for (const auto &it : llvm::enumerate(returnOp.getOperands()))
      valuesToRepl[it.index()].replaceAllUsesWith(it.value());
  }
};
```

其中三个重载的`isLegalToInline`是三个hook：

1. 第一个：给定的可调用操作是否可以內联到call的地方
2. 第二个：给定的操作是否可以內联到指定区域
3. 第三个：源区域是否可以內联到目标区域

`handleTerminator`函数用于处理return操作（toy.return），将返回操作的操作数`it.index()`先前返回的值替换为返回的操作数来处理返回。（todo）

**第二步**

我们可以直接在`ToyDialect`中进行dialect interface的注册

```c++
void ToyDialect::initialize() {
  addInterfaces<ToyInlinerInterface>();
}
```

**第三步**

我们需要提供一种方式给inliner感知到`toy.generic_call`代表一个函数调用，一个`toy.func`表示一个函数。

MLIR中提供了operation interfaces来标记一个operation是call-like还是callable-like。

不像dialect interface，operation interface只是针对一个operation的。

这个例子中，我们在Dialect的定义td中添加`CallOpInterface`和`CallableOpInterface`两个interfaces

```python
include "mlir/Interfaces/CallInterfaces.td"

def FuncOp : Toy_Op<"func",
    [DeclareOpInterfaceMethods<CallableOpInterface>]> {
  ...
}

def GenericCallOp : Toy_Op<"generic_call",
    [DeclareOpInterfaceMethods<CallOpInterface>]> {
  ...
}
```

我们使用`DeclareOpInterfaceMethods`来声明`GenericCallOp`和`FuncOp`所用的接口方法。这意味着我们只需要提供一个定义：

```c++
/// Returns the region on the function operation that is callable.
Region *FuncOp::getCallableRegion() { return &getBody(); }

// ....

/// Return the callee of the generic call operation, this is required by the
/// call interface.
CallInterfaceCallable GenericCallOp::getCallableForCallee() {
  return getAttrOfType<SymbolRefAttr>("callee");
}

/// Set the callee for the generic call operation, this is required by the call
/// interface.
void GenericCallOp::setCalleeFromCallable(CallInterfaceCallable callee) {
  (*this)->setAttr("callee", callee.get<SymbolRefAttr>());
}

/// Get the argument operands to the called function, this is required by the
/// call interface.
Operation::operand_range GenericCallOp::getArgOperands() { return inputs(); }
```

- `getCallableForCallee`：返回泛化调用operation的被调用方
- `getArgOperands`：用来获取被调用函数的参数操作数
- `setCalleeFromCallable`：为通用调用操作设置被调用方

**第四步**

因为在函数调用时，输入张量的类型是确定的。但是在函数定义时，输入张量的类型是不确定的。因此在调用的时候需要一个隐藏的数据类型转换，否则无法进行內联。因此要引入一个cast。cast将确定的数据类型转换为函数期望的数据类型。

例如`multiply_transpose`函数需要的参数是`tensor<*xf64>`，但是输入的是`tensor<2x3xf64>`。因此需要一个cast来完成类型转换。

在mlir/examples/toy/Ch5/include/toy/Ops.td中添加 cast 操作：

```python
def CastOp : Toy_Op<"cast", [
     DeclareOpInterfaceMethods<CastOpInterface>,
     DeclareOpInterfaceMethods<ShapeInferenceOpInterface>,
     NoSideEffect,
     SameOperandsAndResultShape
  ]> {
  let summary = "shape cast operation";
  let description = [{
    The "cast" operation converts a tensor from one type to an equivalent type
    without changing any data elements. The source and destination types must
    both be tensor types with the same element type. If both are ranked, then
    shape is required to match. The operation is invalid if converting to a
    mismatching constant dimension.
  }];

  let arguments = (ins F64Tensor:$input);
  let results = (outs F64Tensor:$output);

  let assemblyFormat = "$input attr-dict `:` type($input) `to` type($output)";
}
```

同样使用`DeclareOpInterfaceMethods`来声明`CastOp`所用的`CastOpInterface`方法。

还需要重写cast op的`areCastCompatible`方法，用于判断是否需要进行类型转换，如果inputs和outputsde类型是兼容的返回true，否则返回false。false意味着要进行cast。

```c++
/// Returns true if the given set of input and result types are compatible with
/// this cast operation. This is required by the `CastOpInterface` to verify
/// this operation and provide other additional utilities.
bool CastOp::areCastCompatible(TypeRange inputs, TypeRange outputs) {
  if (inputs.size() != 1 || outputs.size() != 1)
    return false;
  // The inputs must be Tensors with the same element type.
  TensorType input = inputs.front().dyn_cast<TensorType>();
  TensorType output = outputs.front().dyn_cast<TensorType>();
  if (!input || !output || input.getElementType() != output.getElementType())
    return false;
  // The shape is required to match if both types are ranked.
  return !input.hasRank() || !output.hasRank() || input == output;
}
```

在inliner interface中，则需要实现一个添加cast op的hook函数`materializeCallConversion`

```c++
struct ToyInlinerInterface : public DialectInlinerInterface {
  ...

  /// Attempts to materialize a conversion for a type mismatch between a call
  /// from this dialect, and a callable region. This method should generate an
  /// operation that takes 'input' as the only operand, and produces a single
  /// result of 'resultType'. If a conversion can not be generated, nullptr
  /// should be returned.
  Operation *materializeCallConversion(OpBuilder &builder, Value input,
                                       Type resultType,
                                       Location conversionLoc) const final {
    return builder.create<CastOp>(conversionLoc, resultType, input);
  }
};
```

**第五步**

将inliner pass添加到Toy用的pass manager中，代码位于`mlir/examples/toy/Ch5/toyc.cpp`中：

```c++
if (enableOpt) {
    mlir::PassManager pm(&context);
    // Apply any generic pass manager command line options and run the pipeline.
    applyPassManagerCLOptions(pm);

    // Inline all functions into main and then delete them.
    pm.addPass(mlir::createInlinerPass());
...
}
```

经过这个pass之后，toy ir变为

```python
func @main() {
  %0 = "toy.constant"() {value = dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>} : () -> tensor<2x3xf64>
  %1 = "toy.constant"() {value = dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>} : () -> tensor<2x3xf64>
  %2 = "toy.cast"(%1) : (tensor<2x3xf64>) -> tensor<*xf64>
  %3 = "toy.cast"(%0) : (tensor<2x3xf64>) -> tensor<*xf64>
  %4 = "toy.transpose"(%2) : (tensor<*xf64>) -> tensor<*xf64>
  %5 = "toy.transpose"(%3) : (tensor<*xf64>) -> tensor<*xf64>
  %6 = "toy.mul"(%4, %5) : (tensor<*xf64>, tensor<*xf64>) -> tensor<*xf64>
  toy.print %6 : tensor<*xf64>
  toy.return
}
```
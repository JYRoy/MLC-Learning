# Partial Lowering to Lower-Level Dialects for Optimization

- [Partial Lowering to Lower-Level Dialects for Optimization](#partial-lowering-to-lower-level-dialects-for-optimization)
  - [Dialect Conversions](#dialect-conversions)
    - [Conversion Target](#conversion-target)
    - [Conversion Patterns](#conversion-patterns)
    - [指定lowering过程](#指定lowering过程)
    - [部分lowering的设计考量](#部分lowering的设计考量)
    - [Complete Toy Example](#complete-toy-example)
    - [Affine Optimization](#affine-optimization)

## Dialect Conversions

因为MLIR可以定义非常多的dialects，所以MLIR提供了一个统一的DialectConversion框架来支持这些 Dialect 互转。

为了使用这个框架，我们需要提供两个条件（还有一个可选条件）。

- 转换目标（Conversation Target）。明确哪些 Dialect 操作是需要合法转换的，不合法的操作需要重写模式 (rewrite patterns) 来进行合法化。
- 一组重写模式（Rewrite Pattern）。这是用于将非法操作转换为零个或多个合法操作的一组模式。
- 类型转换器 （Type Converter）（可选）。如果提供，则用于转换块参数的类型。这一节将不需要此转换。

### Conversion Target

定义转换目标

对于目前的场景，我们期望将计算密集型的toy operation转换为`Affine`、`Arith`、`Func`和`MemRef`的operations的结合来进行后续的优化。

首先我们需要定义conversion target转换目标：将`Affine`、`Arith`、`Func`和`MemRef`设置为合法的dialects，将toy dialect设置为非法的dialect，将toy的print op指定为合法的operation。

```c++
void ToyToAffineLoweringPass::runOnOperation() {
  // The first thing to define is the conversion target. This will define the
  // final target for this lowering.
  mlir::ConversionTarget target(getContext());

  // We define the specific operations, or dialects, that are legal targets for
  // this lowering. In our case, we are lowering to a combination of the
  // `Affine`, `Arith`, `Func`, and `MemRef` dialects.
  target.addLegalDialect<affine::AffineDialect, arith::ArithDialect,
                         func::FuncDialect, memref::MemRefDialect>();

  // We also define the Toy dialect as Illegal so that the conversion will fail
  // if any of these operations are *not* converted. Given that we actually want
  // a partial lowering, we explicitly mark the Toy operations that don't want
  // to lower, `toy.print`, as *legal*. `toy.print` will still need its operands
  // to be updated though (as we convert from TensorType to MemRefType), so we
  // only treat it as `legal` if its operands are legal.
  target.addIllegalDialect<ToyDialect>();
  target.addDynamicallyLegalOp<toy::PrintOp>([](toy::PrintOp op) {
    return llvm::none_of(op->getOperandTypes(),
                         [](Type type) { return type.isa<TensorType>(); });
  });
  ...
}
```

### Conversion Patterns

明确转换模式

定义了Conversion Target之后，我们需要定义如何将非法的operation转换为合法的operation。

和canonicalization框架类似，DialectConversion框架也使用RewritePatterns来执行转换逻辑。这些patterns可能和之前看到的RewritePatterns类似，也可能是新的类型。但是ConversionPatterns和传统的RewritePatterns的不同点是，它接受一个额外的操作数参数，其中包含重新映射/替换的操作数。

对于transpose operation的lowering代码如下，将`toy.transpose` operation lowering到affine loop nest.

```c++
/// Lower the `toy.transpose` operation to an affine loop nest.
struct TransposeOpLowering : public mlir::ConversionPattern {
  TransposeOpLowering(mlir::MLIRContext *ctx)
      : mlir::ConversionPattern(TransposeOp::getOperationName(), 1, ctx) {}

  /// Match and rewrite the given `toy.transpose` operation, with the given
  /// operands that have been remapped from `tensor<...>` to `memref<...>`.
  llvm::LogicalResult
  matchAndRewrite(mlir::Operation *op, ArrayRef<mlir::Value> operands,
                  mlir::ConversionPatternRewriter &rewriter) const final {
    auto loc = op->getLoc();

    // Call to a helper function that will lower the current operation to a set
    // of affine loops. We provide a functor that operates on the remapped
    // operands, as well as the loop induction variables for the inner most
    // loop body.
    lowerOpToLoops(
        op, operands, rewriter,
        [loc](mlir::PatternRewriter &rewriter,
              ArrayRef<mlir::Value> memRefOperands,
              ArrayRef<mlir::Value> loopIvs) {
          // Generate an adaptor for the remapped operands of the TransposeOp.
          // This allows for using the nice named accessors that are generated
          // by the ODS. This adaptor is automatically provided by the ODS
          // framework.
          TransposeOpAdaptor transposeAdaptor(memRefOperands);
          mlir::Value input = transposeAdaptor.input();

          // Transpose the elements by generating a load from the reverse
          // indices.
          SmallVector<mlir::Value, 2> reverseIvs(llvm::reverse(loopIvs));
          return rewriter.create<mlir::AffineLoadOp>(loc, input, reverseIvs);
        });
    return success();
  }
};
```

接下来我们可以将这个pattern添加到模式集合中

```c++
void ToyToAffineLoweringPass::runOnOperation() {
  ...

  // Now that the conversion target has been defined, we just need to provide
  // the set of patterns that will lower the Toy operations.
  mlir::RewritePatternSet patterns(&getContext());
  patterns.add<..., TransposeOpLowering>(&getContext());

  ...
```

### 指定lowering过程

DialectConversion框架提供了几种不同的 Lowering 模式。这里使用的是部分lowering，因为`toy.print`不需要进行lowering。这个在前面target中已经声明过。

`mlir::applyPartialConversion(getOperation(), target, patterns)`表示对当前的mlir表达式中的operation应用了lowering。

```c++
void ToyToAffineLoweringPass::runOnOperation() {
  ...

  // With the target and rewrite patterns defined, we can now attempt the
  // conversion. The conversion will signal failure if any of our *illegal*
  // operations were not converted successfully.
  if (mlir::failed(mlir::applyPartialConversion(getOperation(), target, patterns)))
    signalPassFailure();
}
```

### 部分lowering的设计考量

在 Lowering 过程中，我们从值类型 TensorType 转换为已分配（类似缓冲区）的类型 MemRefType。

但是对于toy.print操作，这里不想 Lowering，因为这里主要是处理一些计算密集型算子并寻求优化机会，toy.print只是一个有打印功能的算子。

toy.print操作的定义中只支持打印输入类型为F64Tensor的输入数据，所以现在为了能将其和 MemRef Dialect 联系，我们需要为其增加一个F64MemRef类型

有三种不同的方式来处理：

1. 从buffer中生成load operations
2. 生成一个新版本的`toy.print`，对降低的类型进行操作
3. 更新`toy.print`来允许对降低的类型进行操作：更新定义`toy.print`的td文件

第三种方式更加简单，涉及到更新PrintOp的定义文件中的类型限制

```python
def PrintOp : Toy_Op<"print"> {
  ...

  // The print operation takes an input tensor to print.
  // We also allow a F64MemRef to enable interop during partial lowering.
  let arguments = (ins AnyTypeOf<[F64Tensor, F64MemRef]>:$input);
}
```

### Complete Toy Example

lowering之前的toy ir如下：

```python
toy.func @main() {
  %0 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>
  %2 = toy.transpose(%0 : tensor<2x3xf64>) to tensor<3x2xf64>
  %3 = toy.mul %2, %2 : tensor<3x2xf64>
  toy.print %3 : tensor<3x2xf64>
  toy.return
}
```

执行下面的命令生成lowering之后的代码

```shell
./toyc-ch5 ../../mlir/test/Examples/Toy/Ch5/affine-lowering.mlir -emit=mlir-affine
```

能够看到所有的toy operation已经被转换为四种dialect的operations了。只保留了`toy.print`。

```python
func.func @main() {
  %cst = arith.constant 1.000000e+00 : f64
  %cst_0 = arith.constant 2.000000e+00 : f64
  %cst_1 = arith.constant 3.000000e+00 : f64
  %cst_2 = arith.constant 4.000000e+00 : f64
  %cst_3 = arith.constant 5.000000e+00 : f64
  %cst_4 = arith.constant 6.000000e+00 : f64

  // Allocating buffers for the inputs and outputs.
  %0 = memref.alloc() : memref<3x2xf64>
  %1 = memref.alloc() : memref<3x2xf64>
  %2 = memref.alloc() : memref<2x3xf64>

  // Initialize the input buffer with the constant values.
  affine.store %cst, %2[0, 0] : memref<2x3xf64>
  affine.store %cst_0, %2[0, 1] : memref<2x3xf64>
  affine.store %cst_1, %2[0, 2] : memref<2x3xf64>
  affine.store %cst_2, %2[1, 0] : memref<2x3xf64>
  affine.store %cst_3, %2[1, 1] : memref<2x3xf64>
  affine.store %cst_4, %2[1, 2] : memref<2x3xf64>

  // Load the transpose value from the input buffer and store it into the
  // next input buffer.
  affine.for %arg0 = 0 to 3 {
    affine.for %arg1 = 0 to 2 {
      %3 = affine.load %2[%arg1, %arg0] : memref<2x3xf64>
      affine.store %3, %1[%arg0, %arg1] : memref<3x2xf64>
    }
  }

  // Multiply and store into the output buffer.
  affine.for %arg0 = 0 to 3 {
    affine.for %arg1 = 0 to 2 {
      %3 = affine.load %1[%arg0, %arg1] : memref<3x2xf64>
      %4 = affine.load %1[%arg0, %arg1] : memref<3x2xf64>
      %5 = arith.mulf %3, %4 : f64
      affine.store %5, %0[%arg0, %arg1] : memref<3x2xf64>
    }
  }

  // Print the value held by the buffer.
  toy.print %0 : memref<3x2xf64>
  memref.dealloc %2 : memref<2x3xf64>
  memref.dealloc %1 : memref<3x2xf64>
  memref.dealloc %0 : memref<3x2xf64>
  return
}
```

### Affine Optimization

完成lowering之后，我们可以在affine dialect的基础上进行进一步的优化。

例如：上面的lowering会将`toy.mul`转换为`affine.load` operation。但是会有冗余的operation。

添加`LoopFusion`和`AffineScalarReplacement` passes之后。

执行有优化的命令

```shell
./toyc-ch5 ../../mlir/test/Examples/Toy/Ch5/affine-lowering.mlir -emit=mlir-affine -opt
```

产生的新的ir如下，可以看到`affine.load`只有一个了。

```python
func.func @main() {
  %cst = arith.constant 1.000000e+00 : f64
  %cst_0 = arith.constant 2.000000e+00 : f64
  %cst_1 = arith.constant 3.000000e+00 : f64
  %cst_2 = arith.constant 4.000000e+00 : f64
  %cst_3 = arith.constant 5.000000e+00 : f64
  %cst_4 = arith.constant 6.000000e+00 : f64

  // Allocating buffers for the inputs and outputs.
  %0 = memref.alloc() : memref<3x2xf64>
  %1 = memref.alloc() : memref<2x3xf64>

  // Initialize the input buffer with the constant values.
  affine.store %cst, %1[0, 0] : memref<2x3xf64>
  affine.store %cst_0, %1[0, 1] : memref<2x3xf64>
  affine.store %cst_1, %1[0, 2] : memref<2x3xf64>
  affine.store %cst_2, %1[1, 0] : memref<2x3xf64>
  affine.store %cst_3, %1[1, 1] : memref<2x3xf64>
  affine.store %cst_4, %1[1, 2] : memref<2x3xf64>

  affine.for %arg0 = 0 to 3 {
    affine.for %arg1 = 0 to 2 {
      // Load the transpose value from the input buffer.
      %2 = affine.load %1[%arg1, %arg0] : memref<2x3xf64>

      // Multiply and store into the output buffer.
      %3 = arith.mulf %2, %2 : f64
      affine.store %3, %0[%arg0, %arg1] : memref<3x2xf64>
    }
  }

  // Print the value held by the buffer.
  toy.print %0 : memref<3x2xf64>
  memref.dealloc %1 : memref<2x3xf64>
  memref.dealloc %0 : memref<3x2xf64>
  return
}
```


